<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[利用深度学习对医学CT图像(LIDC-IDRI)中的肺结节进行良恶性判断]]></title>
    <url>%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%2F2019-3-29-%E5%88%A9%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AF%B9%E5%8C%BB%E5%AD%A6-CT-%E5%9B%BE%E5%83%8F%E4%B8%AD%E7%9A%84%E8%82%BA%E7%BB%93%E8%8A%82%E8%BF%9B%E8%A1%8C%E8%89%AF%E6%81%B6%E6%80%A7%E5%88%A4%E6%96%AD%2F</url>
    <content type="text"><![CDATA[肺癌是最常见的癌症，目前，CT可用于帮助医生在早期阶段检测肺癌。 在许多情况下，识别肺癌的诊断取决于医生的经验，这可能会忽略一些患者并导致一些问题。 在许多医学影像诊断领域，深度学习已被证明是一种流行且有效的方法。 本文主要基于LIDC-IDRI这一公开数据集，对其进行了肺结节的提取，并利用CNN对其分类训练，从而辅助医生作出判断。由于篇幅较长，将分为2篇博客，这篇主要介绍数据处理，即肺结节的提取。 数据集数据集采用为 LIDC-IDRI （The Lung Image Database Consortium），该数据集由胸部医学图像文件(.dcm)(如CT、X光片)和对应的诊断结果病变标注(.xml)组成。数据是由美国国家癌症研究所(National Cancer Institute)发起收集的，目的是为了研究高危人群早期癌症检测。该数据集中，共收录了1018个研究实例。对于每个实例中的图像，都由4位经验丰富的胸部放射科医师进行两阶段的诊断标注。在第一阶段，每位医师分别独立诊断并标注病患位置，其中会标注三中类别： $&gt;=$3mm的结节 $&lt;$3mm的结节 $&gt;=$3mm的非结节 在随后的第二阶段中，各位医师都分别独立的复审其他三位医师的标注，并给出自己最终的诊断结果。这样的两阶段标注可以在避免forced consensus的前提下，尽可能完整的标注所有结果。 图像信息（.dcm）图像文件为Dicom格式，是医疗图像的标准格式，其中除了图像像素外，还有一些辅助的元数据如图像类型、图像时间等信息。一张CT图像有 512x512 个像素点，在dicom文件中每个像素由2字节表示，所以每张图片约512KB大小。目前测试一共1012个病例数据，对于每个实例，可以看为一个三维矩阵D(slicer rows cols), slicer表示切片的个数(对应每个病例的.dcm文件数)，rows和cols分别表示图片的行数和列数(默认为512)。eg: 对于病例LIDC-IDRI-0001，即为$133 \times 512 \times 512$的矩阵，一共133张切片，每张大小$512 \times 512$。 查看dcm文件： 通过pip或者Anaconda安装pydicom模块，该模块是python专门用来处理dicom格式文件的库。 通过软件MicroDicom viewer 通过上面2种方式，我们可以看出dicom文件中包含了一些图像信息（SOP Instance UID、Study Instance UID，Series Instance UID······）SOP Instance UID：用于唯一区分每一张dcm切片Study Instance UID: 每个病例对应的检查实例号Series Instance UID: 不同检查对应的序列实例号 注释信息（.xml）Xml文件中包含放射科医生对病人CT图像中疑似肺结节的标注信息，主要分为三类： 结节(3mm-33mm):包含结节的特征信息（characteristics）、结节的完整轮廓(roi) 结节（&lt;3mm）：只显示结节的近似三维重心，若不透明则不标记 非结节（&gt;3mm）：只显示其近似的三维重心，指出非结节连接区域 Xml文件大体结构图如下： 其中对于3mm—33mm结节的characteristics，包含了如下信息：1） Subtlety：检测难度（1-5级，1最难，5最明显）2） internalStructure：内部结构（4种，软组织、液体、脂肪、空气）3） calcification：钙化（6种情况）4） sphericity：球形度（5种程度，但只明确3种）5） margin：边缘（5种程度）6） lobulation：分叶征（5种情况，但只明确2种）7） spiculation：毛刺征（5种情况，但只明确2种）8） texture：纹理（5种情况，但只明确3种） 9） maliynancy：恶性程度（1-5，1最低，5最高） 数据预处理本部分主要做的工作是分割肺实质，提取肺结节。 图像存储格式转换原始数据集的图像信息是以dcm格式存储的，但通常我们用作训练数据输入网络的图像大多是jpg或者png格式，所以为了方便以后的训练，我们首先要将原始图像转为jpg格式或者png格式存储，在这里我们是转为jpg格式存储的。此处测试共包含1012个病例，每个病例包含约100—300个dcm文件，我们使用MicroDicom viewer软件对其进行批量转换。以LIDC-IDRI-0001 为例： 原始数据转换后 将原始数据转换为jpg格式的图片后，下面我们会利用matlab编写函数分割肺实质，提取肺结节，主要包含下面6个函数： find_files(): 递归的遍历文件目录 fengefeishizhi(): 分割肺实质 readxml(): 读取标注信息(.xml)文件 readdicom() 存储标注信息中的肺结节信息，方便后面提取 jianqieimage(): 剪切肺结节 jianqie(): 根据肺结节的轮廓信息将其剪切出来，存为图片 分割肺实质将图像转换为jpg格式存储后，我们还要对数据进一步处理。由于我们最终是以肺结节图像作为训练数据输入网络，那么CT图像中除肺部以外的信息是无用的，所以我们要将肺实质分割出来。主要用到1个函数： fengefeishizhi()。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133clear all;clc;ticfor q =2:3 str1 = num2str(q); %jpg数据格式的存储路径 str2 = 'D:\MATLAB\work\0001-0120\LIDC-IDRI-000'; str3 ='\*.jpg'; str4 ='\'; %分割好肺实质后的图片存储路径 str5='D:\MATLAB\work\0001-0120_fenge\000'; str_imagedir = strcat(str2,str1,str3); str_dirname = strcat(str2,str1,str4); str_write= strcat(str5,str1,str4); %disp(str_imagedir) %disp(str_dirname) %disp(str_write)7 imagelist = dir(str_imagedir); for i = 1:length(imagelist) name = imagelist(i).name; dirname = [str_dirname,name];%B=imread(dirname);%读取原图像% B=rgb2gray(A);%将原图像转换为灰度图像 A=imread(dirname); B=rgb2gray(A);%subplot(2,2,1),imshow(B,[]),title('DICOM图像导入后显示');% figure,imshow(B),title('图像导入后显示');%==================================================== min(min(B)); max(max(B)); t=graythresh(B);%计算阈值t C=im2bw(B,t);%根据阈值二值化图像% figure(),imshow(C,[]),title('显示二值化图像');% C=bwareaopen(C,6000);%去除面积小于T的部分（气管）。%%%%%%%%%在肺实质比较大的时候，而且操作床特殊分段构造，面积为10000 D=imfill(C,4,'holes');%对二值化后的图像填充肺实质% figure(),imshow(D,[]),title('显示填充肺实质图像'); E=D-C;%得到肺实质的图像E% figure(),imshow(E,[]),title('显示肺实质的图像'); F=imfill(E,8,'holes');%填充肺实质空洞% FMask=bwareaopen(F,1000);%去除面积小于T的部分（气管）。%%%%%%%%%在肺实质比较大的时候，而且操作床特殊分段构造，面积为4600 FMask=bwareaopen(F,6000);%去除面积小于T的部分（气管）。%%%%%%%%%在肺实质比较大的时候，而且操作床特殊分段构造，面积为4600% figure(),imshow(FMask,[]),title('显示掩摸');%得到掩膜%-------------------------分开左右肺---------------------------------------- r_ball=90;%可变的，取值为10/15,越小越细致 se_ball=strel('ball',r_ball,10);%椭圆体半径10，高度10 r_disk=ceil(r_ball/6);%圆整r_ball/6得到大于或等于它的最接近整数。ceil取整 if r_disk==0; r_disk=1;%最小为1 end se_erode=strel('disk',r_disk,0); %圆形半径 mask=imopen(FMask,1);%开操作% figure(),imshow(mask,[]); L=bwlabel(FMask); %数学形态重建，基于膨胀运算，用掩摸对二值图像标记，将图像分成多个区域%stat = regionprops(FMask);%,计算图像区域特征，区域连通，object为二值图像， [row,col]=size(B);%im2bw，Convert image to binary image, based on threshold%im2bw默认threshold0.5，得到512*512空矩阵 mask_leftlung=im2bw(zeros(row,col));%左肺掩膜 mask_rightlung=im2bw(zeros(row,col));%右肺掩膜 for i=1:row for j=1:col if L(i,j)==1 %如果是左肺 mask_leftlung(i,j)=1;% 分开左右肺，肺是白色的 end if L(i,j)==2 mask_rightlung(i,j)=1; end end end% figure(),imshow(mask_leftlung,[]);title('左肺掩摸显示')% figure(),imshow(mask_rightlung,[]);title('右肺掩摸显示')%----------------------对左肺修补------------------------------------------- object1=1-mask_leftlung; %左肺反向% figure();imshow(object1,[]);title('左肺反向后显示') object2=imopen(object1,se_ball);%开操作，椭圆体半径30，高度10% figure();imshow(object2,[]);title('反向左肺模糊重影图显示') %得到反向左肺模糊重影图 leftmask1=1-object2;%左肺模糊重影图 % figure();imshow(leftmask1,[]);title('左肺模糊重影图显示') leftmask2=im2bw(leftmask1,0.5);%根据阈值0.5将图像生成二值图像%figure();imshow(leftmask2,[]);title('左肺清晰二值图像显示')%%得到左肺清晰的二值图像，支气管消去了，结节的毛刺也消除，结节变小；对左肺进行了修补 leftmask3=imfill(leftmask2,'hole'); %填充左肺实质空洞% figure();imshow(leftmask3,[]),title('填充左肺实质后显示'); %只是填充了左肺实质，得到不平滑的左肺图像 leftmask4=imerode(leftmask3,se_erode);%腐蚀左肺操作，肺结节大了点，平滑作用% figure();imshow(leftmask4,[]),title('leftlungmask');%得到平滑效果图像%---------------------补回空洞---------------------------------------------- ConvHull=bwconvhull(leftmask4,'object');%对左肺掩摸求凸壳%figure();imshow(ConvHull,[]),title('凸壳图像'); DIF_ConvHull=ConvHull-leftmask4;%将补的缺口部分取出来%figure();imshow(DIF_ConvHull,[]),title('与左肺原图差值图像'); BW1 = bwconncomp(DIF_ConvHull);%利用连通域分析左肺凸壳 stats = regionprops(BW1, 'Area','Eccentricity');%获得每个连通域得面积、离心率 idx = find([stats.Area] &gt; 80 &amp; [stats.Eccentricity] &lt; 0.8); % % % % BW2 = ismember(labelmatrix(BW1), idx);%取出符合要求的区域% % % % figure();imshow(BW2,[]),title('左肺所需要补的部分显示');% % % % leftmask5=BW2+leftmask4;%将符合要求的区域“补”到左肺掩摸中%figure();imshow(leftmask5,[]),title('显示最终的左肺掩摸');%---------------------对右肺修补-------------------------------------------- object1=1-mask_rightlung; %反转右肺轮廓 %figure();imshow(object1,[]);title('右肺反向后显示') object2=imopen(object1,se_ball);%开操作 %figure();imshow(object2,[]);title('反向右肺模糊重影图显示') %得到反向右肺模糊重影图 rightmask1=1-object2;%得到右肺模糊掩膜，反转回来，实质为白色 %figure();imshow(rightmask1,[]);title('右肺模糊重影图显示') rightmask2=im2bw(rightmask1,0.5);%右肺转换为二值图像 %figure();imshow(rightmask2,[]);title('右肺清晰二值图像显示') rightmask3=imfill(rightmask2,'hole');%填充右肺实质空洞 %figure();imshow(rightmask3,[]),title('填充右肺实质后显示'); rightmask4=imerode(rightmask3,se_erode);%腐蚀操作，平滑作用% figure();imshow(rightmask4,[]),title('rightlungmask'); % % % lungmask=im2bw(leftmask5+rightmask4);%将左右肺合并，得到全肺掩膜 lungmask=im2bw(leftmask4+rightmask4);%将左右肺合并，得到全肺掩膜 lung=immultiply(lungmask,B);%相与,得到的是灰度值从0到max-min+1的灰度图像 %dicomwrite(lung,'E:\1_毕业设计\images_CT\S60\I00');%dicomwrite()函数将lung（从源图像提取出来的肺实质）图像保存为dicom文件格式，方便下次使用 %subplot(2,2,2),imshow(lung,[]),title('提取的肺实质'); %figure;imshow(lung,[]),title('提取的肺实质');%name = + name; feishizhi = [str_write,name]; imwrite(lung,feishizhi);%break endend 以LIDC-IDRI-0001 中的部分切片为例，其中左侧为原始CT图像，右侧为分割肺实质后的图像，效果图如下: 读取标注信息并存储我们从医生的标注信息文件（.xml）读取肺结节的位置信息和良恶性程度，然后存储到对应的xls表中。主要用到2个函数： readxml()和readdicom()代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172function [num_mal,sop_text,max_min_xy]=zl_readxml(xml_path)% % function [sop_text,max_min_xy]=zl_readxml(xml_path)% clear all% clc%xml_path = 'H:\肺结节\数据\LIDC-IDRI\900-300\LIDC-IDRI\LIDC-IDRI-0060\1.3.6.1.4.1.14519.5.2.1.6279.6001.203745372924354240670222118382\1.3.6.1.4.1.14519.5.2.1.6279.6001.463214953282361219537913355115\191.xml';%% 跳转到内层标签unblindedReadNoduledocNode = xmlread(xml_path); %读取XML文件返回一个文件模型节点* document = docNode.getDocumentElement();readingSession = document.getElementsByTagName('readingSession'); %返回与给定的元素所有子节点的Nodelist对象*%% 最后返回的三个值%% 最后返回的三个值num_mal = []; %每个结节的恶性度和属于该类别的图片的数量sop_text = &#123; &#125;; %每个图片的标号max_min_xy = []; %每个图像中肺结节的x和y的最小值和最大值sop_num = 0; %总结节个数？*%%for r = 0:readingSession.getLength()-1 unblinded_nodule = readingSession.item(r).getElementsByTagName('unblindedReadNodule'); %unblindedReadNodule一个节点标记，&lt;unblindedReadNodule&gt;节点数据包括在&lt;/unblindedReadNodule&gt;* for u = 0 : unblinded_nodule.getLength()-1 roi = unblinded_nodule.item(u).getElementsByTagName('roi'); %item() 方法可返回节点列表中处于指定索引号的节点。*&lt;roi&gt;结节轮廓&lt;/roi&gt;* mal = unblinded_nodule.item(u).getElementsByTagName('malignancy'); %&lt;malignancy&gt;结节恶性度&lt;/malignancy&gt;* %如果xml文件中没有malignancy或者roi标签直接跳过 if isempty(roi.item(0)) continue; end if isempty(mal.item(0)) continue; end Num_roi = roi.getLength(); %该类别的图片的数量 mal_int = str2num(char(mal.item(0).getTextContent())); num_mal = [num_mal();mal_int,Num_roi]; for i = 0 : Num_roi-1 %遍历* sop_id = roi.item(i).getElementsByTagName('imageSOP_UID'); %图片编号* sop_text&#123;sop_num + i + 1&#125; = char(sop_id.item(0).getTextContent()); %数组* edgeMap = roi.item(i).getElementsByTagName('edgeMap'); %边界* xy = []; for j = 0 :edgeMap.getLength()-1 %获得坐标* xCoord = edgeMap.item(j).getElementsByTagName('xCoord'); xCoord_int = str2num(char(xCoord.item(0).getTextContent())); yCoord = edgeMap.item(j).getElementsByTagName('yCoord'); yCoord_int = str2num(char(yCoord.item(0).getTextContent())); xy=[xy();xCoord_int,yCoord_int]; end %找到结节轮廓* if edgeMap.getLength()==1 max_min_xy = [max_min_xy();xy,xy]; continue; end [maxr,max_index] = max(xy); [minr,min_index] = min(xy); max_min_xy = [max_min_xy();minr,maxr]; end sop_num = sop_num + Num_roi; %总个数 end if isempty(num_mal) continue; end num_mal = [num_mal();0,0]; %扩展维数*endend 上述是辅助函数，提取函数如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384clear;clc; %% Each treatment 100 or 200 %处理数据导入到表格中%LIDC_path = 'E:\zhaolei\深度学习\肺结节\400-499\LIDC-IDRI\'; 原路径LIDC_path = 'D:\MATLAB\tiqu\LIDC-IDRI';%IDRI_path = 'H:\肺结节\数据\LIDC-IDRI\'; 原路径%XLS_path = 'H:\肺结节\数据\excel\excel_all'; 原路径IDRI_path = 'D:\MATLAB\tiqu\LIDC-IDRI';XLS_path = 'D:\MATLAB\tiqu\xls';IDRI_child_path = dir(IDRI_path); %打开文件目录并返回文件结构体*num_IDRI_child = size(IDRI_child_path); %返回列和行数的数组* %for n = 8 :num_IDRI_child 原版 %为啥从8开始？？？for n = 3 :num_IDRI_child %非原版 % child_idri_path = [IDRI_path,IDRI_child_path(n).name];原版（可能有错） child_idri_path = [IDRI_path,'\',IDRI_child_path(n).name]; %非原版 child_idri_path_temp = dir(child_idri_path); %打开文件* LIDC_path = [child_idri_path,'\',child_idri_path_temp(3).name]; %文件目录* LIDC_child_path = dir(LIDC_path); %打开 num_child = size(LIDC_child_path); %返回文件的列和行数的数组* for i = 3 : num_child(1) %从3开始（前两个是. ..） %% find dicom file list child_path = [LIDC_path,'\',LIDC_child_path(i).name]; %一步步打开文件夹 child_path_temp = dir(child_path); child_path1 = [child_path,'\',child_path_temp(3).name]; child_path_temp = dir(child_path1); %xml_path = [child_path1,'\',child_path_temp(3).name]; xml_path = [child_path,'\']; %获取单个文件夹中的dicom和xml文件 dcm_files = find_files(xml_path, '.dcm'); % 获得文件列表 xml_files = find_files(xml_path, '.xml'); xml_path = char(xml_files); [num_mal,sop_text,max_min_xy]=zl_readxml(xml_path); %函数调用 % num_mal = []; %每个结节的恶性度和属于该类别的图片的数量 % sop_text = &#123; &#125;; %每个图片的标号 % max_min_xy = []; %每个图像中肺结节的x和y的最小值和最大值 sop_num = size(sop_text); % 获得行列数，行：？ 列：图片数* mal_num = size(num_mal); %行： 图片数？* dcm_number = [ ]; %图片编号* %?? if sop_num(2)&gt;mal_num(1) %要根据他们两个的差值来决定补多少个0 for m = 1 : sop_num(2)-mal_num(1) num_mal = [num_mal();0,0]; %添加扩展维度* end end if sop_num(2)&lt; mal_num(1) for m = 1 : mal_num(1) - sop_num(2) % 只有数据维度一样才能被写入到文件中！所以少的要补上四个0 dcm_number= [dcm_number;0]; %添加扩展维度 max_min_xy = [max_min_xy;0,0,0,0]; %添加扩展维度 end end %?? %% Get the number and file name of the image In a single folder for md = 1 : sop_num(2) %??? dcm_number= [dcm_number;0]; end for j = 1:numel(dcm_files) %遍历文件 dicomInformation = dicominfo(dcm_files&#123;j&#125;); %存储图片信息 instance = dicomInformation.SOPInstanceUID; imagenum = dicomInformation.InstanceNumber; % Make sure that the StudyInstanceUID matches that found in % the XML annotations for s = 1 : sop_num(2) %对比 if strcmpi(instance,sop_text(1,s)) dcm_number(s) = imagenum; %编号？？?* end end end total = [num_mal,dcm_number,max_min_xy]; if isempty(total) continue; end child_path = [XLS_path,'\',LIDC_child_path(i).name] xlswrite(child_path,total); %导入到表格中 2017/4/10 endend 提取肺结节读取到肺结节的位置信息和良恶性程度后，我们要根据该信息提取肺结节。主要用到2个函数： jianqieimage()和jianqie()代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546function zl_jianqie(img_path,dir,times,size_center,col4xy) %dir 为患病可能程度，col4xy为剪切区域 train_path = 'I:\肺结节\数据\result2\train23jpg\'; result_name = [img_path(24:37),'_',char(num2str(dir)),'_',char(num2str(times)),img_path(38:46)]; train_path = [train_path,char(num2str(dir)),'\',result_name]; %剪切路径* img=imread(img_path); %读取图片文件* img1=imcrop(img,col4xy); %返回图像的一个裁剪区域* I2=imcrop(I,[a b c d]);%利用裁剪函数裁剪图像，其中， %（a,b）表示裁剪后左上角像素在原图像中的位置；c表示裁剪后图像的宽，d表示裁剪后图像的高 %% 分割肺结节实质 img1_size = size(img1); min(min(img1)); % 找到最小值，最大值 max(max(img1)); t=graythresh(img1); %使用最大类间方差法找到图片的一个合适的阈值threshold C=im2bw(img1,t); %转换为二值图像* D=imfill(C,4,'holes');%对二值化后的图像填充肺实质 if dir &gt;=4 %大概率为肺癌* FMask=bwareaopen(D,10); % 除二值图像中面积小于10的对象 D = FMask; end total = 0; for i = 1:img1_size(1) %行数 for j = 1:img1_size(2) %列数 if D(i,j) == 0 %二值图像当值为0时 （黑色） img1(i,j) = 0; end if D(i,j) == 1 %二值图像当值为1时 （白色） if ~(i &gt; size_center(1) &amp;&amp; j &gt; size_center(1)&amp;&amp; j &lt; size_center(1) + size_center(3)&amp;&amp; i &lt; size_center(1) + size_center(3)) %不在范围内*？ img1(i,j) = 0; %取为黑色* end end end end %% 保存图片 for m = 1:img1_size(1) for n = 1:img1_size(2) if img1(m,n) == 0 %黑色元素点个数* total = total + 1; end end end if total ~= img1_size(1)*img1_size(2) %如果不全是黑* imwrite(img1,train_path); %存入图片* endend 12345678910111213141516171819202122232425262728293031323334clear;clc;%肺实质的图片image_path = 'I:\肺结节\数据\result2\jpg2\';%肺结节的位置信息和良恶性程度xls_path = 'I:\肺结节\数据\result2\result22.xls';[txt,xls_text] = xlsread(xls_path);xls_num = size(xls_text);xls_num(1);for m = 1:xls_num(1) img_name = xls_text(m,1); str = img_name&#123;1&#125;; img_name = [str,'.jpg']; jpg_child_path = [image_path,img_name] if exist(jpg_child_path,'file') col4x = txt(m,4) - txt(m,2); col4y = txt(m,5) - txt(m,3); dir = txt(m,6); times = txt(m,7); size_center = [ ]; if col4x &lt; 32 &amp;&amp; col4y &lt; 32 ma = 0.5 * (32 - max(col4x,col4y)); col4xy = [txt(m,2)-ma,txt(m,3)-ma,32,32]; size_center =[ma,ma,max(col4x,col4y)]; zl_jianqie(jpg_child_path,dir,times,size_center,col4xy); continue; end size_center =[0,0,max(col4x,col4y)]; col4xy = [txt(m,2),txt(m,3),max(col4x,col4y),max(col4x,col4y)]; zl_jianqie(jpg_child_path,dir,times,size_center,col4xy); end % break;end 通过上面的函数，我们可以将肺结节提取出来，并按照良恶性程度分类存储。部分示例如下： 其中1-5表示肺结节的良恶性程度，5表示恶性可能性最大，1表示恶性可能性最小。注意：3表示不确定是否为肺结节，良恶性程度也不确定。 至此，我们的数据预处理结束了，分类训练见下一篇博文。]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN网络发展史]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-3-28-CNN%E7%BD%91%E7%BB%9C%E5%8F%91%E5%8F%91%E5%B1%95%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[卷积神经网络可谓是现在深度学习领域中大红大紫的网络框架，尤其在计算机视觉领域更是一枝独秀。 CNN从90年代的LeNet开始，21世纪初沉寂了10年，直到12年AlexNet开始又再焕发第二春，从ZF Net到VGG，GoogLeNet再到ResNet和最近的DenseNet，网络越来越深，架构越来越复杂，解决反向传播时梯度消失的方法也越来越巧妙。本文总结一波CNN的各种经典架构。 转载自：CNN网络架构演进]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[马氏距离和欧式距离详解]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-27-%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB%E5%92%8C%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[一般在机器学习模型中会涉及到衡量两个样本间的距离，如聚类、KNN，K-means等，使用的距离为欧式距离。其实，除了欧氏距离之外，还有很多的距离计算标准，本文主要介绍欧氏距离和马氏距离。 欧氏距离最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点 $x = (x_1,…,x_n)$ 和 $y = (y_1,…,y_n)$ 之间的距离为：$$d(x,y) = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2+…+(x_n-y_n)^2} = \sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}$$ 二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离：$$d_{12} = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2}$$ 两个n维向量$a(x_{11},x_{12},…,x_{1n})$与 $b(x_{21},x_{22},…,x_{2n})$间的欧氏距离：$$d_{12} = \sqrt{\sum_{k=1}^{n}(x_{1k}-x_{2k})^2}$$ 马氏距离在介绍马氏距离之前，我们先来看如下几个概念： 方差：方差是标准差的平方，而标准差的意义是数据集中各个点到均值点距离的平均值。反应的是数据的离散程度。 协方差：标准差与方差是描述一维数据的，当存在多维数据时，我们通常需要知道每个维数的变量中间是否存在关联。协方差就是衡量多维数据集中，变量之间相关性的统计量。比如说，一个人的身高与他的体重的关系，这就需要用协方差来衡量。如果两个变量之间的协方差为正值，则这两个变量之间存在正相关，若为负值，则为负相关。 协方差矩阵：当变量多了，超过两个变量了。那么，就用协方差矩阵来衡量这么多变量之间的相关性。假设 $X$ 是以 $n$个随机变数（其中的每个随机变数是也是一个向量，当然是一个行向量）组成的列向量： 其中，$μ_i$是第i个元素的期望值，即$μ_i=E(X_i)$。协方差矩阵的第$i,j$项（第$i,j$项是一个协方差）被定义为如下形式：$$\sum_{ij} = cov(X_i,X_j = E[(X_i-\mu_i)(X_j-\mu_j)])$$即： 矩阵中的第 $(i,j)$ 个元素是 $X_i$ 与 $X_j$ 的协方差。 马氏距离的定义：马氏距离（Mahalanobis Distance）是由马哈拉诺比斯（P. C. Mahalanobis）提出的，表示数据的协方差距离。它是一种有效的计算两个未知样本集的相似度的方法。与欧氏距离不同的是它考虑到各种特性之间的联系（例如：一条关于身高的信息会带来一条关于体重的信息，因为两者是有关联的）并且是尺度无关的（scale-invariant），即独立于测量尺度。对于一个均值为$μ=(μ_1,μ_2,μ_3,…,μ_p)^T$，协方差矩阵为$S$的多变量$x=(x_1,x_2,x_3,…,x_p)^T$，其马氏距离为：$$D_M(x) = \sqrt{(x-\mu)^T {S}^{-1}(x-\mu)}$$我们可以发现如果$S^{-1}$是单位阵的时候，马氏距离简化为欧氏距离。 那我们为什么要用马氏距离呢？马氏距离有很多优点： 马氏距离不受量纲的影响，两点之间的马氏距离与原始数据的测量单位无关；由标准化数据和中心化数据(即原始数据与均值之差）计算出的二点之间的马氏距离相同。马氏距离还可以排除变量之间的相关性的干扰。 下面我们来看一个例子：如果我们以厘米为单位来测量人的身高，以克（g）为单位测量人的体重。每个人被表示为一个两维向量，如一个人身高173cm，体重50000g，表示为（173,50000），根据身高体重的信息来判断体型的相似程度。 我们已知小明（160,60000）；小王（160,59000）；小李（170，60000）。根据常识可以知道小明和小王体型相似。但是如果根据欧几里得距离来判断，小明和小王的距离要远远大于小明和小李之间的距离，即小明和小李体型相似。这是因为不同特征的度量标准之间存在差异而导致判断出错。 以克（g）为单位测量人的体重，数据分布比较分散，即方差大，而以厘米为单位来测量人的身高，数据分布就相对集中，方差小。马氏距离的目的就是把方差归一化，使得特征之间的关系更加符合实际情况。 下图（a）展示了三个数据集的初始分布，看起来竖直方向上的那两个集合比较接近。在我们根据数据的协方差归一化空间之后，如图（b），实际上水平方向上的两个集合比较接近。 深入分析：当求距离的时候，由于随机向量的每个分量之间量级不一样，比如说x1可能取值范围只有零点几，而x2有可能时而是2000，时而是3000，因此两个变量的离散度具有很大差异马氏距离除以了一个方差矩阵，这就把各个分量之间的方差都除掉了，消除了量纲性，更加科学合理。 如上图，看左下方的图，比较中间那个绿色的和另外一个绿色的距离，以及中间绿色到蓝色的距离 如果不考虑数据的分布，就是直接计算欧式距离，那就是蓝色距离更近 但实际上需要考虑各分量的分布的，呈椭圆形分布 蓝色的在椭圆外，绿色的在椭圆内，因此绿色的实际上更近 马氏距离除以了协方差矩阵，实际上就是把右上角的图变成了右下角 参考资料：马氏距离通俗理解]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>distance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习中的Bagging和Boosting]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-25-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84Bagging%E5%92%8CBoosting%2F</url>
    <content type="text"><![CDATA[在机器学习和统计学习中, 集成学习(Ensemble Learning)是一种将多种学习算法组合在一起以取得更好表现的一种方法。它本身不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。也就是我们常说的“博采众长”。集成学习可以用于分类问题集成，回归问题集成，特征选取集成，异常点检测集成等。 集成学习概述什么是集成学习(此处以分类为例) 将多个分类方法聚集在一起，以提高分类的准确率（可以是相同or不同算法） 集成学习法由训练数据构建一组基分类器，然后通过对每个基分类器的预测进行投票来进行分类 严格来说，集成学习并不算是一种分类器，而是一种分类器结合的方法。 如果把单个分类器比作一个决策者的话，集成学习的方法就相当于多个决策者共同进行一项决策。 我们可以对集成学习的思想做一个概括（见下图）。对于训练集数据，我们通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的。 也就是说，集成学习有两个主要的问题需要解决: 如何得到若干个个体学习器 如何选择一种结合策略，将这些个体学习器集合成一个强学习器。 对于第一个问题，如何得到若干个个体学习器，一般有2种选择 所有的个体学习器都是一个种类的,或者说是同质的。比如都是决策树个体学习器，或者都是神经网络个体学习器。（广泛） 所有的个体学习器不全是一个种类的，或者说是异质的。比如我们有一个分类问题，对训练集采用支持向量机个体学习器，逻辑回归个体学习器和朴素贝叶斯个体学习器来学习，再通过某种结合策略来确定最终的分类强学习器。 目前来说，同质个体学习器的应用是最广泛的，一般我们常说的集成学习的方法都是指的同质个体学习器。而同质个体学习器使用最多的模型是CART决策树和神经网络。同质个体学习器按照个体学习器之间是否存在依赖关系可以分为两类： 第一个是个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成，代表算法是Boosting系列算法; 第二个是个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是Bagging和随机森林（Random Forest）系列算法。 集成学习之BoostingBoosting的算法原理图如下： 从图中可以看出，Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，让这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Tree)。 集成学习之BaggingBagging的算法原理和 Boosting不同，它的弱学习器之间没有依赖关系，可以并行生成，原理图如下： 从上图可以看出，bagging的个体弱学习器的训练集是通过随机采样得到的。通过T次的随机采样，我们就可以得到T个采样集，对于这T个采样集，我们可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集合策略来得到最终的强学习器。详细算法过程描述如下： 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的） 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等） 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同） 随机森林（RF）是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择。对于Bagging需要注意的是，每次训练集可以取全部的特征进行训练，也可以随机选取部分特征训练，例如随机森林就是每次随机选取部分特征 集成学习之结合策略 平均法对于数值类的回归预测问题，通常使用的结合策略是平均法，也就是说，对于若干个弱学习器的输出进行平均得到最终的预测输出。 最简单的平均是算术平均，也就是说最终预测是$$H(x) = \frac{1}{T}\sum\limits_{1}^{T}h_i(x)$$ 如果每个个体学习器有一个权重w，则最终预测是$$H(x) = \sum\limits_{i=1}^{T}w_ih_i(x)$$其中wi是个体学习器hi的权重，通常有$$w_i \geq 0 ,\;\;\; \sum\limits_{i=1}^{T}w_i = 1$$ 投票法 对于分类问题的预测，我们通常使用的是投票法。假设我们的预测类别是${c_1,c_2,…c_K}​$,对于任意一个预测样本x，我们的T个弱学习器的预测结果分别是$(h_1(x),h_2(x)…h_T(x))$。 最简单的投票法是相对多数投票法，也就是我们常说的少数服从多数，也就是T个弱学习器的对样本x的预测结果中，数量最多的类别ci为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。 稍微复杂的投票法是绝对多数投票法，也就是我们常说的要票过半数。在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数。否则会拒绝预测。 更加复杂的是加权投票法，和加权平均法一样，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别。 学习法 前2种方法都是对弱学习器的结果做平均或者投票，相对比较简单，但是可能学习误差较大，于是就有了学习法这种方法，对于学习法，代表方法是stacking，当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。 在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。 对比Bagging和Boosting 样本选择上： Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。 Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。 样例权重： Bagging：使用均匀取样，每个样例的权重相等 Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。 预测函数： Bagging：所有预测函数的权重相等。 Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。 并行计算： Bagging：各个预测函数可以并行生成 Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。 偏差和方差 Bagging：主要关注降低偏差（bias），因为他更加关注分类错误的样本 Boosting：更加关注降低方差（variance），因为他不容易受极值点影响详细的解释可参考Bagging和Boosting的区别偏差相当于预测准确性，而方差相当于预测稳定性，下图就能明显的说明偏差和方差。 总结 参考资料Bagging,Boosting,StackingBagging和Boosting的区别]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>ensemble learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数字图像处理的常用方法]]></title>
    <url>%2FComputer-vision%2F2019-3-25-%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[数字图像处理是指将图像信号转换成数字信号并利用计算机对其进行处理的过程。图像处理中，输入的是质量低的图像，输出的是改善质量后的图像，常用的图像处理方法有图像增强、复原、编码、压缩等。 数字图像处理常用方法 图像变换：由于图像阵列很大，直接在空间域中进行处理，涉及计算量很大。因此，往往采用各种图像变换的方法，将空间域的处理转换为变换域处理，这样不仅可减少计算量，而且可获得更有效的处理。常见的有傅立叶变换（在频域中进行数字滤波处理）、沃尔什变换、离散余弦变换等间接处理技术。 图像编码压缩：图像编码压缩技术可减少描述图像的数据量（即比特数），以便节省图像传输、处理时间和减少所占用的存储器容量。压缩可以在不失真的前提下获得，也可以在允许的失真条件下进行。编码是压缩技术中最重要的方法，它在图像处理技术中是发展最早且比较成熟的技术。 图像增强和复原：图像增强和复原的目的是为了提高图像的质量，如去除噪声，提高图像的清晰度等。图像增强不考虑图像降质的原因，突出图像中所感兴趣的部分。如强化图像高频分量，可使图像中物体轮廓清晰，细节明显；如强化低频分量可减少图像中噪声影响。图像复原要求对图像降质的原因有一定的了解，一般讲应根据降质过程建立“降质模型”，再采用某种滤波方法，恢复或重建原来的图像。 图像分割：图像分割是数字图像处理中的关键技术之一。图像分割是将图像中有意义的特征部分提取出来，其有意义的特征有图像中的边缘、区域等，这是进一步进行图像识别、分析和理解的基础。 图像描述：图像描述是图像识别和理解的必要前提。作为最简单的二值图像可采用其几何特性描述物体的特性，一般图像的描述方法采用二维形状描述，它有边界描述和区域描述两类方法。对于特殊的纹理图像可采用二维纹理特征描述。随着图像处理研究的深入发展，已经开始进行三维物体描述的研究，提出了体积描述、表面描述、广义圆柱体描述等方法。 图像分类（识别）：图像分类（识别）属于模式识别的范畴，其主要内容是图像经过某些预处理（增强、复原、压缩）后，进行图像分割和特征提取，从而进行判决分类。图像分类常采用经典的模式识别方法，有统计模式分类和句法（结构）模式分类，近年来新发展起来的模糊模式识别和人工神经网络模式分类在图像识别中也越来越受到重视。]]></content>
      <categories>
        <category>Computer vision</category>
      </categories>
      <tags>
        <tag>Image processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[准确率，精确率，召回率和F1值]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-22-%E5%87%86%E7%A1%AE%E7%8E%87%EF%BC%8C%E7%B2%BE%E7%A1%AE%E7%8E%87%EF%BC%8C%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8CF1%E5%80%BC%2F</url>
    <content type="text"><![CDATA[机器学习(ML),自然语言处理(NLP),信息检索(IR)等领域,评估(Evaluation)是一个必要的 工作,而其评价指标往往有如下几点:准确率(Accuracy),精确率(Precision),召回率(Recall)和F1-Measure。 (注： 相对来说，IR 的 ground truth 很多时候是一个 Ordered List, 而不是一个 Bool 类型的 Unordered Collection，在都找到的情况下，排在第三名还是第四名损失并不是很大，而排在第一名和第一百名，虽然都是“找到了”，但是意义是不一样的，因此 更多可能适用于 MAP(下面会介绍) 之类评估指标。) 准确率、精确率、召回率在介绍准确率，精确率，召回率和F1值之前，我们先来看这样一个例子： 假设一个班级有100个学生，其中男生70人，女生30人。如下图，蓝色矩形表示男生，橙色矩形表示女生。又假设，我们不知道这些学生的性别，只知道他们的身高和体重。我们有一个程序(分类器)，这个程序可以通过分析每个学生的身高和体重，对这100个学生的性别分别进行预测。最后的预测结果为，60人为男生，40人为女生，(我们假设男生为正例，女生为负例)如下图。 TP：(实际为正例，预测也为正例) 实际为男生，预测为男生； FP：(实际为负例，预测为正例) 实际为女生，预测为男生； FN：(实际为正例，预测为负例) 实际为男生，预测为女生； TN：(实际为负例，预测也为负例) 实际为女生，预测为女生； 准确率(Accuracy) ＝ (TP + TN) / 总样本 ＝(40 + 10)/100 = 50%。 定义是: 对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。 精确率(Precision) ＝ TP / (TP + FP) = 40/60 = 66.67%。它表示：预测为正的样本中有多少是真正的正样本，它是针对我们预测结果而言的。Precision又称为查准率。 召回率(Recall) ＝ TP / (TP + FN) = 40/70 = 57.14% 。它表示：样本中的正例有多少被预测正确了， 它是针对我们原来的样本而言的。Recall又称为查全率。 总结：准确率就是找得对，召回率就是找得全 准确率、召回率、F1信息检索、分类、识别、翻译等领域两个最基本指标是召回率(Recall Rate)和准确率(Precision Rate)，召回率也叫查全率，准确率也叫查准率，下面我们主要看看在信息检索中的情况。概念公式: 召回率(Recall) = 系统检索到的相关文件 / 系统所有相关的文件总数准确率(Precision) = 系统检索到的相关文件 / 系统所有检索到的文件总数 图示如下： A：检索到的，也相关的 （搜到的也想要的） B：检索到的，但是不相关的 （搜到的但没用的） C：未检索到的，但却是相关的 （没搜到，然而实际上想要的） D：未检索到的，也不相关的 （没搜到也没用的） 注意：准确率和召回率是互相影响的，理想情况下肯定是做到两者都高，但是一般情况下准确率高、召回率就低，召回率低、准确率高。一般情况，用不同的阀值，统计出一组不同阀值下的精确率和召回率，如下图： 如果是做搜索，那就是保证召回的情况下提升准确率；如果做疾病监测、反垃圾，则是保准确率的条件下，提升召回率。 所以，在两者都要求高的情况下，可以用F1来衡量，计算公式如下。 MAP(mean Average Precision) 信息检索MAP是为解决P，R，F-measure的单点值局限性的。为了得到 一个能够反映全局性能的指标，可以看考察下图，其中两条曲线(方块点与圆点)分布对应了两个检索系统的准确率-召回率曲线 分析： 可以看出，虽然两个系统的性能曲线有所交叠但是以圆点标示的系统的性能在绝大多数情况下要远好于用方块标示的系统。 我们可以 发现一点，如果一个系统的性能较好，其曲线应当尽可能的向上突出。更加具体的，曲线与坐标轴之间的面积应当越大。 最理想的系统， 其包含的面积应当是1，而所有系统的包含的面积都应当大于0。这就是用以评价信息检索系统的最常用性能指标，平均准确率MAP其规范的定义如下:(其中P，R分别为准确率与召回率) ROC和AUC 分类识别ROC和AUC是评价分类器的指标，ROC的全名叫做Receiver Operating Characteristic。ROC关注两个指标 True Positive Rate ( TPR ) = TP / ( TP + FN) ，TPR代表能将正例分对的概率 False Positive Rate( FPR ) = FP / (FP + TN)，FPR代表将负例错分为正例的概率 在ROC 空间中，每个点的横坐标是FPR，纵坐标是TPR，这也就描绘了分类器在TP（真正的正例）和FP（错误的正例）间的trade-off。ROC的主要分 析工具是一个画在ROC空间的曲线——ROC curve。我们知道，对于二值分类问题，实例的值往往是连续值，我们通过设定一个阈值，将实例分类到正类或者负类（比如大于阈值划分为正类）。因此我们 可以变化阈值，根据不同的阈值进行分类，根据分类结果计算得到ROC空间中相应的点，连接这些点就形成ROC curve。ROC curve经过（0,0）（1,1），实际上(0, 0)和(1, 1)连线形成的ROC curve实际上代表的是一个随机分类器。一般情况下，这个曲线都应该处于(0, 0)和(1, 1)连线的上方。如图所示。 用ROC curve来表示分类器的performance很直观好用。可是，人们总是希望能有一个数值来标志分类器的好坏。于是Area Under roc Curve(AUC)就出现了。顾名思义，AUC的值就是处于ROC curve下方的那部分面积的大小。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>evaluation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛客笔试题之顺丰机器学习真题]]></title>
    <url>%2F%E7%89%9B%E5%AE%A2%E7%AC%94%E8%AF%95%E9%A2%98%2F2019-3-22-%E7%89%9B%E5%AE%A2%E7%AC%94%E8%AF%95%E9%A2%98%E4%B9%8B%E9%A1%BA%E4%B8%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9C%9F%E9%A2%98%2F</url>
    <content type="text"><![CDATA[昨天做了一套顺丰人工智能和机器学习的真题，下面是对其中一些知识点的总结。 Java中的String解析： 链表链表的特性，使其在某些操作上比数组更加高效。 增删不必挪动元素。当进行插入和删除操作时，链表操作的时间复杂度仅为O(1)。 无需实现估计空间。链表在内存中不是连续存储的，所以可以充分利用内存中碎片空间。 UDP与TCP TCP 面向有连接 可靠 面向字节流 数据无边界 速度慢 一对一 UDP 无连接 不可靠会丢包 面向报文 有边界 速度块 一对一或一对多 死锁产生必要条件： 互斥 请求与保持 循环等待 非剥夺 OSI七层模型OSI（Open System Interconnect），即开放式系统互联。 一般都叫OSI参考模型，是ISO（国际标准化组织）组织在1985年研究的网络互连模型。它定义了网络互连的七层框架：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层 TCP/IP五层协议和OSI的七层协议对应关系如下，在每一层都工作着不同的设备： 在每一层实现的协议也各不同，即每一层的服务也不同： 数据库索引 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性 当用户查询索引字段时，索引可以快速地执行检索操作，借助索引，在执行查询的时候不需要扫描整个表就可以快速地找到所需要的数据。 创建索引和维护索引要耗费时间、空间,当对表中的数据进行增加、删除和修改的时候,会降低数据的维护速度 Numpy解析：123456789101112131415161718192021222324import numpy as np'''numpy.repeat(a, repeats, axis=None)将a重复b次&gt;&gt;&gt; x = np.array([[1,2],[3,4]])&gt;&gt;&gt; np.repeat(x, 2)array([1, 1, 2, 2, 3, 3, 4, 4])&gt;&gt;&gt; np.repeat(x, 3, axis=1)array([[1, 1, 1, 2, 2, 2], [3, 3, 3, 4, 4, 4]])&gt;&gt;&gt; np.repeat(x, [1, 2], axis=0)array([[1, 2], [3, 4], [3, 4]])'''a = np.repeat(np.arange(5).reshape([1,-1]),10,axis = 0)+10.0b = np.random.randint(5, size= a.shape) # 生成[0,5)随机矩阵，大小和矩阵a相同c = np.argmin(a*b, axis=1) #矩阵a和b乘积，返回每行最小值位置b = np.zeros(a.shape) #与矩阵a相同大小的全零矩阵print("b变之前：",str(b))print("c的值：",str(c))b[np.arange(b.shape[0]), c] = 1 #将b中每一行的c位置处赋值为1print("b变之后：",str(b))print(b.shape) 输出结果：12345678910111213141516171819202122b变之前： [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]]c的值： [0 3 4 3 1 3 4 2 2 0]b变之后： [[1. 0. 0. 0. 0.] [0. 0. 0. 1. 0.] [0. 0. 0. 0. 1.] [0. 0. 0. 1. 0.] [0. 1. 0. 0. 0.] [0. 0. 0. 1. 0.] [0. 0. 0. 0. 1.] [0. 0. 1. 0. 0.] [0. 0. 1. 0. 0.] [1. 0. 0. 0. 0.]](10, 5) RF GBDT XgBoostRF、GBDT、XgBoost 三者都是集成学习中的方法，其中RF属于Bagging方法，GBDT和XgBoost属于Boosting方法。过段时间会专门写一篇有关Bagging和Boosting介绍的博文。 RF（Random Forest） 随机森林主要运用到的方法是bagging，采用Bootstrap的随机有放回的抽样，抽样出N份数据集，训练出N个决策树。然后根据N个决策树输出的结果决定最终结果（离散型的输出：取最多的类别，连续型的输出：取平均数）。 优势： 容易理解和解释 不需要太多的数据预处理工作 隐含地创造了多个联合特征，并能够解决非线性问题 随机森林模型不容易过拟合 自带out-of-bag (oob)错误评估功能 并行化容易实现 劣势： 不适合小样本，只适合大样本 精度较低 适合决策边界是矩形的，不适合对角线型的 GBDT 通过采用加法模型（即基函数的线性组合），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法。通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。一般选择cart tree且树的深度不会很深。 优势： 精度高 能处理非线性数据 能处理多特征类型 适合低维稠密数据 模型可解释性好 不需要做特征的归一化，可以自动选择特征 能适应多种损失函数 劣势： 不太适合并发执行 计算复杂度高 不适用高维稀疏特征 XgBoost 简单来说XgBoost是GBDT的一种高效实现，主要具有以下几个优势： 显式的把树模型复杂度作为正则项加到优化目标中。 实现了分裂点寻找近似算法。 可以并行执行 RF和GBDT的比较: 相同点： 都是由多棵树组成 最终的结果都是由多棵树一起决定 不同点： 组成RF的树可以是分类树，也可以是回归树；而GBDT只由回归树组成 组成RF的树可以并行生成；而GBDT只能是串行生成 对于最终的输出结果而言，RF采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来 RF对异常值不敏感，GBDT对异常值非常敏感 RF对训练集一视同仁，GBDT是基于权值的弱分类器的集成 RF是通过减少模型方差(variance)提高性能，GBDT是通过减少模型偏差(bias)提高性能 Boosting和Bagging的差别过拟合的模型，通常variance比较大，这时应该用bagging对其进行修正。欠拟合的模型，通常bias比较大，这时应该可以用boosting进行修正。使用boosting时， 每一个模型可以简单一些。 bagging中的模型是强模型，偏差低，方差高。目标是降低方差(variance)。在bagging中，每个模型的bias和variance近似相同，但是互相相关性不太高，因此一般不能降低bias，而一定程度上能降低variance。典型的bagging是random forest (RF)。 boosting中每个模型是弱模型，偏差高，方差低。目标是通过平均降低偏差(bias)。boosting的基本思想就是用贪心法最小化损失函数，显然能降低偏差，但是通常模型的相关性很强，因此不能显著降低variance。典型的Boosting是Adaboost，另外一个常用的并行Boosting算法是GBDT（gradient boosting decision tree）。这一类算法通常不容易出现过拟合。]]></content>
      <categories>
        <category>牛客笔试题</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>computer basis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习正则化之L0、L1与L2范数]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-21-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%AD%A3%E5%88%99%E5%8C%96%E4%B9%8BL0%E3%80%81L1%E4%B8%8EL2%E8%8C%83%E6%95%B0%2F</url>
    <content type="text"><![CDATA[最近刷题时，经常会遇到关于L1和L2范数的知识点，本文就其详细的分析记录一下。 前言我们常见的监督机器学习问题无非就是“minimizeyour error while regularizing your parameters”，也就是在规则化参数的同时要最小化误差。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。 如果参数太多，会导致我们的模型复杂度上升，容易过拟合，也就是我们的训练误差会很小。但训练误差小并不是我们的最终目标，我们的目标是希望模型的测试误差小，也就是能准确的预测新的样本。所以，我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能（也就是测试误差也小），而模型“简单”就是通过规则函数来实现的。另外，规则项的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑等等。 看待规则化的其他角度： 规则化符合奥卡姆剃刀(Occam&#39;s razor)原理,也就是在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型。 从贝叶斯估计的角度来看，规则化项对应于模型的先验概率。 规则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)。 一般来说，监督学习可以被看做最小化下面的目标函数：其中，第一项$L(y_{i},f(x_{i};w))$ 衡量我们的模型（分类或者回归）对第$i$个样本的预测值$f(x_{i};w)$和真实的标签$y_{i}$之前的误差。因为模型目的是拟合我们的训练样本，所以我们要求这一项最小，也就是要求我们的模型尽量的拟合我们的训练数据。但正如之前所说，我们不仅要保证训练误差最小，也更希望我们的模型测试误差小，所以我们需要加上第二项，也就是对参数$w$的规则化函数$\Omega(w)​$去约束我们的模型尽量的简单。 机器学习的大部分带参模型都和这个不但形似，而且神似。是的，其实大部分无非就是变换这两项而已。对于第一项Loss函数，如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是exp-Loss，那就是集成Boosting了；如果是log-Loss，那就是 Logistic Regression了；等等。不同的loss函数，具有不同的拟合特性，这个也得就具体问题具体分析的。本文主要关注第二项“规则项Ω(w)”。规则化函数$Ω(w)​$也有很多种选择,本文主要介绍L0、L1和L2范数。 L0范数与L1范数 L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。但是在大多数论文中，稀疏都是通过L1范数来实现的。这是因为L0和L1有着某种不寻常的关系。那我们再来看看L1范数是什么？它为什么可以实现稀疏？为什么大家都用L1范数去实现稀疏，而不是L0范数呢？ L1范数是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。 现在我们来分析为什么L1范数会使权值稀疏？有人可能会这样给你回答“它是L0范数的最优凸近似”。实际上，还存在一个更好的回答：任何的规则化算子，如果他$W_{i}=0$的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。W的L1范数是绝对值，$|w|$在$w=0$处是不可微，但这还是不够直观。这里因为我们需要和L2范数进行对比分析。 上面还有一个问题：既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。一句话总结：L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。 参数稀疏的好处： 特征选择(Feature Selection)： 大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，$x_{i}$的大部分元素（也就是特征）都是和最终的输出$y_{i}$没有关系或者不提供任何信息的，在最小化目标函数的时候考虑$x_{i}$这些额外的特征，虽然可以获得更小的训练误差（可能导致模型过拟合），但在预测新的样本时，这些没用的信息反而会被考虑(噪音)，从而干扰了对正确$y_{i}​$的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。 可解释性(Interpretability)： 另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：（一般为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的$w*$就只有很少的非零元素，例如只有5个非零的$wi$，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个$w_{i}$都非0，医生面对这1000种因素，累觉不爱。 L2范数除了L1范数，还有一种更受宠幸的规则化范数是L2范数:$ ||W||_2$。它也不逊于L1范数，它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。这用的很多，因为它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。至于过拟合是什么，上面也解释了，就是模型训练时候的误差很小，但在测试的时候误差很大，也就是我们的模型复杂到可以拟合到我们的所有训练样本了，但在实际预测新的样本的时候，准确率很低。通俗的讲就是应试能力很强，实际应用能力很差。擅长背诵知识，却不懂得灵活利用知识。例如下图所示： 上面的图是线性回归，下面的图是Logistic回归，也可以说是分类的情况。从左到右分别是欠拟合（underfitting，也称High-bias）、合适的拟合和过拟合（overfitting，也称High variance）三种情况。可以看到，如果模型复杂（可以拟合任意的复杂函数），它可以让我们的模型拟合所有的数据点，也就是基本上没有误差。对于回归来说，就是我们的函数曲线通过了所有的数据点，如上图右。对分类来说，就是我们的函数曲线要把所有的数据点都分类正确，如下图右。这两种情况很明显过拟合了。 为什么L2范数可以防止过拟合？回答这个问题之前，我们得先看看L2范数的实质。 L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。为什么越小的参数说明模型越简单？我也不懂，我的理解是：限制了参数很小，实际上就限制了多项式某些分量的影响很小（看上面线性回归的模型的那个拟合的图），这样就相当于减少参数个数。 一句话总结下：通过L2范数，我们可以实现了对模型空间的限制，从而在一定程度上避免了过拟合。 L2范数的好处： 从学习理论的角度来说，L2范数可以防止过拟合，提升模型的泛化能力。 从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。关于condition number，大家可以看文末的参考资料链接，里面讲的很详细。 L1和L2的差别L1让绝对值最小，L2让平方最小，为什么会有那么大的差别呢？这里给出两种几何上直观的解析： 下降速度： 我们知道，L1和L2都是规则化的方式，我们将权值参数以L1或者L2的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。而这个最小化就像一个下坡的过程，L1和L2的差别就在于这个“坡”不同，如下图：L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在0附近，L1的下降速度比L2的下降速度要快。所以会非常快得降到0。(此解释待商榷) 模型空间的限制（看的不是太懂）：实际上，对于L1和L2规则化的代价函数来说，我们可以写成以下形式： 也就是说，我们将模型空间限制在w的一个L1-ball 中。为了便于可视化，我们考虑两维的情况，在(w1, w2)平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为C的一个 norm ball 。等高线与 norm ball 首次相交的地方就是最优解： 可以看到，L1-ball 与L2-ball 的不同就在于L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置就会产生稀疏性，例如图中的相交点就有w1=0，而更高维的时候（想象一下三维的L1-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。 相比之下，L2-ball 就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1-regularization 能产生稀疏性，而L2-regularization 不行的原因了。 因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0（稀疏），而L2会选择更多的特征，这些特征都会接近于0（平滑）。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。 参考资料：机器学习中的范数规则化之（一）L0、L1与L2范数]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>regularization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛客笔试题之机器学习]]></title>
    <url>%2F%E7%89%9B%E5%AE%A2%E7%AC%94%E8%AF%95%E9%A2%98%2F2019-3-21-%E7%89%9B%E5%AE%A2%E7%AC%94%E8%AF%95%E9%A2%98%E4%B9%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[昨天做完了牛客网上的机器学习试题，下面是对一些错题的分析，并简要总结了一些机器学习中应该注意的知识点，过段时间会对其中的一些方法进行更加详细的分析介绍。题中打问号？代表该题答案存在争议，不一定准确。 过拟合问题解析：造成过拟合的原因主要有： 训练数据不足 训练模型过度导致模型非常复杂，泛化能力差 样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系； 权值学习迭代次数足够多(Overtraining)，拟合了训练数据中的噪声和训练样例中没有代表性的特征 选项A增加训练集可以解决训练数据不足的问题，防止过拟合 选项B对应使得模型的复杂度降低，防止过拟合 选项C类似主成分分析，降低数据的特征维度，使得模型复杂度降低，防止过拟合 选项D使得模型更加复杂化，会充分训练数据导致过拟合 条件概率解析：由条件概率公式可知： 先验概率未知解析： 朴素贝叶斯（NB）NB的核心在于它假设向量的所有分量之间是独立的。在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分。 分支定界法（branch and bound）解析： 基于核的机器学习算法解析： A EM算法，聚类算法 B 径向基核函数 C 线性判别分析 D 支持向量机核函数的本质就是将一个空间转化为另一个空间的变化，线性判别分析是把高纬空间利用特征值和特征向量转化到一维空间，核化的LDA模型是KFDA。 L1和L2范数解析： L1范数是指向量中各个元素的绝对值之和，也叫”系数规则算子（Lasso regularization）。它可以实现稀疏，通过将无用特征对应的参数W置为零实现。 L2范数是指向量各元素的平方和然后开方，用在回归模型中也称为岭回归（Ridge regression）。L2避免过拟合的原理是：让L2范数的规则项||W||2 尽可能小，可以使得W每个元素都很小，接近于零，但是与L1不同的是，不会等于0；这样得到的模型抗干扰能力强，参数很小时，即使样本数据x发生很大的变化，模型预测值y的变化也会很有限。 准确率、召回率及F1值解析：精准度和召回率是一对矛盾的度量，一般来说，精准度越高，召回率越低；召回率越高，精准度越低。 生成式模型和判别式模型生成式模型(Generative Model)与判别式模型(Discrimitive Model)是分类器常遇到的概念，它们的区别在于： 对于输入x，类别标签y： 生成式模型估计它们的联合概率分布P(x,y) 判别式模型估计条件概率分布P(y|x) 生成式模型可以根据贝叶斯公式得到判别式模型，但反过来不行。 公式上看生成模型： 学习时先得到 P(x,y)，继而得到 P(y|x)。预测时应用最大后验概率法（MAP）得到预测类别 y。判别模型： 直接学习得到P(y|x)，利用MAP得到 y。或者直接学得一个映射函数 y=f(x)。 直观上看生成模型： 关注数据是如何生成的判别模型： 关注类别之间的差别例子：假如你的任务是识别一个语音属于哪种语言。例如对面一个人走过来，和你说了一句话，你需要识别出她说的到底是汉语、英语还是法语等。那么你可以有两种方法达到这个目的： 学习每一种语言，你花了大量精力把汉语、英语和法语等都学会了，我指的学会是你知道什么样的语音对应什么样的语言。然后再有人过来对你说，你就可以知道他说的是什么语音. 不去学习每一种语言，你只学习这些语言之间的差别，然后再判断（分类）。意思是指我学会了汉语和英语等语言的发音是有差别的，我学会这种差别就好了。那么第一种方法就是生成方法，第二种方法是判别方法。 常见的判别式模型： 逻辑回归 Logistic Regression 支持向量机 SVM 神经网络 NN 传统神经网络 Traditional Neural Networks 邻近取样 Nearest Neighbor 条件随机场 CRF 线性判别分析 Linear Discriminant Analysis 提升算法 Boosting 线性回归 Linear Regression 高斯过程 Gaussian Process 分类回归树 Classification and Regression Tree (CART) 区分度训练 常见的生成式模型： 高斯 Gaussians 朴素贝叶斯 Naive Bayes 混合多项式 Mixtures of Multinomials 混合高斯模型 Mixtures of Gaussians 多专家模型 Mixtures of Experts 隐马尔科夫模型 HMM S型信念网络 Sigmoidal Belief Networks 贝叶斯网络 Bayesian Networks 马尔科夫随机场 Markov Random Fields 潜在狄利克雷分配 Latent Dirichlet Allocation(LDA) 判别式分析 K近邻 KNN 深度信念网络 DBN 聚类算法影响因素解析：聚类的目标是使同一类对象的相似度尽可能地大，不同类对象之间的相似度尽可能的小。聚类分析算法主要可以分为： 划分法（Partitioning Methods） 层次法（Hierarchical Me thods） 基于密度的方法（Density-Based Methods） 基于网格的方法（Grid-Based M ethods） 基于模型的方法（Model-Based Methods） 谱聚类（Spectral Clustering） C大约说的是度量方式，例如KMeans 可以用欧式距离啊，也可用其他的距离，这也是分类准则。（C正确） 不过个人觉得C有歧义；特征选取的差异会影响聚类效果（A正 确）。聚类的目标是使同一类对象的相似度尽可能地大，因此不同的相似度测度方法对聚类结 果有着重要影响（B正确）。由于聚类算法是无监督方法，不存在带类别标签的样本，因此， D选项不是聚类算法的输入数据。 隐马尔科夫模型解析： 前向后向算法解决的是一个评估问题，即给定一个模型，求某些特定观测序列的概率，用于评估该序列最匹配的模型。 Baum-Welch算法解决的是一个模型训练问题（学习），即参数估计，是一种无监督的训练方法，主要通过EM迭代实现。 维特比算法解决的是一个预测问题，通信中的解码问题，即给定一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。比如通过海藻变化（输出序列）来观测天气（状态序列）。 AdaBoost及SVM解析： 卷积大小计算解析： 特征选择方法解析： 缺失值处理方法解析：由于调查、编码和录入误差，数据中可能存在一些无效值和缺失值，需要给予适当的处理。常用的处理方法有：估算，整例删除，变量删除和成对删除。 估算(estimation)。最简单的办法就是用某个变量的样本均值、中位数或众数代替无效值和缺失值。这种办法简单，但没有充分考虑数据中已有的信息，误差可能较大。 整例删除(casewise deletion)是剔除含有缺失值的样本。由于很多问卷都可能存在缺失值，这种做法的结果可能导致有效样本量大大减少，无法充分利用已经收集到的数据。因此，只适合关键变量缺失，或者含有无效值或缺失值的样本比重很小的情况。 变量删除(variable deletion)。如果某一变量的无效值和缺失值很多，而且该变量对于所研究的问题不是特别重要，则可以考虑将该变量删除。这种做法减少了供分析用的变量数目，但没有改变样本量。 成对删除(pairwise deletion)是用一个特殊码(通常是9、99、999等)代表无效值和缺失值，同时保留数据集中的全部变量和样本。但是，在具体计算时只采用有完整答案的样本，因而不同的分析因涉及的变量不同，其有效样本量也会有所不同。 信息增益解析：本题主要考察信息增益的计算方式，具体可参考我之前博客决策树：$$Gain(A) = Info(D) - InfoA(D)$$其中Info表示信息熵，计算公式如下：所以可以计算出各特征的信息增益如下所示： 置信度及支持度解析：置信度计算规则为： 同时购买商品A和商品B的交易次数/购买了商品A的次数支持度计算规则为： 同时购买了商品A和商品B的交易次数/总的交易次数 求解线性不可分方法解析：伪逆法： 径向基（RBF）神经网络的训练算法，径向基解决的就是线性不可分的情况。感知器算法： 线性分类模型。H-K算法： 在最小均方误差准则下求得权矢量，二次准则解决非线性问题。势函数法： 势函数非线性。 时间序列模型解析：AR模型：自回归模型，是一种线性模型MA模型：移动平均法模型，其中使用趋势移动平均法建立直线趋势的预测模型ARMA模型：自回归滑动平均模型，拟合较高阶模型GARCH模型：广义回归模型，对误差的方差建模，适用于波动性的分析和预测 PMF PDF CDF解析： CRF（条件随机场）解析： HMM（隐马尔科夫模型）：HMM是一种产生式模型，定义了联合概率分布p(x,y) ，其中x和y分别表示观察序列和相对应的标注序列的随机变量。它包含2个基本假设： 后一个隐藏状态只依赖于前一个隐藏状态。 观测值之间相互独立，观测值只依赖于该时刻的马尔科夫链的隐状态。缺点：1. HMM只依赖于每一个状态和它对应的观察对象：2、目标函数和预测目标函数不匹配： MEMM（最大熵马尔科夫模型）：最大熵马尔科夫模型把HMM模型和Maximum Entropy模型的优点集合成一种生成模型（Generative Model）。克服了观察值之间严格独立产生的问题，但仍存在标注偏置问题（Label bias problem）。 CRF（条件随机场）：CRF模型解决了标注偏置问题，去除了HMM中两个不合理的假设，当然，模型相应得也变复杂了。MEMM是局部归一化，CRF是全局归一化。CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢总结：三者都是NLP（自然语言处理）中的基础语言模型。 线性回归描述解析： K-L与PCA解析：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。 数据不均衡解析： 重采样。A可视作重采样的变形。改变数据分布消除不平衡，可能导致过拟合。 欠采样。C的方案 提高少数类的分类性能，可能丢失多数类的重要信息。 权值调整。D方案也是其中一种方式。 HK算法与感知器解析： 时间序列算法模型解析：常见的时间序列算法模型有 移动平均法 (MA) 简单移动平均法 自回归模型(AR) 自回归滑动平均模型(ARMA) GARCH模型 指数平滑法ABD都是一些关于股票涨跌的分析方法。 SVM核函数解析：SVM核函数包括：线性核函数、多项式核函数、径向基核函数（RBF）、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数。 Logit与SVM解析：A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。A错误B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化，正确。D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。（个人觉得但最好是加上正则项吧） LDA与PCALDA（线性判别分析）用于降维，和PCA（主成分分析）有很多相同，也有很多不同的地方，因此值得好好的比较一下两者的降维异同点。相同点： 两者均可以对数据进行降维。 两者在降维时均使用了矩阵特征分解的思想。 两者都假设数据符合高斯分布。 不同点： LDA是有监督的降维方法，而PCA是无监督的降维方法 LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。 LDA除了可以用于降维，还可以用于分类。 LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向 常见的数据降维方法线性 LDA（线性判别分析） PCA（主成分分析） 非线性 核方法（KPCA、KFDA等） 二维化 流行学习（LLE、LPP、ISOMap等） 其他方法： 神经网络（自编码） 聚类3.小波分析 LASSO（参数压缩） SVD奇异值分解 线性分类器准则解析：线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。 感知器准则函数：代价函数J=-(W*X+w0)，分类的准则是最小化代价函数。感知器是神经网络（NN）的基础。 SVM：支持向量机也是很经典的算法，优化目标是最大化间隔（margin），又称最大间隔分类器，是一种典型的线性分类器。（使用核函数可解决非线性问题） Fisher准则：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。贝叶斯分类器：一种基于统计方法的分类器，要求先了解样本的分布特点（高斯、指数等），所以使用起来限制很多。在满足一些特定条件下，其优化目标与线性分类器有相同结构（同方差高斯分布等），其余条件下不是线性分类。]]></content>
      <categories>
        <category>牛客笔试题</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习框架的对比及分析]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-3-19-%E4%B8%89%E5%A4%A7%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E7%9A%84%E5%AF%B9%E6%AF%94%E5%8F%8A%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文主要介绍了现在主流的三大深度学习框架Tensorflow、Caffe和Pytorch的组成结构，并对其特点进行了简要分析。 Tensorflow TensorFlow是一个使用数据流图进行数值计算的开源软件库。图中的节点表示数学运算，而图边表示节点之间传递的多维数据阵列（又称张量）。灵活的体系结构允许使用单个API将计算部署到服务器或移动设备中的某个或多个CPU或GPU。它是一个静态图模型，一旦图被构建后，结构不变，每次输入不同的数据。Tensorflow一般先定义各种变量,然后建立一个计算图(数据流图),计算图指定了各个变量之间的计算关系,若此时对计算图进行编译,没有任何输出,计算图还是一个空壳,只有把需要运算的输入放入后,才会在模型中形成数据流，形成输出。Tensorflow中的运算都要放在图中,图中的运算进行发生在回话启动后(session),给节点填充数据,然后进行运算,关闭会话,最后结束运算。 张量 所有的数据都通过张量的形式来表示，它可以被简单的理解为多为数组。张量的使用主要可以分为2大类： 对中间计算结果的引用 当计算图构造完成后，张量可以用来获取计算结果 OP操作 计算图中每一个节点代表着一个操作(operation,Op),一般用来表示施加的的数学运算,也可以表示数据输入的起点以及输出的终点,或者是读取/写入持久变量(persistent veriable)的终点,Op也可用于状态初始化的任务。 类比： 一个神经元有多个输入，一个或者多个输出。这里的OP可以看作神经元，tensor可以看作输入的数据。 计算图中的边 计算图中的边包含这两种关系:数据依赖和控制依赖。(不需要对计算图中的边进行定义,因为在tensorflow中创建节点是已包含了相应的Op完成计算所需的全部输入,tensorflow会自动绘制必要的连接) 。 实线的边代表着数据依赖：标识连接的两个节点之间有tensor的流动(传递) 虚线的边代表着控制依赖：用于控制操作的运行,确保发生前关系, 连接的节点之间没有tensor的传递,但是源节点必须在目的节点执行前完成执行。 TensorFlow程序的阶段 TensorFlow程序通常被组织成一个构建阶段和一个执行阶段。在构建阶段，op 的执行步骤被描述成一个图。在执行阶段，使用会话执行图中的op。 阶段一：如何构建图？ 构建图从创建op开始。有些op的创建是不需要input的，比如Constant。这样的op被成为源op（source op）。 在python中op对象是由op构造器（ops constructors）创建的。op构造器创建一个op对象时可以传递一个源op作为待构造op对象的输入。 op对象被op构造器创建后是作为一个node加入到graph中的。TensorFlow Python 库有一个默认图 (default graph), op 构造器可以为其增加节点。 总结：因为graph是由op对象组成的，所以构建图的过程其实就是创建op对象的过程，以及如果将这些个op对象连接起来（比如某个op对象作为另外某个op对象的输入）的过程。 阶段二：图构建好了，如何执行？ 因为graph需要在session中启动。所以为了启动一个graph，第一步就是创建session对象。 sessoin对象创建的时候如果不制定graph，则使用默认图（default graph）。 Variable及Constant 变量用于维护图执行过程中的状态信息。通常会将一个统计模型中的参数表示为一组变量。 例如, 你可以将一个神经网络的权重作为一个tensor存储在某个变量中。在训练过程中, 通过重复运行训练图，更新这个 tensor。 tensor存储在Constants或者Variables。就像数据可以放在常量和变量中一样。放在变量中的数据是可以修改的，放在常量中的数据是不可以修改的。 搭建神经网络步骤 定义添加神经层的函数； 准备训练的数据； 定义节点准备接收数据； 定义神经层：隐藏层和预测层； 定义loss表达式； 选择optimizer使loss达到最小； 对所有变量进行初始化，通过sess.run optimizer，迭代多次进行学习。 优势 TensorFlow有内置的TF.Learn和TF.Slim等上层组件可以帮助快速地设计新网络，并且兼容Scikit-learn estimator接口，可以方便地实现evaluate、grid search、cross validation等功能。同时TensorFlow不只局限于神经网络，其数据流式图支持非常自由的算法表达，当然也可以轻松实现深度学习以外的机器学习算法。事实上，只要可以将计算表示成计算图的形式，就可以使用TensorFlow。用户可以写内层循环代码控制计算图分支的计算，TensorFlow会自动将相关的分支转为子图并执行迭代运算。TensorFlow也可以将计算图中的各个节点分配到不同的设备执行，充分利用硬件资源。 在可视化方面，Tensorboard非常棒。该工具包含在TensorFlow里，它对于调试和比较不同的训练过程非常有用。例如，在训练模型的时候，你可以在调整某些超参数之后再训练一遍。两次运行过程可以同时显示在Tensorboard上，以显示它们之间存在的差异。Tensorboard能够： 显示模型图 绘制标量变量 使分布和直方图可视化 使图像可视化 使嵌入可视化 播放音频Tensorboard能够显示通过tf.summary模块收集的各种摘要。我们将为前面提到的那个指数例子定义摘要操作，并使用tf.summary.FileWriter将其保存到磁盘上。执行tensorboard --logdir=./tensorboard以启动Tensorboard。这个工具对于云实例来说非常方便，因为它是一个webapp。 Caffe优势及劣势Caffe是一个清晰而高效的深度学习框架，主要优势为： 上手容易，网络结构都是以配置文件形式定义，不需要用代码设计网络 训练速度快，组件模块化，可以方便的拓展到新的模型和学习任务上 拥有大量的训练好的经典模型（AlexNet、VGG、Inception）乃至其他state-of-the-art（ResNet等）的模型，收藏在它的Model Zoo劣势： 但是Caffe最开始设计时的目标只针对于图像，没有考虑文本、语音或者时间序列的数据，因此Caffe对卷积神经网络的支持非常好，但是对于时间序列RNN，LSTM等支持的不是特别充分。 Caffe 的配置文件不能用编程的方式调整超参数，也没有提供像 Scikit-learn 那样好用的 estimator 可以方便地进行交叉验证、超参数的 Grid Search 等操作。 组成Caffe由低到高依次把网络中的数据抽象成Blob, 各层网络抽象成Layer ，整个网络抽象成Net，网络模型的求解方法抽象成Solver。 Blob: 表示网络中的数据，包括训练数据，网络各层自身的参数，网络之间传递的数据都是通过Blob来实现的，同时Blob数据也支持在CPU与GPU上存储，能够在两者之间做同步。 Layer:对神经网络中各种层的抽象，包括卷积层和下采样层，还有全连接层和各种激活函数层等。同时每种Layer都实现了前向传播和反向传播，并通过Blob来传递数据。 Net: 是对整个网络的表示，由各种Layer前后连接组合而成，也是所构建的网络模型。 Solver: 定义了针对Net网络模型的求解方法，记录网络的训练过程，保存网络模型参数，中断并恢复网络的训练过程。自定义Solver能够实现不同的网络求解方式。 核心概念Caffe 的核心概念是Layer，每一个神经网络的模块都是一个Layer。Layer 接收输入数据，同时经过内部计算产生输出数据。设计网络结构时，只需要把各个Layer 拼接在一起构成完整的网络（通过写 protobuf配置文件定义）。比如卷积的Layer，它的输入就是图片的全部像素点，内部进行的操作是各种像素值与Layer 参数的 convolution 操作，最后输出的是所有卷积核filter 的结果。每一个Layer 需要定义两种运算，一种是正向（forward）的运算，即从输入数据计算输出结果，也就是模型的预测过程；另一种是反向（backward）的运算，从输出端的gradient 求解相对于输入的gradient，即反向传播算法，这部分也就是模型的训练过程。实现新Layer 时，需要将正向和反向两种计算过程的函数都实现, 这部分计算需要用户自己写 C++或者 CUDA （当需要运行在 GPU 时）代码，较为困难。 Pytorch新颖处不同于tensorflow的静态图，pytorch是一个动态的框架，这就和python的逻辑是一样的，对变量做任何操作都是灵活的。 组成Pytorch的设计追求最少的封装，不像tensorflow中充斥着session、graph、operation、variable、layer等全新概念。Pytorch的设计遵循tensor—&gt;variable(autograd) —&gt;nn.Model三个由低到高的抽象层次，分别代表高维数组（张量）、自动求导（变量）和神经网络（层/模块），三者紧密联系，可以同时进行修改和操作。 tensor variable nn.Model 优势及劣势优势： 动态图的思想直观明了，更符合人的思考过程。动态图的方式可以使得我们可以任意修改前向传播，还可以随时查看变量的值。如果静态图框架比作C++,每次运行都要编译（sess.run）,那么动态图框架就是python，动态执行，可以交互式查看修改。 动态图调试更容易，pytorch中，代码报错的地方，一般就是你写错代码的地方，而静态图需要先根据你的代码生成Graph对象，然后在session.run()时报错，找到错误很难。 劣势： PyTorch并没有一个类似于Tensorboard的工具，但有一个可以将Tensorboard集成进来的工具。或者，也可以免费使用标准绘图工具：matplotlib和seaborn。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>tensorflow</tag>
        <tag>architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛客笔试题之Python]]></title>
    <url>%2F%E7%89%9B%E5%AE%A2%E7%AC%94%E8%AF%95%E9%A2%98%2F2019-3-19-%E7%89%9B%E5%AE%A2%E7%AC%94%E8%AF%95%E9%A2%98%E4%B9%8Bpython%2F</url>
    <content type="text"><![CDATA[这几天做完了牛客网上的Python试题，下面是对一些错题的分析，并总结了一些python中应该注意的知识点。 字符串比较解析：a,b为字符串不可变类型，所以指向相同地址，所以 a is bis：指地址相同==: 内容相同a+b:字符串连接为’123123’ LEGB规则解析：Python一切皆对象，所以在Python中变量名是字符串对象。Python的命名空间是一个字典，字典内保存了变量名称与对象之间的映射关系，因此，查找变量名就是在命名空间字典中查找键-值对。LEGB就是用来规定命名空间查找顺序的规则。LEGB规定了查找一个名称的顺序为：local–&gt;enclosing function locals–&gt;global–&gt;builtinLocal: 即函数内部命名空间；Enclosing function locals: 外部嵌套函数的名字空间module(文件本身)：Global(module): 函数定义所在模块（文件）的名字空间Builtin: Python内置模块的名字空间 Set解析：集合（set）是一个无序的不重复元素序列。可以使用大括号 { } 或者 set() 函数创建集合，注意：创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典。本题中列表转集合，集合没有重复元素。 转义字符解析：python里面%d表数字，%s表示字符串，%%表示一个%；单引号内嵌套单引号需要转义字符/;单引号内嵌套双引号不需要嵌套；双引号内嵌套双引号需要转义字符/；双引号内引用单引号不需要转义字符； 类实例关系解析：isinstance（object，classinfo）: 用于判断object是否是classinfo的一个实例，或者object是否是classinfo类的子类的一个实例，如果是返回True.issubclass（class，classinfo）: 用于判断class是否是classinfo类的子类，如果是返回True. new和init的区别解析： init是当实例对象创建完成后被调用的，然后设置对象属性的一些初始值。 new是在实例创建之前被调用的，因为它的任务就是创建实例然后返回该实例，是个静态方法。 new在init之前被调用，new的返回值（实例）将传递给init方法的第一个参数，然后init给这个实例设置一些参数。 字典解析：字典是python中唯一的映射类型，阐述了键与键值之间的对应关系。字典中键必须是唯一的。列表中的项目包括在方括号中。列表是可变的数据类型（可以增加或删除项目）。所以，列表中的项目不能用来作为字典的键。 浅拷贝和深拷贝解析见上图中注释 try else finally解析：try的语句出现异常才会执行except后的语句，如果正常，则执行完try后执行else。另外，finally语句不管有无异常都会执行。所以上图中答案为4。 name解析： name定义在一个模块中，当解释器执行这个py文件时，name的值就为main； 当这个模块被引用即被其他模块import时，name的值就是模块名，也就是运行的py文件名。 闭包解析：在函数中可以定义另一个函数时，如果内部的函数引用了外部的函数的变量，则可能产生闭包。所以若a=2,b=3,则程序运行值为8。 装饰器解析见上图 大小比较类似元组、字符串、列表这类格式，在进行两者之间的比较时，先从第一个元素开始比较 ASCII 码值大小，如果相等，则依次向后比较，如果全部相等，则比较数量大小。ASCII 码值大小：数字： 0-9: 48-57字母：A-Z：65-90.a-z： 97-122 布尔值所有标准对象均可以用于布尔测试，下列对象的布尔值是False:• None• False• 所有值为零的数：0（整型），（浮点型），0L（长整型），0.0+0.0j(复数)• “”（空字符串），[ ] (空列表)， （）（空元祖），{} (空字典) 语言类型 语言特性解释性语言的特性是非独立和效率低。Python是解释性语言，在此以Python举例。非独立性： Python代码解释执行结果依赖于IDLE的版本，其中大版本有Python2和Python3之分，Python2最经典的版本为Python2.7，Python3有Python3.4，Python3.6等等。效率低： 由于Python是解释性语言，动态编译，直到代码执行时，才加以解释，相比于编译型语言，可以生成编译代码，执行效率低。 三元运算符$max = x &gt; y ? x : y$ Java和 C中正确，在Python中的三元运算符不是这样的，是$max=x \ if \ x&gt;y \ else\ y​$ 标识符python标识符可以使用下划线 字母 数字组成,但是数字不允许作为标识符的开头出现。 线程协程线程由操作系统控制，协程由程序自身控制。 编译及解码 Python中字符串编译的过程：gbk=&gt;unicode=&gt;utf16=&gt;url 解码 字符串解码顺序为：url解码=&gt;utf16=&gt;unicode=&gt;gbk map函数map() 会根据提供的函数对指定序列做映射。第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表。例如：1234567891011&gt;&gt;&gt;def square(x) : # 计算平方数... return x ** 2... &gt;&gt;&gt; map(square, [1,2,3,4,5]) # 计算列表各个元素的平方[1, 4, 9, 16, 25]&gt;&gt;&gt; map(lambda x: x ** 2, [1, 2, 3, 4, 5]) # 使用 lambda 匿名函数[1, 4, 9, 16, 25] # 提供了两个列表，对相同位置的列表数据进行相加&gt;&gt;&gt; map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])[3, 7, 11, 15, 19] 逻辑运算符 数据类型 结束符C语言中字符串使用‘\0’作为结束符以防止越界，但python中字符串其实是一个固定长度的字符数组，并不需要结束符。 math.floor函数及除法 Python3 中math.floor() 函数的返回值应为整型，而Python2 的 math.floor() 函数返回值是浮点型。 Python2 中除法默认向下取整，为整型；Python3 中的除法为正常除法，会保留小数位，为浮点型。 Python 中万物皆为对象，函数也不例外，函数作为对象可以赋值给一个变量、可以作为元素添加到集合对象中、可作为参数值传递给其它函数，还可以当做函数的返回值。 变量 Python 是弱类型脚本语言，变量就是变量，没有特定类型，因此不需要声明。 但每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 用 del 语句可以释放已创建的变量（已占用的资源）。 切片slicePython 中 切片（Slice）功能的理解：L[start : stop [ : step]]start 默认值是 0；stop 默认值为 L 的长度；step 默认值是 1。 命名方式]]></content>
      <categories>
        <category>牛客笔试题</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之K-means聚类]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-14-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8BK-means%2F</url>
    <content type="text"><![CDATA[聚类(Clustering)，就是将相似的事物聚集在一 起，而将不相似的事物划分到不同的类别的过程，是数据分析之中十分重要的一种手段。与此前介绍的决策树，支持向量机等监督学习不同，聚类算法是非监督学习(unsupervised learning )，在数据集中，并不清楚每条数据的具体类别。 算法K-means 算法是数据挖掘十大经典算法之一。由于该算法的效率高，所以在对大规模数据进行聚类时被广泛应用。目前，许多算法均围绕着该算法进行扩展和改进。 k-means 算法接受一个参数 k ，表示将数据集中的数据分成 k 个聚类。在同一个聚类中，数据的相似度较高；而不同聚类的数据相似度较低。 算法的步骤：1. 选择任意 k 个数据，作为各个聚类的质心，（质心也可以理解为中心的意思），执行步骤 2；2. 对每个样本进行分类，将样本划分到最近的质心所在的类别（欧氏距离），执行步骤 3；3. 取各个聚类的中心点作为新的质心，执行步骤 2 进行迭代。迭代结束的条件：1. 当新的迭代后的聚类结果没有发生变化；2. 当迭代次数达到预设的值。算法流程图： 实例分析有如下4种药物，我们要根据其2个特征值对其进行分类，事先并不知道它们属于何种类别。聚类后分为2类（1 和 2） 按照之前的算法流程，我们将4种药划分为了2类，聚类过程如下： 代码实现本部分我们将使用和上面实例分析中一致的数据，采用2种方法实现k-means聚类。 自己实现代码主要包含4个小方法，分别是： shouldStop()：聚类迭代的终止条件 updateLabels()：更新迭代后数据的类标签 getLabelFromClosestCenterpoints()：计算各数据到中心点的距离，选取最近距离更新数据类标签 getCenterpoints()：根据聚类结果选取新的中心点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import numpy as npdef kmeans(x,k,maxIt): ''' :param x: 待分类数据 :param k: 最终分为几类 :param maxIt: 最大迭代次数 :return: 分好类的数据 ''' numpoints,numDim = x.shape #数据行数和列数 dataSet = np.zeros((numpoints,numDim+1)) # 创建一个新数组存储分类好的数据 dataSet[:,:-1] = x # 将数据x赋值给dataSet的前n-列 centerpoints = dataSet[np.random.randint(numpoints,size=k),:] # 随机选取k个中心点 centerpoints = dataSet[0:2, :] # 强制选取前2条数据作为中心点，为了对照实例分析 centerpoints[:,-1] = range(1,k+1) # 为选好的中心点数据打上标签 iterations = 0 # 迭代次数 oldCenterpoints = None # 调用函数循环迭代，实现聚类 while not shouldStop(oldCenterpoints,centerpoints,iterations,maxIt): # 输出每次迭代的聚类过程 print("iterations: \n",iterations) print("dataSet: \n",dataSet) print("centerpoints: \n",centerpoints) # 将原始中心点复制存储，方便迭代完后，比较新旧中心点是否发生变化 oldCenterpoints =np.copy(centerpoints) iterations += 1 # 调用方法更新每条数据的类标签 updateLabels(dataSet,centerpoints) # 根据每一次迭代后的聚类结果，重新选取新的中心点 centerpoints = getCenterpoints(dataSet,k) return dataSet# 聚类迭代的终止条件def shouldStop(oldCenterpoints,centerpoints,iterations,maxIt): ''' :param oldCenterpoints: 迭代前的中心点 :param centerpoints: 迭代后的中心点 :param iterations: 当前迭代次数 :param maxIt: 最大迭代次数 :return: True或False ''' if iterations &gt; maxIt: # 超出设定好的最大迭代次数 return True return np.array_equal(oldCenterpoints,centerpoints) # 判断迭代前后中心点是否发生了变化# 更新迭代后数据的类标签def updateLabels(dataSet,centerpoints): ''' :param dataSet: 数据 :param centerpoints: 中心点 ''' numpoints,numDim = dataSet.shape for i in range(0,numpoints): # 调用方法循环更新数据的类标签 dataSet[i,-1] = getLabelFromClosestCenterpoints(dataSet[i,:-1],centerpoints)# 根据计算各数据到中心点的距离，选取最近距离更新数据类标签def getLabelFromClosestCenterpoints(dataSetRow,centerpoints): ''' :param dataSetRow: 待更新类标签的数据 :param centerpoints: 中心点 :return: 数据新的类标签 ''' label = centerpoints[0,-1] # 选取初始类标签 minDist = np.linalg.norm(dataSetRow - centerpoints[0,:-1]) # 计算和当前中心点的距离 # 循环计算数据和每个中心点的距离，选取最近的更新类标签 for i in range(1,centerpoints.shape[0]): dist = np.linalg.norm(dataSetRow - centerpoints[i,:-1]) # 计算距离 if dist &lt; minDist: minDist = dist label = centerpoints[i,-1] print("minDist: ",minDist) return label# 根据聚类结果选取新的中心点def getCenterpoints(dataSet,k): ''' :param dataSet: 数据 :param k: 最终分为几类 :return: 新的中心点 ''' result = np.zeros((k,dataSet.shape[1])) for i in range(1,k+1): oneCluster = dataSet[dataSet[:,-1]==i,:-1] # 同类数据按行求均值，算出新的中心点 result[i-1,:-1] = np.mean(oneCluster,axis=0) result[i-1,-1] = i # 打上标签 return result# 创建测试数据x1 = np.array([1,1])x2 = np.array([2,1])x3 = np.array([4,3])x4 = np.array([5,4])test_x = np.vstack((x1,x2,x3,x4)) #沿着列方向将矩阵堆叠起来result = kmeans(test_x,2,10)print("final result: \n",result) 程序运行结果如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647iterations: 0dataSet: [[1. 1. 1.] [2. 1. 2.] [4. 3. 0.] [5. 4. 0.]]centerpoints: [[1. 1. 1.] [2. 1. 2.]]minDist: 0.0minDist: 0.0minDist: 2.8284271247461903minDist: 4.242640687119285iterations: 1dataSet: [[1. 1. 1.] [2. 1. 2.] [4. 3. 2.] [5. 4. 2.]]centerpoints: [[1. 1. 1. ] [3.66666667 2.66666667 2. ]]minDist: 0.0minDist: 1.0minDist: 0.4714045207910319minDist: 1.885618083164127iterations: 2dataSet: [[1. 1. 1.] [2. 1. 1.] [4. 3. 2.] [5. 4. 2.]]centerpoints: [[1.5 1. 1. ] [4.5 3.5 2. ]]minDist: 0.5minDist: 0.5minDist: 0.7071067811865476minDist: 0.7071067811865476final result: [[1. 1. 1.] [2. 1. 1.] [4. 3. 2.] [5. 4. 2.]] 通过比较，可以发现结果和我们在实例分析中的一致。 Sklearn实现1234567891011121314from sklearn import clusterimport numpy as np# 创建测试数据x1 = np.array([1,1])x2 = np.array([2,1])x3 = np.array([4,3])x4 = np.array([5,4])test_x = np.vstack((x1,x2,x3,x4)) #沿着列方向将矩阵堆叠起来sk = cluster.KMeans(2)sk.fit(test_x)print("中心点：\n",sk.cluster_centers_)print("类别：\n",sk.labels_) 程序运行结果如下：12345中心点： [[4.5 3.5] [1.5 1. ]]类别： [1 1 0 0] 最后的中心点一致，也成功分为了2类 优缺点优点： 速度快，复杂度低，为 O(Nkq)，N是数据总量，k是类别数，q是迭代次数。一般来讲k、q会比N小得多，那么此时复杂度相当于O(N) ，在各种算法中是算很小的； 原理简单，易于理解。 缺点： 对异常点敏感； 局部最优解而不是全局优，(分类结果与初始点选取有关);不能发现非凸形状的聚类。 K-means++算法2007年由D. Arthur等人提出的K-means++算法在k-means的基础上做了进一步的改进。可以直观地将这改进理解成这K个初始聚类中心相互之间应该分得越开越好。整个算法的描述如下图所示： 下面结合一个简单的例子说明K-means++是如何选取初始聚类中心的。数据集中共有8个样本，分布以及对应序号如下图所示： 假设经过图2的步骤一后6号点被选择为第一个初始聚类中心，那在进行步骤二时每个样本的D(x)和被选择为第二个聚类中心的概率如下表所示： 其中的$P(x)$就是每个样本被选为下一个聚类中心的概率。最后一行的$Sum$是概率$P(x)$的累加和，用于轮盘法选择出第二个聚类中心。方法是随机产生出一个0~1之间的随机数，判断它属于哪个区间，那么该区间对应的序号就是被选择出来的第二个聚类中心了。例如1号点的区间为[0,0.2)，2号点的区间为[0.2, 0.525)。 从上表可以直观的看到第二个初始聚类中心是1号，2号，3号，4号中的一个的概率为0.9。而这4个点正好是离第一个初始聚类中心6号点较远的四个点。这也验证了K-means的改进思想：即离当前已有聚类中心较远的点有更大的概率被选为下一个聚类中心。可以看到，该例的K值取2是比较合适的。当K值大于2时，每个样本会有多个距离，需要取最小的那个距离作为$D(x)​$。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>k-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回归中的相关度和决定系数]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-13-%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%2F</url>
    <content type="text"><![CDATA[训练集中可能是有若干维度的特征。但有时并不是所有特征都是有用的，有的特征其实和结果并没有关系。因此需要一个能衡量自变量和因变量之间的相关度。 皮尔逊相关系数皮尔逊相关系数(Pearson correlation coefficient），是用于度量两个变量 X 和 Y 之间的相关（线性相关），其值介于[-1,1] 之间。有三种相关情况： 正向相关: &gt;0 负向相关：&lt;0 无相关性：=0 下图从左到右分别代表了正向相关、无相关性和负向相关： 在介绍皮尔逊相关系数之前，要先理解协方差(Covariance ) ，协方差是一个反映两个随机变量相关程度的指标，如果一个变量跟随着另一个变量同时变大或者变小，那么这两个变量的协方差就是正值，反之相反，公式如下：$$Conv(X,Y) = \frac{\sum_{i=1}^{n}(X_i-\overline{X})(Y_i-\overline{Y})}{n-1}$$皮尔逊相关系数的公式如下：$$r_{xy} = \frac{Conv(X,Y)}{\sqrt{Var(X)Var(Y)}} = \frac{\sum(x-\overline{x})(y-\overline{y})}{\sqrt{\sum{(x-\overline{x})^2}\sum(y-\overline{y})^2}}$$Var表示方差，相关度越高，皮尔逊相关系数其值趋于 1 或 -1 （趋于1表示它们呈正相关， 趋于 -1 表示它们呈负相关）；如果相关系数等于0，表明它们之间不存在线性相关关系。 决定系数决定系数即 R 平方值，反应因变量的全部变异能通过回归关系被自变量解释的比例。如R平方为0.8，则表示回归关系可以解释因变量80%的变异。换句话说，如果我们能控制自变量不变，则因变量的变异程度会减少 80%。 在简单线性回归中，决定系数可以是 R^2 = r * r。而更通用的是： SST 其实是两部分组成的，一部分是模型可预测的SSR，一部分是变异的SSError无法用模型解释的。它们之间的计算公式是: 注意: R平方也有其局限性：R平方随着自变量的增加会变大，R平方和样本量是有关系的。因此，我们要到R平方进行修正。修正的方法：$$\overline{R^2} = 1-(1-R^2)\frac{n-1}{n-p-1}$$其中，n 表示样本大小，p 表示模型中解释变量的总数（不包括常数）。 代码实例代码完全按照上述中的公式计算12345678910111213141516171819202122232425262728293031323334353637383940import numpy as npimport mathfrom sklearn import linear_model#计算皮尔逊相关系数( Pearson correlation coefficient）def computer_conv(x,y): var_x = 0 var_y = 0 SSR = 0 x_bar = np.mean(x) # x的方差 y_bar = np.mean(y) # y的方差 for i in range(len(x)): diff_xbar = x[i] - x_bar diff_ybar = y[i] - y_bar SSR += diff_xbar * diff_ybar var_x += diff_xbar**2 var_y += diff_ybar**2 SST = math.sqrt(var_x*var_y) return SSR/SST#计算决定系数R平方值def computer_r(x,y): SSR = 0 SST = 0 linear = linear_model.LinearRegression() # 创建线性模型 linear.fit(x,y) y_hat = linear.predict(x) y_mean = np.mean(y) for i in range(len(x)): SSR += (y_hat[i] - y_mean)**2 SST += (y[i] - y_mean)**2 return SSR/SSTtest_x = [1,3,8,7,9]test_y = [10,12,24,21,34]test_x2 = [[x] for x in test_x]print("r: ",computer_conv(test_x,test_y))print("r平方: ",computer_conv(test_x,test_y)**2)print("R平方: ",computer_r(test_x2,test_y)) 程序运行结果123r: 0.94031007654487r平方: 0.8841830400518192R平方: 0.8841830400518192 我们发现：在简单线性回归中，决定系数的确满足$ R^2 = r * r$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>Conv&amp;&amp;R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之非线性回归（ Logistic Regression）]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-13-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8B%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[非线性回归是线性回归的延伸，其目标预测函数不是线性的。本文主要介绍逻辑回归（Logistic Regression），它是非线性回归的一种，虽然名字中有“回归”二字，但其本质上是一个分类模型。 含义我们知道，线性回归的模型是求出输出特征向量Y和输入样本矩阵X之间的线性关系系数θ，满足$Y=Xθ$。此时Y是连续的，所以是回归模型。如果Y是离散的话，如何解决？一个可以想到的办法是，我们对于Y再做一次函数转换，变为g(Y)。如果我们令g(Y)的值在某个实数区间的时候是类别A，在另一个实数区间的时候是类别B，以此类推，就得到了一个分类模型。如果结果的类别只有两种，那么就是一个二元分类模型了。逻辑回归的出发点就是从这来的。 看如下实例有这么几组医疗数据，X特征是肿瘤的大小（连续型），Y是其良恶性（离散型），Y只有0（良性）和1（恶性）2种取值。我们选取阈值0.5，h（x）&gt;0.5（恶性），Malignant=1，反之为0，良性。 我们选取阈值0.2，h（x）&gt;0.2（恶性），Malignant=1，反之为0，良性。 比较上述两种情况，新的数值加入时需要不断调整阈值，说明用线性的方法进行回归不太合理。 基本模型我们假设测试数据为$X(x_0,x_1,…,x_n)$需要学习的参数为$\Theta(\theta_0,\theta_1,…,\theta_n)$ 给定函数 $$Z = \theta_0 + \theta_1x_1 + \theta_2x_2+…+\theta_nx_n$$向量化可表示为$$Z = \Theta^TX$$经常需要一个分界线作为区分两类结果。再次需要一个函数进行曲线平滑化，由此引入Sigmoid 函数进行转化： $$g(z) = \frac{1}{1+e^{-z}}$$这样的，可以以 0.5 作为分界线。因此逻辑回归的最终目标函数就是：$$h_\theta(X) = g(\theta_0 + \theta_1x_1 + \theta_2x_2+…+\theta_nx_n) = g(\theta^TX) = \frac{1}{1+e^{-\theta^TX}}$$回归是用来得到样本属于某个分类的概率。因此在分类结果中，假设 y 值是 0 或 1，那么正例 (y = 1): $$h_\theta(X) = P(y=1|X;\theta)$$反例(y = 0):$$1 - h_\theta(X) = P(y=0|X;\theta)$$在线性回归中，我们要找到合适的$\theta^{(i)}$使下面的损失函数值最小：$$Cost(h_\theta(x^{(i)}),y^{(i)}) = \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$$如果在逻辑回归中运用上面这种损失函数，得到的函数 J 是一个非凸函数，存在多个局部最小值，很难进行求解，因此需要换一个 cost 函数。重新定义个 cost 函数如下：$$Cost(h_\theta(x^{(i)}),y^{(i)}) = -\frac{1}{m}[\sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\theta(x^{(i)})))]$$ 求解方法我们采用梯度下降法求解最佳解。梯度下降法的计算过程就是沿梯度下降的方向求解极小值。 先确定向下一步的步幅大小，称之为Learning rate; 任意给定一个初始值：$\theta_0,\theta_1,…,\theta_n$; 确定一个向下的方向，并向下走预先规定的步伐，并更新$\theta_0,\theta_1,…,\theta_n$; 当下降的高度小于某个定义的阈值时，停止下降更新。 这就好比是下山，下一步的方向选的是最陡的方向。梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。θ 的更新方程如下：$$\theta_j = \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)$$其中，偏导是：$$\frac{\partial}{\partial\theta_j}J(\theta) = \frac{1}{m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})x_j^{(i)}$$ 代码实例本部分我们将采用2种方式实现逻辑回归模型，一种自己编写函数方法，一种调用sklearn中的方法库（LogisticRegression）。 自己实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import numpy as npimport randomimport matplotlib.pyplot as plt#sigmod函数，将值量化到0-1def sigmoid(x): return 1.0 / (1 + np.exp(-x))# 梯度下降算法def gradientDescent(x, y, theta, alpha, m, numIteration): ''' :param x: 输入实例 :param y: 分类标签 :param theta: 学习的参数 :param alpha: 学习率 :param m: 实例个数 :param numIteration: 迭代次数 :return: 学习擦参数theta ''' xTrans = x.transpose() # 矩阵的转置 J = [] # 存储损失的列表，方便绘图 for i in range(0, numIteration): hypothsis = sigmoid(np.dot(x, theta)) #量化到0-1 loss = hypothsis - y #计算误差 cost = np.sum(loss ** 2) / (2 * m) #计算损失 J.append(cost) # 将损失存入列表 print("Iteration %d / Cost:%f" % (i, cost)) gradient = np.dot(xTrans, loss) / m theta = theta - alpha * gradient # 更新梯度 plt.plot(J) # 可视化损失变化 plt.show() return theta# 创建数据，用作测试def genData(numPoints, bais, variance): ''' :param numPoints: 输入实例数目 :param bais: 偏向 :param variance: 方差 :return 创建的数据 x,y ''' # 实例（行数）、偏向、方差 x = np.zeros(shape=(numPoints, 2)) # 初始化numPoints行2列(x1,x2)的全零元素矩阵 y = np.zeros(shape=numPoints) # 归类标签 y_list = [0,1] # 标签列表y的取值 for i in range(0, numPoints): x[i][0] = random.uniform(0, 1) * variance # 创建X的特征对 x[i][1] = (i + bais) + random.uniform(0, 1) * variance random.shuffle(y_list) # 给X特征对附上随机的0 1 标签 y[i] =y_list[0] return x, yx, y = genData(100, 2, 5)#print(x)#print(y)#x和y的维度m, n = np.shape(x)n_y = np.shape(y)numIteration = 100000alpha = 0.0005theta = np.ones(n) # 初始化thetatheta = gradientDescent(x, y, theta, alpha, m, numIteration)print(theta) #output [-0.14718538 0.00381781] Sklearn实现123456from sklearn.linear_model import LogisticRegression# 调用sklearn自带的逻辑回归方法sk = LogisticRegression(max_iter=100000)sk.fit(x, y) # 此处的x和y与上面自己实现的一致print(sk.intercept_)print(sk.coef_) #output [-0.11612453 0.00272452] 我们发现自己闪现求解的$\theta$值与sklearn中的相比，还是存在一定误差的，这是因为sklearn中的方法 LogisticRegression有很多参数进行了详细的优化，详情参见https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn-linear-model-logisticregression Cost损失可视化 模型优缺点优点： 适合需要得到一个分类概率的场景 计算代价不高，容易理解实现。LR在时间和内存需求上相当高效 LR对于数据中小噪声的鲁棒性很好 缺点： 容易欠拟合，分类精度不高 对异常值敏感]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>Unlinear regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之线性回归（LR）]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-12-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[线性回归(linear regression)是利用数理统计和归回分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。与之前的分类问题( Classification )不一样的是，分类问题的结果是离散型的；而回归问题中的结果是连续型（数值）的。 数据特征数理统计中，常用的描述数据特征的有： 均值（mean）：又称平均数或平均值，是计算样本中算术平均数：$$\overline{x} = \frac{\sum_{i=1}^{n}x_i}{n}$$ 中位数（median）：将数据中的各个数值按照大小顺序排列，居于中间位置的变量。当n为基数的时候：直接取位置处于中间的变量；当n为偶数的时候，取中间两个量的平均值。 众数（mode）：数据中出现次数最多的数。 方差( variance )：一种描述离散程度的衡量方式： $$s^2 = \frac{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}{n-1}$$ 标准差 （standard deviation） :将方差$s^2​$开方就能得到标准差。 简单线性回归算法简单线性回归(Simple Linear Regression ),，也就是一元线性回归，包含一个自变量x 和一个因变量y 。常被用来描述因变量(y)和自变量(X)以及偏差(error)之间关系的方程叫做回归模型，这个模型是：$$y = \beta_0 + \beta_1x + \epsilon$$其中偏差 $\epsilon$ 满足正态分布的。因此它的期望值是 0 。 $\epsilon$ 的方差(variance)对于所有的自变量 x 是一样的。 等式两边求期望可得： $$E(y) = \beta_0 + \beta_1x$$其中，β0 是回归线的截距，β1 是回归线的斜率，E(y) 是在一个给定 x 值下 y 的期望值（均值）。 假设我们的估值函数（注意，是估计函数）:$$\widehat{y} = b_0 + b_1x$$b0是估计线性方程的纵截距，b1是估计线性方程的斜率，ŷ是在自变量x等于一个给定值的时候，y的估计值。b0和b1是对β0和β1的估计值。 线性回归的分析流程 如何寻找最佳回归线 寻找的回归方程，要求与真实值的离散程度是最小的： $$min\sum_{i=0}^{n}(y_{i}-\widehat {y})^2$$$$b_1 = \frac{\sum(x_{i}-\overline{x})(y_{i}-\overline{y})}{\sum(x_{i}-\overline{x})^2}$$$$b_0 = \overline{y} - b_1\overline{x}$$ 上面方法类似于数值分析中的最小二乘法，详细推导可见https://zh.wikipedia.org/wiki/最小二乘法 代码实例123456789101112131415161718192021222324252627#简单现行回归：只有一个自变量 y=k*x+b 预测使 (y-y*)^2 最小import numpy as npdef SLR(x:list,y:list): n =len(x) fenzi = 0 fenmu = 0 #按照上面推导的公式计算出估计回归方程中的b0，b1 for i in range(n): fenzi += (x[i]-np.mean(x)) * (y[i]-np.mean(y)) fenmu += (x[i]-np.mean(x)) **2 #print("fenzi: " + str(fenzi)) #print("fenmu: " + str(fenmu)) b1 = fenzi/fenmu # 斜率 b0 = np.mean(y) - b1 * np.mean(x) # 截距 return b0,b1#测试def predict(x,b0,b1): return b0 + b1*xif __name__ == "__main__": x = [1,3,2,1,3] y = [14,24,18,17,27] b0,b1 = SLR(x,y) y_predict = predict(6,b0,b1) print("y_predict: " + str(y_predict)) 多元线性回归算法当自变量有多个时，回归模型就变成了: $$E(y) = \beta0 + \beta_1x_1 +…+\beta_nx_n$$估值函数的自变量也变多：$$\widehat {y} = b_0 + b_1x_1+ b_2x_2+…+b_nx_n$$令矩阵：函数可以转换为：$$h(x) = B^TX$$如果有训练数据:$$D = (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…, (x^{(m)},y^{(m)}),$$损失函数( cost function )，寻找的目标函数应该尽可能让损失函数小，这个损失函数为：$$J(B) = \frac{1}{2m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2 = \frac{1}{2m}(XB-y)^T(XB-y)$$目的就是求解出一个使得代价(损失)函数最小的 B。 利用梯度下降求解估值函数梯度下降算法是一种求局部最优解的方法，详见https://www.cnblogs.com/pinard/p/5970503.html ，今后的博客也会详细介绍。B 由多个元素组成，对于损失函数可以求偏导如下： $$\frac{\partial}{\partial{B_j}} = \frac{1}{m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})x_{j}^{(i)}$$更新B：$$B_{j} = B_{j} - \alpha\frac{\partial}{\partial{B_j}}J(B)$$其中α 表示学习率，决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。 最小二乘法求解估值函数多元多项式的情况一样可以利用最小二乘法来进行求解，将m条数据都带入估值函数可以得到线性方程组:向量化后可以表示为：我们要让$$\sum_{i=1}^{m} = (y_i-\widehat{y})^2$$的值最小，则系数矩阵中，各个参数的偏导的值都会0，因此可以得到 n 个等式：得到正规方程组： 写成矩阵表示为:$$X^TXB = X^TY$$解得：$$B = (X^TX)^{-1}X^TY$$ 代码实例本部分我们会用2种方式实现多元线性回归(Multiple Linear Regression )，一种是sklearn库包中自带的方法，一种是我们依据最小二乘法实现的求解方法。应用实例：一家快递公司送货：X1： 运输里程 X2： 运输次数 Y：总运输时间将其数据存储为csv文件如下： sklearn实现123456789101112131415161718192021from numpy import genfromtxtfrom sklearn import linear_modeldatapath = 'G:/PycharmProjects/Machine_Learning/Linear_Regression/data1.csv'data = genfromtxt(datapath,delimiter=',')x = data[:,:-1]y = data[:,-1]# print(x)# print(y)mlr = linear_model.LinearRegression()mlr.fit(x,y)# print (mlr)# print ("coef: " + str(mlr.coef_) )# print ("intercept: " + str(mlr.intercept_))x_predict = [50,3]y1_predict = mlr.predict([x_predict])print("y_predict: " + str(y1_predict))#output [4.95830457] 最小二乘法实现12345678910111213141516171819202122232425262728293031import numpy as npfrom numpy import genfromtxtclass MyLinearRegression(object): def __init__(self): self.b = [] def fit(self, x: list, y: list): # 为每条训练数据前都添加 1 tmpx = [[1] for _ in range(len(x))] for i, v in enumerate(x): tmpx[i] += v x_mat = np.mat(tmpx) y_mat = np.mat(y).T xT = x_mat.T self.b = (xT * x_mat).I * xT * y_mat def predict(self, x): return np.mat([1] + x) * self.bdatapath = 'G:/PycharmProjects/Machine_Learning/Linear_Regression/data1.csv'data = genfromtxt(datapath,delimiter=',')x = data[:,:-1]y = data[:,-1]test_row = [50, 3]linear = MyLinearRegression()linear.fit(x, y)print(linear.predict(test_row))#output[ 4.95830457] 补充：上面实例中，x的特征都是连续数值，如果有离散型的特征（车型）,我们可以采取如下方法，将车型特征进行one-hot编码，代码不需要变化。 参考资料多元线性回归及其优化算法多元线性回归分析理论详解]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>linear regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之神经网络（NN）]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-10-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8BNN%2F</url>
    <content type="text"><![CDATA[神经网络算法( Neural Network )是机器学习中非常非常重要的算法。它 以人脑中的神经网络为启发，是整个深度学习的核心算法。深度学习就是根据神经网络算法进行的一个延伸。 背景神经网络是受神经元启发的，对于神经元的研究由来已久，1904年生物学家就已经知晓了神经元的组成结构。一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。 1949年心理学家Hebb提出了Hebb学习率，认为人脑神经细胞的突触（也就是连接）上的强度上可以变化的。于是计算科学家们开始考虑用调整权值的方法来让机器学习。这为后面的学习算法奠定了基础。 1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字–感知器（ Perceptron ）。 1986年，Rumelhar和Hinton等人提出了反向传播（ Backpropagation ，BP）算法，这是最著名的一个神经网络算法。 组成多层神经网咯主要由三部分组成：输入层(input layer), 隐藏层 (hidden layers), 输入层 (output layers)。 每层由单元(units)组成， 输入层(input layer)是由训练集的实例特征向量传入，经过连接结点的权重(weight)传入下一层，一层的输出是下一层的输入。 隐藏层(hidden layers)的个数可以是任意的，输入层有一层，输出层(output layers)有一层。每个单元(unit)也可以被称作神经结点。上图为2层的神经网络（一般输入层不算）。一层中加权的求和，然后根据非线性方程转化输出。作为多层向前神经网络，理论上，如果有足够多的隐藏层(hidden layers) 和足够大的训练集, 可以模拟出任何方程。神经网络可以用来解决分类(classification）问题，也可以解决回归( regression )问题。 单层到多层的神经网络由两层神经网络构成了单层神经网络，它还有个别名——— 感知机（Perceptron）。 上图中，有3个输入，连接线的权值分别是 w1, w2, w3。将输入与权值进行乘积然后求和，作为 z单元的输入，如果 z 单元是函数 g ，那么就有 z = g(a1 * w1 + a2 * w2 + a3 * w3) 。 单层神经网络的扩展(2个z单元)，也是一样的计算方式： 在多层神经网络中，只不过是将输出作为下一层的输入，一样是乘以权重然后求和： 设计在使用神经网络训练数据之前，必须确定神经网络的层数以及每层单元的个数。特征向量在被传入输入层时通常被先标准化(normalize）到0和1之间 （为了加速学习过程）。离散型变量可以被编码成每一个输入单元对应一个特征值可能赋的值例如：特征值A可能取三个值（$a_0, a_1, a_2$), 可以使用3个输入单元来代表A。 如果$A=a_0$, 那么代表$a_0$的单元值就取1，其他取0，[1, 0, 0] 如果$A=a_1$, 那么代表$a_1$的单元值就取1，其他取0，[0, 1, 0] 如果$A=a_2$, 那么代表$a_2$的单元值就取1，其他取0，[0, 0, 1] 对于分类问题，如果是2类，可以用一个输出单元表示（0和1分别代表2类）。如果多余2类，每一个类别用一个输出单元表示，所以输入层的单元数量通常等于类别的数量。一般没有明确的规则来设计最好有多少个隐藏层，通常是根据实验测试和误差，以及准确度来实验并做出改进。 交叉验证模型训练结束后，如何来衡量我们模型的泛化能力（测试集上准确率）呢？常见的方法是将数据集分为两类，训练集和测试集，利用测试集的数据将模型的预测结果与真实测试标签作对比，得出准确度。下面介绍另一个常用但更科学的方法———交叉验证方法( Cross-Validation )。这个方法不局限于将数据集分成两份。如上图所示，我们可以将原始数据集分成 k 份。第一次用第一份作为训练集，其余作为测试集，得出这一部分的准确度 ( evaluation )。第二次再用第二份作为训练集，其余作为测试集，得出这第二部分的准确度。以此类推，最后取各部分准确度的平均值作为最终的模型衡量指标。 BP算法BP 算法 (BackPropagation )是多层神经网络的训练一个核心的算法。目的是更新每个连接点的权重，从而减小预测值(predicted value )与真实值 ( target value )之间的差距。输入一条训练数据就会更新一次权重，反方向（从输出层=&gt;隐藏层=&gt;输入层）来以最小化误差（error）来更新权重（weitht）。在训练神经网络之前，需要初始化权重(weights )和偏向( bias )，初始化是随机值， -1 到 1 之间或者-0.5到0.5之间，每个单元有一个偏向。 BP算法有2个过程，前向传播和反向传播，后者是关键。我们以下图为例，分析BP算法的整个过程。 前向传播利用上一层的输入，得到本层的输入:$$I_j = \sum_{i}w_{i,j}O_{i} + \theta_{j}$$其中$w_{i,j}$表示权重，$O_{j}$表示当前神经单元的值，$\theta_{j}$为当前神经单元的偏置向。$i=0$时，$O_{j}$即为输入单元的值。得到输入值后，神经元要怎么做呢？我们先将单个神经元进行展开如图： 隐藏层单元得到值后进行累加求和，然后需要进行一个非线性转化，这个转化在神经网络中称为激活函数( Activation function )，这个激活函数是一个 S(下面会有所介绍) 函数，图中以 f 表示，它的函数为：$$O_{j} = \frac{1}{1+e^{-I_j}}$$ 反向传播前向传播结束后，我们要计算误差，然后根据误差(error)反向传送，更新权重和偏置向。对于输出层的误差：$$Err_{j} = O_{j}(1-O_{j})(T_{j}-O_{j})$$其中$O_{j}$表示预测值，$T_{j}$表示真实值。 对于隐藏层的误差：$$Err_{j} = O_{j}(1-O_{j})\sum_{k}Err_{k}w_{j,k}$$其中$Err_{k}$为后层单元误差，$w_{j,k}$为当前单元与后层单元的权重值。 更新权重：$$\Delta w_{i,j} = (l)Err_{j}O_{i}$$ $$w_{i,j} = w_{i,j} + \Delta w_{i,j}$$更新偏置向：$$\Delta\theta{j} = (l)Err_{j}$$ $$\theta{j} = \theta_{j} + \Delta\theta_{j}$$上面$（l）$为学习率 终止条件 权重的更新低于某个阈值，这个阈值是可以人工指定的； 预测的错误率低于某个阈值； 达到预设一定的循环次数。 S型函数（sigmod）神经元在对权重进行求和后，需要进行一个非线性转化，即作为参数传入激活函数去。这个激活函数是一个 S 型函数(Sigmoid)。S 函数不是指某个函数，而是一类函数,详解可参考 https://zh.wikipedia.org/wiki/S函数 。下面有两个常见的S 函数： 双曲函数（tanh ） 逻辑函数（logistic function ）Sigmod函数S 曲线函数可以将一个数值转为值域在 0 到 1 之间，广义上S 函数是满足y值在某个值域范围内渐变的一条曲线。 双曲函数（tanh） 双曲函数是一类与常见的三角函数（也叫圆函数）类似的函数。详见https://zh.wikipedia.org/wiki/双曲函数 。$$tanhx = \frac{sinhx}{coshx}$$导数为$$\frac{d}{dx}tanhx = 1 - tanh^2x =sech^2x = \frac{1}{cosh^2x}$$ 逻辑函数（logistic function ） 逻辑函数或逻辑曲线是一种常见的S函数，详见https://zh.wikipedia.org/wiki/逻辑函数 。一个常见的逻辑函数可以表示如下：$$f(x) = \frac{1}{1 + e^{-x}}$$导数为 $$\frac{d}{dx} = \frac{e^x}{(1+e^x)^2} = f(x)(1 - f(x))$$ BP算法举例假设有一个两层的神经网络，结构，权重和数据集如下： 根据上述公式（3）到（8）计算误差和更新权重： 代码实例（手写数字识别）基础函数根据神经网络原理，代码实现一个完整的神经网络来对手写数字图片分类。（完整代码见附录）根据上面的sigmoid 函数以及其求导得到对应的python代码：123456789101112def tanh(x): return np.tanh(x)def tanh_deriv(x): return 1.0 - np.tanh(x) * np.tanh(x)def logistic(x): return 1.0 / (1 + np.exp(-x))def logistic_deriv(x): fx = logistic(x) return fx * (1 - fx) 网络函数在网络函数中，需要确定神经网络的层数，每层的个数，从而确定单元间的权重规格和单元的偏向。12345678910111213141516171819202122def __init__(self, layers, activation='logistic'): if activation == 'tanh': self.activation = tanh self.activation_deriv = tanh_deriv elif activation == 'logistic': self.activation = logistic self.activation_deriv = logistic_deriv # 初始化随即权重 self.weights = [] # len(layers)layer是一个list[2,2,1]，则len(layer)=3 # 除了输出层,其它层层之间的单元都要赋予一个随机产生的权重 for i in range(len(layers) - 1): tmp_weights = (np.random.random([layers[i], layers[i + 1]]) * 2 - 1) * 0.25 self.weights.append(tmp_weights) #初始化偏置向 #除了输入层，其它层单元都有一个偏置向 self.bias = [] for i in range(1, len(layers)): tmp_bias = (np.random.random(layers[i]) * 2 - 1) * 0.25 self.bias.append(tmp_bias) 其中，layers 参数表示神经网络层数以及各个层单元的数量，例如下图这个模型：上述网络模型就对应了 layers = [4, 3, 2] 。这是一个 2 层，即len(layers) - 1层的神经网络。输入层 4 个单元，其他依次是 3 ，2 。权重是表示层之间单元与单元之间的连接。因此权重也有 2 层。第一层是一个4 x 3的矩阵，即layers[0] x layers[1]。同理，偏向也有 2 层，第一层个数 3 ，即leyers[1] 。利用np.random.random([m, n]) 来创建一个 m x n 的矩阵。在这个神经网络的类中，初始化都随机-0.25 到 0.25之间的数。 训练函数在神经网络的训练中，需要先设定一个训练的终止条件，前面介绍了3种停止条件，这边使用的是 达到预设一定的循环次数 。每次训练是从样本中随机挑选一个实例进行训练，将这个实例的预测结果和真实结果进行对比，再进行反向传播得到各层的误差，然后再更新权重和偏向：12345678910111213141516171819202122232425262728293031323334353637383940def fit(self, X, y, learning_rate=0.2, epochs=10000): #X：数据集,确认是二维，每行是一个实例，每个实例有一些特征值 X = np.atleast_2d(X) #每个实例的标签 y = np.array(y) # 随即梯度 for k in range(epochs): i = np.random.randint(X.shape[0]) a = [X[i]] # 随即取某一条实例 ''' #正向传播计算各单元的值 #np.dot代表两参数的内积，x.dot(y) 等价于 np.dot(x,y) #即a与weights内积加上偏置求和，之后放入非线性转化function求下一层 #a输入层，append不断增长，完成所有正向计算 ''' for j in range(len(self.weights)): a.append(self.activation(np.dot(a[j], self.weights[j]) + self.bias[j] )) # 计算错误率，y[i]真实标记 ，a[-1]预测的classlable errors = y[i] - a[-1] # 计算输出层的误差，根据最后一层当前神经元的值，反向更新 deltas = [errors * self.activation_deriv(a[-1]) ,] # 输出层的误差 # 反向传播，对于隐藏层的误差，从后往前 for j in range(len(a) - 2, 0, -1): tmp_deltas = np.dot(deltas[-1], self.weights[j].T) * self.activation_deriv(a[j]) deltas.append(tmp_deltas) #reverse将deltas的层数跌倒过来 deltas.reverse() # 更新权重 for j in range(len(self.weights)): layer = np.atleast_2d(a[j]) delta = np.atleast_2d(deltas[j]) self.weights[j] += learning_rate * np.dot(layer.T, delta) # 更新偏向 for j in range(len(self.bias)): self.bias[j] += learning_rate * deltas[j] 其中参数 learning_rate 表示学习率， epochs 表示设定的循环次数。 预测函数预测就是将测试实例从输入层传入，通过正向传播，最后返回输出层的值：123456#预测 def predict(self, x): a = np.array(x) # 确保x是 ndarray 对象 for i in range(len(self.weights)): a = self.activation(np.dot(a, self.weights[i]) + self.bias[i]) return a 手写数字图片识别手写数字数据集来自 sklearn ，其中由1797个图像组成，其中每个图像是表示手写数字的 8x8 像素图像： 可以推出，这个神经网络的输入层将有 64 个输入单元，分类结果是 0-9 ，因此输出层有10个单元，构造为： 1nn = NeuralNetwork(layers=[64, 100, 10]) 导入数据集并拆分为训练集和测试集 12345678910digits = datasets.load_digits() #导入数据 X = digits.data y = digits.target # 拆分为训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X, y) # 分类结果离散化 labels_train = LabelBinarizer().fit_transform(y_train) labels_test = LabelBinarizer().fit_transform(y_test) 训练并测试模型1234567891011nn.fit(X_train, labels_train) # 收集测试结果 predictions = [] for i in range(X_test.shape[0]): o = nn.predict(X_test[i] ) predictions.append(np.argmax(o)) # 打印对比结果 print (confusion_matrix(y_test, predictions) ) print (classification_report(y_test, predictions)) 程序运行结果如下：混淆矩阵中，对角线计数越大表示该类别预测越准确，最后预测准确率在95%。 1234567891011121314151617181920212223242526[[53 0 0 0 0 0 0 0 0 0] [ 0 43 0 0 0 0 0 0 0 0] [ 0 0 51 0 0 0 0 0 0 0] [ 0 0 3 32 0 0 0 0 3 1] [ 0 0 0 0 39 0 0 0 0 0] [ 0 0 0 0 0 38 1 0 0 2] [ 0 3 0 0 0 0 51 0 0 0] [ 0 0 0 0 0 1 0 36 0 2] [ 0 4 0 0 0 0 0 0 41 0] [ 0 2 0 0 0 1 0 0 0 43]] precision recall f1-score support 0 1.00 1.00 1.00 53 1 0.83 1.00 0.91 43 2 0.94 1.00 0.97 51 3 1.00 0.82 0.90 39 4 1.00 1.00 1.00 39 5 0.95 0.93 0.94 41 6 0.98 0.94 0.96 54 7 1.00 0.92 0.96 39 8 0.93 0.91 0.92 45 9 0.90 0.93 0.91 46 micro avg 0.95 0.95 0.95 450 macro avg 0.95 0.95 0.95 450weighted avg 0.95 0.95 0.95 450 附录（完整代码）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130import numpy as npfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import LabelBinarizerfrom sklearn.metrics import confusion_matrix, classification_report'''一些激活函数及其导数的计算定义'''def tanh(x): return np.tanh(x)def tanh_deriv(x): return 1.0 - np.tanh(x) * np.tanh(x)def logistic(x): return 1.0 / (1 + np.exp(-x))def logistic_deriv(x): fx = logistic(x) return fx * (1 - fx)#构建神经网络类class NeuralNetwork: ''' 根据类实例化一个函数，_init_代表的是构造函数 self相当于java中的this layers:一个列表，包含了每层神经网络中有几个神经元，至少有两层，输入层不算作 [, , ,]中每个值代表了每层的神经元个数 activation：激活函数可以使用tanh 和 logistics，不指明的情况下就是logistics函数 ''' def __init__(self, layers, activation='logistic'): if activation == 'tanh': self.activation = tanh self.activation_deriv = tanh_deriv elif activation == 'logistic': self.activation = logistic self.activation_deriv = logistic_deriv # 初始化随即权重 self.weights = [] # len(layers)layer是一个list[2,2,1]，则len(layer)=3 # 除了输出层,其它层层之间的单元都要赋予一个随机产生的权重 for i in range(len(layers) - 1): tmp_weights = (np.random.random([layers[i], layers[i + 1]]) * 2 - 1) * 0.25 self.weights.append(tmp_weights) #初始化偏置向 #除了输入层，其它层单元都有一个偏置向 self.bias = [] for i in range(1, len(layers)): tmp_bias = (np.random.random(layers[i]) * 2 - 1) * 0.25 self.bias.append(tmp_bias) def fit(self, X, y, learning_rate=0.2, epochs=10000): #X：数据集,确认是二维，每行是一个实例，每个实例有一些特征值 X = np.atleast_2d(X) #每个实例的标签 y = np.array(y) # 随即梯度 for k in range(epochs): i = np.random.randint(X.shape[0]) a = [X[i]] # 随即取某一条实例 ''' #正向传播计算各单元的值 #np.dot代表两参数的内积，x.dot(y) 等价于 np.dot(x,y) #即a与weights内积加上偏置求和，之后放入非线性转化function求下一层 #a输入层，append不断增长，完成所有正向计算 ''' for j in range(len(self.weights)): a.append(self.activation(np.dot(a[j], self.weights[j]) + self.bias[j] )) # 计算错误率，y[i]真实标记 ，a[-1]预测的classlable errors = y[i] - a[-1] # 计算输出层的误差，根据最后一层当前神经元的值，反向更新 deltas = [errors * self.activation_deriv(a[-1]) ,] # 输出层的误差 # 反向传播，对于隐藏层的误差，从后往前 for j in range(len(a) - 2, 0, -1): tmp_deltas = np.dot(deltas[-1], self.weights[j].T) * self.activation_deriv(a[j]) deltas.append(tmp_deltas) #reverse将deltas的层数跌倒过来 deltas.reverse() # 更新权重 for j in range(len(self.weights)): layer = np.atleast_2d(a[j]) delta = np.atleast_2d(deltas[j]) self.weights[j] += learning_rate * np.dot(layer.T, delta) # 更新偏向 for j in range(len(self.bias)): self.bias[j] += learning_rate * deltas[j] #预测 def predict(self, x): a = np.array(x) # 确保x是 ndarray 对象 for i in range(len(self.weights)): a = self.activation(np.dot(a, self.weights[i]) + self.bias[i]) return aif __name__ == "__main__": ''' 手写体数字图片识别，每张图片像素大小为8*8，共（0-9）10类 调用NeuralNetwork类并设置layer参数： 输入层64个单元，隐藏层100个单元，输出层单元数为类别数10 ''' nn = NeuralNetwork(layers=[64, 100, 10]) digits = datasets.load_digits() #导入数据 X = digits.data y = digits.target # 拆分为训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X, y) # 分类结果离散化 labels_train = LabelBinarizer().fit_transform(y_train) labels_test = LabelBinarizer().fit_transform(y_test) nn.fit(X_train, labels_train) # 收集测试结果 predictions = [] for i in range(X_test.shape[0]): o = nn.predict(X_test[i] ) predictions.append(np.argmax(o)) # 打印对比结果 print (confusion_matrix(y_test, predictions) ) print (classification_report(y_test, predictions)) 参考资料神经网络算法多层神经网络BP算法 原理及推导]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>neural network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之SVM]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-8-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8BSVM%2F</url>
    <content type="text"><![CDATA[支持向量机（support vector machine）,简称SVM，最早在1963年，由 Vladimir N. Vapnik 和 Alexey Ya. Chervonenkis 提出。目前的版本(soft margin)是由Corinna Cortes 和 Vapnik在1993年提出，并在1995年发表。 背景深度学习（2012）出现之前，SVM被认为机器学习中近十几年来最成功，表现最好的算法。SVM本质上是一种二分类器，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。看下图在这个二维的平面中，用一条直线将上面的点分成两类，显然 H1 无法将点区分开，而H2 和 H3 都可以，但这两条哪个更好呢？作为分界线，H3 是更合适的，因为分界线其两边有尽可能大的间隙，这样的好处之一就是增强分类模型的泛化能力，能较为正确的分类预测新的实例。 上面的例子是在二维平面，我们找的是一条直线。假如在三维空间，我们要找的就是一个平面。总的来说就是要寻找区分两类的超平面(hyper plane )，使边界(margin )最大。在一个 n 维空间中，超平面的方程定义为: &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$a_1x_1+a_2x_2+…+a_nx_n = b$ 总共可以有多少个可能的超平面？无数条。我们要寻找的超平面是到边界一测最近点的距离等于到另一侧最近点的距离，在这个边界中，边界两侧的超平面平行。 点到超平面的距离在二维平面中，计算点 (x0, y0) 到线ax + by + c = 0的距离是:在 n 维空间中，点到超平面的距离：将点的坐标和系数都向量化表示，距离公式可以视为:其中 w = {w0, w1, w2,...,wn}我们寻找超平面时，先寻找各分类到超平面的距离最小，再寻找距离之和最大的超平面。共N 个训练点，点的坐标记为xi，结果分类为yi，构成 (xi, yi)：常见的SVM是 二分类模型，因此一般yi 有两种取值为 1 和 -1，取这两个值可以简化求解过程。 超平面推导在 (xi, yi) 中，我们用 xi 表示了点的坐标，yi 表示了分类结果。超平面表示如下：$$w^Tx + b = 0$$在超平面的上方的点满足：$$w^T + b &gt; 0$$在超平面的下方的点满足：$$w^T + b &lt; 0$$因为 yi 只有两种取值 1 和 -1。因此就满足：整合这两个等式（左右都乘以 yi，当yi是负值时，不等号要改方向）得：$$y_i(w^T + b)\geq1,\forall{i}$$ 支持向量所有坐落在边界的边缘上的点被称作是 “支持向量”。分界边缘上的任意一点到超平面的距离为:$\left|\frac{1}{w}\right|$。其中，||w|| 是向量的范数(norm)，或者说是向量的模。它的计算方式为：$$\left|{w}\right| = \sqrt{w * w} = \sqrt{w_1^2 + w_2^2 +…+ w_n^2}$$所以最大边界距离为$\left|\frac{2}{w}\right|$。 找出最大边界的超平面 利用一些数学推导，把 yi(w^T*x + b) &gt;= 1 变为有限制的凸优化问题( convex quadratic optimization )； 利用Karush-Kuhn-Tucker( KKT ) 条件和拉格朗日公式，可以推出 MMH 可以被表示为以下的“决定边界( decision boundary)” $$d(X^T) = \sum_{i=1}^{l}y_ia_iX_iX^T + b_0$$ 上述公式中各个符号含义如下： l：表示 支持向量点 的个数； yi : 第i个支持向量点； Xi ：支持向量的类别标记( class label )； ai 与b0：都是单一数值型参数。 对于测试实例，代入以上公式，按得出结果的正负号来进行分类。 SVM 算法有下面几个特性： 训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以 SVM 不太容易产生过拟合(overfitting )的情况； SVM 训练出来的模型完全依赖于支持向量，即使训练集里所有非支持向量的点都被去除，重新训练，结果仍然会得到完全一样的模型； 一个 SVM 如果训练得出的支持向量个数比较小，那训练出的模型比较容易被泛化 线性不可分在有些情况下，我们无法用一条直线对实例进行划分。其实在很多情况下，数据集在空间中对应的向量不可被一个超平面区分开。这种时候我们要采取以下2个步骤： 利用一个非线性的映射把原数据集中的向量点转化到一个更高维度的空间中； 在这个高维度的空间中找一个线性的超平面来进行可区分处理。如上图表示的，将其从一维转为二维空间，然后在二维空间进行求解。动态演示 高纬空间（核方法）我们如何将原始数据转化到高纬空间呢，举个例子，在三维空间中的向量 X = (x1, x2, x3)转化到六维空间 Z 中去，令:新的决策超平面为:其中，W和Z 都是向量，这个超平面是线性的，解出 W 和b 后，带回原方程：上面转换的求解过程中，需要计算内积，而内积的复杂度非常高，这就需要使用到核方法。 核方法在处理非线性的情况中，SVM 选择一个核函数 ( kernel trick )，通过函数 k(.,.) 将数据映射到高维空间。支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维空间中构造出最优分离超平面。核函数例子： 假设两个向量：x = (a1, a2, a3) y = (b1, b2, b3) ，定义方程：1f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2, x2x3, x3x1, x3x2, x3x3) 假设核函数为:K(x, y) = (&lt;x, y&gt;)^2 , 且设x = (1, 2, 3) y=(4, 5, 6) 则有：123f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)f(y) = (16, 20, 24, 20, 25, 36, 24, 30, 36)&lt;f(x), f(y)&gt; = 16 + 40 + 72 + 40 + 100 + 180 + 72 + 180 + 324 = 1024 核函数计算为: K(x, y) = (4 + 10 + 18 ) ^2 = 32^2 = 1024 ，相同的结果，使用核函数计算会快很多。 核函数能很好的避开直接在高维空间中进行计算，而结果却是等价的。常见的几个核函数 (kernel functions)有 h度多项式核函数(polynomial kernel of degree h) 高斯径向基核函数(Gaussian radial basis function kernel) S型核函数(Sigmoid function kernel)如何选择使用哪个核函数？ 根据先验知识，比如图像分类，使用RBF，尝试不同的核函数，根据结果准确度而定。 多分类SVM 常见的是二分类模型，如果空间中有多个分类，比方有猫，狗，鸟。那么SVM就需要对每个类别做一次模型，是猫还是不是猫？是狗还是不是狗？是鸟还是不是鸟？从中选出可能性最大的。也可以两两做一次SVM：是猫还是狗？是猫还是鸟？是狗还是鸟？最后三个分类器投票决定。因此，多分类情况有两种做法： 对于每个类，有一个当前类和其他类的二类分类器( one-versus-the-rest approach) 两两做一次二类分类器( one-versus-one approace ) 代码实例几个点的分类在上面的二维空间中，有三个点：(1, 1) (2, 0) (2, 3) 。前两个点属于一类，第三个点属于另一类，我们使用这个例子来简单说明 sklearn 中 SVM 的初步用法：123456789101112131415from sklearn import svmx = [[2,0],[1,1],[2,3]]y = [0,0,1]classify = svm.SVC(kernel='linear')classify.fit(x,y)print(classify)#打印支持向量 output:[[1. 1.],[2. 3.]]print(classify.support_vectors_)#打印支持向量在数据实例中的索引 output:[1 2]print(classify.support_)#打印每一类中支持向量的个数 output:[1 1]print(classify.n_support_)#预测新的实例 output:[1]print(classify.predict([[2,3]])) 上面的例子中有两个点是支持向量：(1, 1) (2, 3)，通过clf.support_vectors_可以得到具体的点。这些支持向量点在数据集中的索引可以通过 clf.support_ 得到。在这个例子中，分界线的两侧各有一个支持向量，可以通过clf.n_support_得到，结果为 [1, 1]。 多个点的分类我们增加点的个数，并将超平面画出来，进行可视化展示。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import numpy as npfrom sklearn import svmimport pylab as plnp.random.seed(0) #设置相同的seed,每次产生的随机数相同'''生成2个（20*2）的二维数组，然后按列将它们连接组成（40*2）的二维数组生成的点服从正态分布，-[2,2],+[2,2]会让点靠左下方和右上方'''X = np.r_[np.random.randn(20,2)-[2,2],np.random.randn(20,2)+[2,2]]Y = [0]*20 + [1]*20 #给这40个点分类，标记为0和1#print(X.shape)#print(Y)classify = svm.SVC(kernel='linear') #构造SVM分类模型classify.fit(X,Y)'''利用上面的40个点来画一个二维的分类平面图，假设超平面方程为w0x + w1y + b = 0 转为点斜式就是: y = -(w0/w1)x - (b/w1) ：调用classify，获取w的值'''w = classify.coef_[0]a = -w[0]/w[1]xx = np.linspace(-5,5) #在（-5，5）区间内生成一些连续值，方便作图yy = a*xx - (classify.intercept_[0])/w[1]#print(w)#print(a)'''绘制经过支持向量，并与超平面平行的上下2条直线由于平行，因此斜率相同，只是截距不同'''b = classify.support_vectors_[0] #第一分类中的第一个支持向量点yy_down = a*xx + (b[1]-a*b[0])b = classify.support_vectors_[-1] #第二分类中的最后一个支持向量点yy_up = a*xx + (b[1]-a*b[0])#print(classify.support_vectors_)'''将所有的点、超平面和2条分界线绘制出来k-为实线，即超平面线k--为虚线，2条边界线'''pl.plot(xx, yy, 'k-')pl.plot(xx, yy_down, 'k--')pl.plot(xx, yy_up, 'k--')#单独标记出支持向量点pl.scatter(classify.support_vectors_[:, 0], classify.support_vectors_[:, 1], s=80, facecolors='none')pl.scatter(X[:, 0], X[:, 1], c=Y,cmap=pl.cm.Paired )pl.axis('tight')pl.show() 可视化结果如下： SVM人脸分类我们构建SVM模型分类人脸并进行可视化展示，数据集（lfw）默认存储在~/scikit_learn_data 文件夹中。代码讲解见注释。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129from __future__ import print_functionfrom time import timeimport loggingimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import fetch_lfw_peoplefrom sklearn.model_selection import GridSearchCVfrom sklearn.decomposition import PCAfrom sklearn.svm import SVCfrom sklearn import metrics#输出日志信息logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')'''下载数据集（国外名人）lfw，可以手动下载：https://ndownloader.figshare.com/files/5976015min_faces_per_person表示提取每类人超过这一数目的数据集#resize调整每张人脸图片的比例，默认是0.5'''lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)n_samples, h, w = lfw_people.images.shape # 返回数据集的实例数及图片宽和高X = lfw_people.data #获取每个实例的特征n_features = X.shape[1] #获取每个实例的特征数y = lfw_people.target #标签数据，及每个人的身份target_names = lfw_people.target_names #存储每个人的人名n_classes = target_names.shape[0] # 有几类人print("===== 数据集中信息 =====")print("数据个数(n_samples):", n_samples) # 1288print("特征个数，维度(n_features):", n_features) # 1850print("结果集类别个数(n_classes):", n_classes) # 7'''调用train_test_split方法拆分训练集合测试集test_size=0.25表示随机抽取25%的测试集'''X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)'''原始数据的特征向量维度非常高，意味着训练模型的复杂度非常高,我们要采用PCA降维，保存的组件数目，也即保留下来的特征个数，此处我们选择150'''n_components = 150print("Extracting the top %d eigenfaces from %d faces" % (n_components, X_train.shape[0]))t0 = time()pca = PCA(n_components=n_components,whiten=True).fit(X_train)print("done in %0.3fs" % (time() - t0))#降维后提取每个实例的特征点eigenfaces = pca.components_.reshape((n_components,h,w))print("Projecting the input data on the eigenfaces orthonormal basis")t0 = time()#把训练集和测试集特征向量转化为更低维的矩阵X_train_pca = pca.transform(X_train)X_test_pca = pca.transform(X_test)print("done in %0.3fs" % (time() - t0))#构建SVN分类模型print("Fitting the classifier to the training set")t0 = time()'''C是一个对错误部分的惩罚gamma的参数对不同核函数有不同的表现，gamma表示使用多少比例的特征点使用不同的c和不同值的gamma，进行多个量的尝试此处组成5*6的网格参数点，然后进行搜索，最后选出准确率最高模型'''param_grid= &#123;'C':[1e3, 5e3, 1e4, 5e4, 1e5], 'gamma':[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1] &#125;clf = GridSearchCV(SVC(kernel='rbf',class_weight='balanced'),param_grid)clf = clf.fit(X_train_pca,y_train)print("done in %0.3fs" % (time() - t0))print("Best estimator found by grid search:")print(clf.best_estimator_)#评估测试集print("Predicting people's names on the test set")t0 = time()y_pred = clf.predict(X_test_pca)print("done in %0.3fs" % (time() - t0))'''classification_report查看每一类的各种评价指标confusion_matrix(混淆矩阵) 是建一个 n x n 的方格，横行和纵行分别表示真实的每一组测试集的标记和测试集标记的差别，通常表示这些测试数据哪些对了，哪些错了。这个对角线表示了哪些值对了，对角线数字越多，就表示准确率越高。'''print(metrics.classification_report(y_test,y_pred,target_names=target_names))print(metrics.confusion_matrix(y_test, y_pred, labels=range(n_classes)))#将测试结果可视化展示def plot_gallery(images, titles, h, w, n_row=3, n_col=4): plt.figure(figsize=(1.8 * n_col, 2.4 * n_row)) # 定义画布的大小 plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35) # 调整图片显示位置 for i in range(n_row * n_col): plt.subplot(n_row, n_col, i + 1) #画布划分 plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray) # 图片显示 plt.title(titles[i], size=12) # 标题 #获取或设置x、y轴的当前刻度位置和标签 plt.xticks(()) plt.yticks(())#预测函数归类标签和实际归类标签打印#返回预测人脸和测试人脸姓名的对比titledef title(y_pred, y_test, target_names, i): ''' rsplit（' ',1）从右边开始以右边第一个空格为界，分成两个字符 组成一个list 此处代表把'姓'和'名'分开，然后把后面的姓提出来 末尾加[-1]代表引用分割后的列表最后一个元素 ''' pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1] true_name = target_names[y_test[i]].rsplit(' ', 1)[-1] return 'predicted: %s\ntrue: %s' % (pred_name, true_name)#预测出的人名prediction_titles = [title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])]#测试集的特征向量矩阵和要预测的人名打印plot_gallery(X_test, prediction_titles, h, w)#打印原图和预测的信息eigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]#调用plot_gallery函数打印出实际是谁，预测的谁，以及提取过特征的脸plot_gallery(eigenfaces, eigenface_titles, h, w)plt.show() 程序运行结果如下 1234567891011121314151617181920212223242526272829303132333435363738===== 数据集中信息 =====数据个数(n_samples): 1288特征个数，维度(n_features): 1850结果集类别个数(n_classes): 7Extracting the top 150 eigenfaces from 966 facesdone in 0.309sProjecting the input data on the eigenfaces orthonormal basisdone in 0.025sFitting the classifier to the training setdone in 23.043sBest estimator found by grid search:SVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)Predicting people's names on the test setdone in 0.047s precision recall f1-score support Ariel Sharon 0.81 0.62 0.70 21 Colin Powell 0.72 0.88 0.79 58 Donald Rumsfeld 0.66 0.73 0.69 26 George W Bush 0.88 0.86 0.87 130Gerhard Schroeder 0.81 0.69 0.75 32 Hugo Chavez 1.00 0.69 0.82 13 Tony Blair 0.81 0.83 0.82 42 micro avg 0.81 0.81 0.81 322 macro avg 0.81 0.76 0.78 322 weighted avg 0.82 0.81 0.81 322[[ 13 4 2 2 0 0 0] [ 0 51 1 3 1 0 2] [ 1 2 19 3 0 0 1] [ 1 10 5 112 2 0 0] [ 0 1 1 5 22 0 3] [ 1 1 0 0 0 9 2] [ 0 2 1 2 2 0 35]] 可视化结果展示如下 参考资料机器学习-支持向量机的SVMfetch_lfw_people安装失败]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>svm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之决策树]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树是一个类似于流程图的树结构：其中，每个内部结点表示在一个属性上的测试，每个分支代表一个属性输出，而每个树叶结点代表类或类分布。树的最顶层是根结点。 上图中是否出去玩取决于天气情况（sunny、overcast、rain）和空气湿度（humidity、windy）这2个属性的值。 信息熵决策树算法种类很多，本文主要介绍ID3算法。ID3算法在1970-1980年，由J.Ross. Quinlan提出。在介绍决策树算法前，我们先引入熵（entropy）的概念。 信息和抽象，具体该如何度量呢？1948年，香农提出了信息熵（entropy）的概念。一条信息的信息量大小和它的不确定性有直接的关系，要搞清楚一件非常非常不确定的事情，或者是我们一无所知的事情，需要了解大量信息==&gt;信息量的度量就等于不确定性的多少。我们用比特(bit)来衡量信息的多少，变量的不确定性越大，熵也就越大。计算公式如下，其中$P(x_i)$表示每种事件发生的可能性。 ID3算法 该算法在选择每一个属性判断结点的时候是根据信息增益(Information Gain)。通过下面公式可以计算A来作为节点分类获取了多少信息。$$Gain(A) = Info(D) - InfoA(D)$$ 上图是一个买车的实例，一个人是否买车取决于他的年龄、收入、信用度和他是否是学生。下图（右）是原始数据，下图（左）是简单的决策树划分。那我们是如何确定age作为第一个属性结点的呢？一共有14条数据，其中9人买车，5人没买车，所以根据公式可以算出$Info(D)$的值。接着我们计算选择age作为属性结点的信息熵。age有三个取值，其中youth有5条信息（2人买车，3人没买），middle_aged有4条信息（全部买车），senior有5条信息（3人买车，2人没买），所以根据公式可以算出$Info_{age}(D)$和$Gain(age)$的值，如下所示： 类似，$Gain(income) = 0.029, Gain(student) = 0.151, Gain(credit_rating)=0.048​$所以，选择age作为第一个根节点。重复。。。 最终我们得到了如下所示的决策树。 算法流程 从根节点出发，根节点包括所有的训练样本。 一个节点（包括根节点），若节点内所有样本均属于同一类别，那么将该节点就成为叶节点，并将该节点标记为样本个数最多的类别。 否则利用采用信息增益法来选择用于对样本进行划分的特征，该特征即为测试特征，特征的每一个值都对应着从该节点产生的一个分支及被划分的一个子集。在决策树中，所有的特征均为符号值，即离散值。如果某个特征的值为连续值，那么需要先将其离散化。 递归上述划分子集及产生叶节点的过程，这样每一个子集都会产生一个决策（子）树，直到所有节点变成叶节点。 递归操作的停止条件就是： （1） 一个节点中所有的样本均为同一类别，那么产生叶节点（2） 没有特征可以用来对该节点样本进行划分，这里用attribute_list=null为表示。此时也强制产生叶节点，该节点的类别为样本个数最多的类别（3） 没有样本能满足剩余特征的取值，即test_attribute=a_i 对应的样本为空。此时也强制产生叶节点，该节点的类别为样本个数最多的类别 l流程图如下所示： 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465from sklearn.feature_extraction import DictVectorizerimport csvfrom sklearn import treefrom sklearn import preprocessingimport pydotplus#读入csv文件Extradata = open("G:/PycharmProjects/Machine_Learning/Decision_Tree/AllElectronics.csv","r",encoding="utf-8")reader = csv.reader(Extradata)headers = next(reader) #逐行读取数据 python3中为nex(),python2 中为reader.next()#print(headers)featureList = [] #特征列表labelList = [] #标签列表for row in reader: labelList.append(row[len(row)-1]) #读取每行数据最后一列的标签，存入标签列表 rowDict = &#123;&#125; #创建字典，存储每行特征数据 for i in range(1,len(row)-1):#从第2列开始到倒数第2列读取特征数据 rowDict[headers[i]] = row[i] #对应键值对存储数据 featureList.append(rowDict) #将每行特征数据字典存入特征列表#print(featureList)#向量化特征数据vec = DictVectorizer()dum_X = vec.fit_transform(featureList).toarray()# print("dum_X: " + str(dum_X))# print(vec.get_feature_names())## print("labelList: " + str(labelList))##向量化标签数据label = preprocessing.LabelBinarizer()dum_Y = label.fit_transform(labelList)#print("dum_Y: " + str(dum_Y))#调用sklearn方法构建决策树classfiy = tree.DecisionTreeClassifier(criterion='entropy')classfiy = classfiy.fit(dum_X,dum_Y)#print("classfiy: " + str(classfiy))#将决策树存为dot文件with open("G:/PycharmProjects/Machine_Learning/Decision_Tree/AllElectronics.dot", 'w') as f: f = tree.export_graphviz(classfiy,feature_names=vec.get_feature_names(),out_file=f)#可视化决策树的模型，并存为pdf文件dot_data = tree.export_graphviz(classfiy,out_file=None)graph = pydotplus.graph_from_dot_data(dot_data )graph.write_pdf("aa.pdf")#验证决策树分类模型oneRow_X = dum_X[0,:]#print("oneRow_X: " + str(oneRow_X))newRow_X = oneRow_X#修改特征数据的值newRow_X[0] = 1newRow_X[2] = 0newRow_X[3] = 1newRow_X[4] = 0#print("newRow_X: " + str(newRow_X))finalRow_X = newRow_X.reshape(1,10) #将一位数组转换为二维数组predict_Y = classfiy.predict(finalRow_X) #调用决策树模型预测，注意，finalRow_X要求二维#predict_Y = classfiy.predict([[1., 0., 0., 0., 1., 1., 0., 0., 1., 0.]])print("predict_Y: " + str(predict_Y)) 代码中预测结果见下图： 最后生成的决策树模型见下图： 其他决策树算法 C4.5: QuinlanClassification and Regression Trees (CART): (L. Breiman, J. Friedman, R. Olshen, C. Stone)共同点：都是贪心算法，自上而下(Top-down approach)区别：属性选择度量方法不同： C4.5 （gain ratio), CART(gini index), ID3 (Information Gain) 算法 支持模型 树结构 特征选择 连续值处理 缺失值处理 剪枝 ID3 分类 多叉树 信息增益 不支持 不支持 不支持 C4.5 分类 多叉树 信息增益比 支持 支持 支持 CART 分类、回归 二叉树 基尼系数、均方差 支持 支持 支持 决策树的优缺点优点 简单直观，生成的决策树很直观。 基本不需要预处理，不需要提前归一化，处理缺失值。 既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。 可以处理多维度输出的分类问题。 相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释 可以交叉验证的剪枝来选择模型，从而提高泛化能力。 缺点 决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。 决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。 寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。4.有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。 参考资料 Scikit-learn中的决策树]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>desion tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之KNN]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-2-25-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8BKNN%2F</url>
    <content type="text"><![CDATA[K最近邻(k-Nearest Neighbor，KNN)分类算法，思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。 实例分析 有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形or红色小三角形），下面，我们就要解决这个问题：给这个绿色的圆分类。 俗话说物以类聚，人以群分，判别一个人是一个什么样品质特征的人，常常可以从他/她身边的朋友入手。我们不是要判别上图中那个绿色的圆是属于哪一类数据么，好说，从它的邻居下手。但一次性看多少个邻居呢？从上图中，你还能看到： 若K=3，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。 若K=5，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于蓝色的正方形一类。 我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 时间复杂度 KNN 算法本身简单有效，它是一种 lazy-learning 算法，分类器不需要使用训练集进行训练，训练时间复杂度为0。KNN 分类的计算复杂度和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为 n，那么 KNN 的分类时间复杂度为O(n)。 算法三要素 K 值的选择会对算法的结果产生重大影响。K值较小意味着只有与输入实例较近的训练实例才会对预测结果起作用，但容易发生过拟合；如果 K 值较大，优点是可以减少学习的估计误差，但缺点是学习的近似误差增大，这时与输入实例较远的训练实例也会对预测起作用，使预测发生错误。在实际应用中，K 值一般选择一个较小的数值，通常采用交叉验证的方法来选择最优的 K 值。随着训练实例数目趋向于无穷和 K=1 时，误差率不会超过贝叶斯误差率的2倍，如果K也趋向于无穷，则误差率趋向于贝叶斯误差率。 分类决策规则往往是多数表决，即由输入实例的 K 个最临近的训练实例中的多数类决定输入实例的类别。 距离度量一般采用Lp 距离，当p=2时，即为欧氏距离，在度量之前，应该将每个属性的值规范化，这样有助于防止具有较大初始值域的属性比具有较小初始值域的属性的权重过大。 算法实现 算法步骤 （1）计算已知类别数据集中的点与当前点之间的距离（2）按照距离递增次序排序（3）选取与当前点距离最小的K个点（4）确定前K个点所在类别出现的频率（5）返回前K个点出现频率最高的类别作为当前点的预测分类 代码实例 iris数据集，包含150条用例，每条用例共5列，前4列为特征数据，最后一列为标签数据。 简单实现调用sklearn自带的库1234567891011from sklearn import neighborsfrom sklearn import datasetsknn = neighbors.KNeighborsClassifier() # 申明对象iris = datasets.load_iris() # 导入数据print(iris)knn.fit(iris.data,iris.target) # 生成KNN模型predicit_label = knn.predict([[0.2,0.3,0.3,0.2]]) # 预测print(predicit_label) 复杂实现自己编写函数实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889import csvimport randomimport mathimport operator'''导入数据filename数据存储路径radio，按指定比例将数据划分为训练集和测试集'''def loadDateset(filename,radio,trainSet=[],testSet=[]): with open(filename,'rt') as csvfile: lines = csv.reader(csvfile) # 逐行读取数据 dataset = list(lines) # 转换为列表存储 for x in range(len(dataset)-1): # 循环每行数据，将前4个特征值存入数组 for y in range(4): dataset[x][y] = float(dataset[x][y]) if random.random()&lt;radio: # 取随机值，小于radio就划分到训练集 trainSet.append(dataset[x]) else: testSet.append(dataset[x])'''计算2个样例之间的距离（欧氏距离），length表示数据的维度'''def evaluateDistance(instance1,instance2,length): distance = 0 for x in range(length): # 循环每一维度，数值相减并对其平方，然后进行累加 distance += pow((instance1[x]-instance2[x]),2) return math.sqrt(distance) # 开方求距离'''对于一个实例，找到离他最近的k个实例'''def getNeighbors(trainSet,testInstance,k): distance = [] length = len(testInstance)-1 # 每个测试实例的维度 for x in range(len(trainSet)-1): # 训练集中每一个实例到测试实例的距离 dist = evaluateDistance(testInstance,trainSet[x],length) distance.append((trainSet[x],dist)) # 将每一个训练实例和其对应到测试实例的距离存储到列表 distance.sort(key=operator.itemgetter(1)) # operator模块提供的itemgetter函数用于获取距离维度的数据并排序 neighbors = [] # 存储离一个实例最近的几个实例 for x in range(k): # 取distance中前k个实例存储到neighbors neighbors.append(distance[x][0]) return neighbors'''在最近的K个实例中投票，少数服从多数，把要预测的实例归到多数那一类'''def getResponse(neighbors): classvotes = &#123;&#125; # 定义一个字典，存储每一类别的数目 for x in range(len(neighbors)): response = neighbors[x][-1] if response in classvotes: classvotes[response] += 1 else: classvotes[response] = 1 sortedVotes = sorted(classvotes.items(),key=operator.itemgetter(1),reverse=True) # 排序，输出数目最大的类别 return sortedVotes[0][0]'''计算测试集的准确率'''def getAccuracy(testSet,predictions): correct = 0 for x in range(len(testSet)): if testSet[x][-1] == predictions[x]: # 每行测试用例最后一列的标签与预测标签是否相等 correct += 1 return (correct/float(len(testSet)))*100.0def main(): trainSet = [] # 存储训练集 testSet = [] # 存储测试集 radio = 0.80 # 按4：1划分 loadDateset('G:/PycharmProjects/Machine_Learning/KNN/irisdata.txt',radio,trainSet,testSet) #导入数据并划分 print("trainSetNum: "+ str(len(trainSet))) print("testSetNum: "+ str(len(testSet))) predictions = [] k = 3 # 选取前k个最近的实例 for x in range(len(testSet)): # 循环预测测试集合的每个实例 neighbors = getNeighbors(trainSet,testSet[x],k) result = getResponse(neighbors) predictions.append(result) print('&gt;predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1])) accuracy = getAccuracy(testSet,predictions) print('Accuracy: ' + repr(accuracy) + '%')if __name__ == '__main__': main() 运行结果1234567891011121314151617181920212223242526272829trainSetNum: 124testSetNum: 26&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-versicolor', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'Accuracy: 92.3076923076923% 算法优缺点优点 精度高、对异常值不敏感、无数据输入假定。 KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。 缺点 样本分布不均衡时，如果一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。 适用的数据范围 数值型和标称型。标称型：一般在有限的数据中取，而且只存在‘是’和‘否’两种不同的结果（一般用于分类）数值型：可以在无限的数据中取，而且数值比较具体化，例如4.02,6.23这种值（一般用于回归分析） K-Means简介 如图所示，数据样本用圆点表示，每个簇的中心点用叉叉表示。(a) 刚开始时是原始数据，杂乱无章，没有label，看起来都一样，都是绿色的。(b) 假设数据集可以分为两类，令K=2，随机在坐标上选两个点，作为两个类的中心点。(c-f) 演示了聚类的两种迭代。先划分，把每个数据样本划分到最近的中心点那一簇；划分完后，更新每个簇的中心，即把该簇的所有数据点的坐标加起来去平均值。这样不断进行”划分—更新—划分—更新”，直到每个簇的中心不在移动为止。 KNN和K-Means的区别 KNN K-Means KNN是分类算法 K-Means是聚类算法 监督学习 非监督学习 喂给它的数据集是带label的数据，已经是完全正确的数据 喂给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序 没有明显的前期训练过程，属于memory-based learning 有明显的前期训练过程 K的含义：来了一个样本x，要给它分类，即求出它的y，就从数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c K的含义：K是人工固定好的数字，假设数据集合可以分为K个簇，由于是依靠人工定好，需要一点先验知识 两者相似点：都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法。 参考资料 KNN与K-Means的区别]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>knn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow入门学习2]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-2-24-Tensorflow%E5%85%A5%E9%97%A82%2F</url>
    <content type="text"><![CDATA[这篇博文主要是TensorFlow的一个简单入门，并介绍了如何实现Softmax Regression模型，来对MNIST数据集中的数字手写体进行识别。 然而，由于Softmax Regression模型相对简单，所以最终的识别准确率并不高。下面将针对MNIST数据集构建更加复杂精巧的模型，以进一步提高识别准确率。 深度学习模型TensorFlow很适合用来进行大规模的数值计算，其中也包括实现和训练深度神经网络模型。下面将介绍TensorFlow中模型的基本组成部分，同时将构建一个CNN模型来对MNIST数据集中的数字手写体进行识别。 基本设置在我们构建模型之前，我们首先加载MNIST数据集，然后开启一个TensorFlow会话(session)。 加载MNIST数据集TensorFlow中已经有相关脚本，来自动下载和加载MNIST数据集。（脚本会自动创建MNIST_data文件夹来存储数据集）。下面是脚本程序： 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets('MNIST_data', one_hot=True) 这里mnist是一个轻量级的类文件，存储了NumPy格式的训练集、验证集和测试集，它同样提供了数据中mini-batch迭代的功能。 开启TensorFlow会话TensorFlow后台计算依赖于高效的C++，与后台的连接称为一个会话(session)。TensorFlow中的程序使用，通常都是先创建一个图(graph)，然后在一个会话(session)里运行它。 这里我们使用了一个更为方便的类，InteractiveSession，这能让你在构建代码时更加灵活。InteractiveSession允许你做一些交互操作，通过创建一个计算流图(computation graph)来部分地运行图计算。当你在一些交互环境（例如IPython）中使用时将更加方便。如果你不是使用InteractiveSession，那么你要在启动一个会话和运行图计算前，创建一个整体的计算流图。 下面是如何创建一个InteractiveSession： 12import tensorflow as tfsess = tf.InteractiveSession() 计算流图(Computation Graph)为了在Python中实现高效的数值运算，通常会使用一些Python以外的库函数，如NumPy。但是，这样做会造成转换Python操作的开销，尤其是在GPUs和分布式计算的环境下。TensorFlow在这一方面（指转化操作）做了优化，它让我们能够在Python之外描述一个包含各种交互计算操作的整体流图，而不是每次都独立地在Python之外运行一个单独的计算，避免了许多的转换开销。这样的优化方法同样用在了Theano和Torch上。 所以，以上这样的Python代码的作用是简历一个完整的计算流图，然后指定图中的哪些部分需要运行。关于计算流图的更多具体使用见这里。 Softmax Regression模型见这篇博文。 CNN模型Softmax Regression模型在MNIST数据集上91%的准确率，其实还是比较低的。下面我们将使用一个更加精巧的模型，一个简单的卷积神经网络模型(CNN)。这个模型能够达到99.2%的准确率，尽管这不是最高的，但已经足够接受了。 权值初始化为了建立模型，我们需要先创建一些权值(w)和偏置(b)等参数，这些参数的初始化过程中需要加入一小部分的噪声以破坏参数整体的对称性，同时避免梯度为0.由于我们使用ReLU激活函数（详细介绍)），所以我们通常将这些参数初始化为很小的正值。为了避免重复的初始化操作，我们可以创建下面两个函数： 1234567def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) 卷积(Convolution)和池化(Pooling)TensorFlow同样提供了方便的卷积和池化计算。怎样处理边界元素？怎样设置卷积窗口大小？在这个例子中，卷积操作仅使用了滑动步长为1的窗口，使用0进行填充，所以输出规模和输入的一致；而池化操作是在2 * 2的窗口内采用最大池化技术(max-pooling)。为了使代码简洁，同样将这些操作抽象为函数形式： 123456def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') 其中，padding=&#39;SAME&#39;表示通过填充0，使得输入和输出的形状一致。 第一层：卷积层第一层是卷积层，卷积层将要计算出32个特征映射(feature map)，对每个5 * 5的patch。它的权值tensor的大小为[5, 5, 1, 32]. 前两维是patch的大小，第三维时输入通道的数目，最后一维是输出通道的数目。我们对每个输出通道加上了偏置(bias)。 12W_conv1 = weight_variable([5, 5, 1, 32])b_conv1 = bias_variable([32]) 为了使得图片与计算层匹配，我们首先reshape输入图像x为4维的tensor，第2、3维对应图片的宽和高，最后一维对应颜色通道的数目。（-1就是缺省值，就是先以你们合适，到时总数除以你们几个的乘积，我该是几就是几） 1x_image = tf.reshape(x, [-1,28,28,1]) 然后，使用weight tensor对x_image进行卷积计算，加上bias，再应用到一个ReLU激活函数，最终采用最大池化。 12h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)h_pool1 = max_pool_2x2(h_conv1) 第二层：卷积层为了使得网络有足够深度，我们重复堆积一些相同类型的层。第二层将会有64个特征，对应每个5 * 5的patch。 12345W_conv2 = weight_variable([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2) 全连接层到目前为止，图像的尺寸被缩减为7 * 7，我们最后加入一个神经元数目为1024的全连接层来处理所有的图像上。接着，将最后的pooling层的输出reshape为一个一维向量，与权值相乘，加上偏置，再通过一个ReLu函数。 12345W_fc1 = weight_variable([7 * 7 * 64, 1024])b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) 整个CNN的网络结构如下图： Dropout为了减少过拟合程度，在输出层之前应用dropout技术（即丢弃某些神经元的输出结果）。我们创建一个placeholder来表示一个神经元的输出在dropout时不被丢弃的概率。Dropout能够在训练过程中使用，而在测试过程中不使用。TensorFlow中的tf.nn.dropout操作能够利用mask技术处理各种规模的神经元输出。 12keep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) 输出层最终，我们用一个softmax层，得到类别上的概率分布。（与之前的Softmax Regression模型相同）。 1234W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) 模型训练和测试为了测试模型的性能，需要先对模型进行训练，然后应用在测试集上。和之前Softmax Regression模型中的训练、测试过程类似。区别在于： 用更复杂的ADAM最优化方法代替了之前的梯度下降； 增了额外的参数keep_prob在feed_dict中，以控制dropout的几率； 在训练过程中，增加了log输出功能（每100次迭代输出一次）。 下面是程序： 123456789101112131415cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))sess.run(tf.initialize_all_variables())for i in range(20000): batch = mnist.train.next_batch(50) if i%100 == 0: train_accuracy = accuracy.eval(feed_dict=&#123; x:batch[0], y_: batch[1], keep_prob: 1.0&#125;) print("step %d, training accuracy %g"%(i, train_accuracy)) train_step.run(feed_dict=&#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;)print("test accuracy %g"%accuracy.eval(feed_dict=&#123; x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0&#125;)) 最终，模型在测试集上的准确率大概为99.2%，性能上要优于之前的Softmax Regression模型。 完整代码及运行结果利用CNN模型实现手写体识别的完整代码如下： 123456789101112131415161718__author__ = 'chapter'import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datadef weight_varible(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial)def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') ​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)print("Download Done!")sess = tf.InteractiveSession()# parasW_conv1 = weight_varible([5, 5, 1, 32])b_conv1 = bias_variable([32])# conv layer-1x = tf.placeholder(tf.float32, [None, 784])x_image = tf.reshape(x, [-1, 28, 28, 1])h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)h_pool1 = max_pool_2x2(h_conv1)# conv layer-2W_conv2 = weight_varible([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2)# full connectionW_fc1 = weight_varible([7 * 7 * 64, 1024])b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)# dropoutkeep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)# output layer: softmaxW_fc2 = weight_varible([1024, 10])b_fc2 = bias_variable([10])y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)y_ = tf.placeholder(tf.float32, [None, 10])# model trainingcross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)correct_prediction = tf.equal(tf.arg_max(y_conv, 1), tf.arg_max(y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))sess.run(tf.initialize_all_variables())for i in range(20000): batch = mnist.train.next_batch(50) if i % 100 == 0: train_accuacy = accuracy.eval(feed_dict=&#123;x: batch[0], y_: batch[1], keep_prob: 1.0&#125;) print("step %d, training accuracy %g"%(i, train_accuacy)) train_step.run(feed_dict = &#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;)# accuacy on testprint("test accuracy %g"%(accuracy.eval(feed_dict=&#123;x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0&#125;))) 运行结果如下图： 参考资料 Tensorflow 实战 Google 深度学习框架TensorFlow——Mnist手写数字识别实战教程]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow入门学习]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-2-23-Tensorflow%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[TensorFlow 简介TensorFlow是Google在2015年11月份开源的人工智能系统（Github项目地址），是之前所开发的深度学习基础架构DistBelief的改进版本，该系统可以被用于语音识别、图片识别等多个领域。 官网上对TensorFlow的介绍是，一个使用数据流图(data flow graphs)技术来进行数值计算的开源软件库。数据流图中的节点，代表数值运算；节点节点之间的边，代表多维数据(tensors)之间的某种联系。你可以在多种设备（含有CPU或GPU）上通过简单的API调用来使用该系统的功能。TensorFlow是由Google Brain团队的研发人员负责的项目。 什么是数据流图(Data Flow Graph)数据流图是描述有向图中的数值计算过程。有向图中的节点通常代表数学运算，但也可以表示数据的输入、输出和读写等操作；有向图中的边表示节点之间的某种联系，它负责传输多维数据(Tensors)。图中这些tensors的flow也就是TensorFlow的命名来源。 节点可以被分配到多个计算设备上，可以异步和并行地执行操作。因为是有向图，所以只有等到之前的入度节点们的计算状态完成后，当前节点才能执行操作。 TensorFlow的特性1 灵活性 TensorFlow不是一个严格的神经网络工具包，只要你可以使用数据流图来描述你的计算过程，你可以使用TensorFlow做任何事情。你还可以方便地根据需要来构建数据流图，用简单的Python语言来实现高层次的功能。 2 可移植性 TensorFlow可以在任意具备CPU或者GPU的设备上运行，你可以专注于实现你的想法，而不用去考虑硬件环境问题，你甚至可以利用Docker技术来实现相关的云服务。 3 提高开发效率 TensorFlow可以提升你所研究的东西产品化的效率，并且可以方便与同行们共享代码。 4 支持语言选项 目前TensorFlow支持Python和C++语言。（但是你可以自己编写喜爱语言的SWIG接口） 5 充分利用硬件资源，最大化计算性能 基本使用你需要理解在TensorFlow中，是如何： 将计算流程表示成图； 通过Sessions来执行图计算； 将数据表示为tensors； 使用Variables来保持状态信息； 分别使用feeds和fetches来填充数据和抓取任意的操作结果； 概览TensorFlow是一种将计算表示为图的编程系统。图中的节点称为ops(operation的简称)。一个ops使用0个或以上的Tensors，通过执行某些运算，产生0个或以上的Tensors。一个Tensor是一个多维数组，例如，你可以将一批图像表示为一个四维的数组[batch, height, width, channels]，数组中的值均为浮点数。 TensorFlow中的图描述了计算过程，图通过Session的运行而执行计算。Session将图的节点们(即ops)放置到计算设备(如CPUs和GPUs)上，然后通过方法执行它们；这些方法执行完成后，将返回tensors。在Python中的tensor的形式是numpy ndarray对象，而在C/C++中则是tensorflow::Tensor. 图计算TensorFlow程序中图的创建类似于一个 [施工阶段]，而在 [执行阶段] 则利用一个session来执行图中的节点。很常见的情况是，在 [施工阶段] 创建一个图来表示和训练神经网络，而在 [执行阶段] 在图中重复执行一系列的训练操作。 创建图在TensorFlow中，Constant是一种没有输入的ops，但是你可以将它作为其他ops的输入。Python库中的ops构造器将返回构造器的输出。TensorFlow的Python库中有一个默认的图，将ops构造器作为节点，更多可了解Graph Class文档。 见下面的示例代码： 12345678910111213141516import tensorflow as tf# Create a Constant op that produces a 1x2 matrix. The op is# added as a node to the default graph.## The value returned by the constructor represents the output# of the Constant op.matrix1 = tf.constant([[3., 3.]])# Create another Constant that produces a 2x1 matrix.matrix2 = tf.constant([[2.],[2.]])# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.# The returned value, 'product', represents the result of the matrix# multiplication.product = tf.matmul(matrix1, matrix2) 默认的图(Default Graph)现在有了三个节点：两个 Constant()ops和一个matmul()op。为了得到这两个矩阵的乘积结果，还需要在一个session中启动图计算。 在Session中执行图计算见下面的示例代码，更多可了解Session Class： 1234567891011121314151617181920# Launch the default graph.sess = tf.Session()# To run the matmul op we call the session 'run()' method, passing 'product'# which represents the output of the matmul op. This indicates to the call# that we want to get the output of the matmul op back.## All inputs needed by the op are run automatically by the session. They# typically are run in parallel.## The call 'run(product)' thus causes the execution of threes ops in the# graph: the two constants and matmul.## The output of the op is returned in 'result' as a numpy `ndarray` object.result = sess.run(product)print(result)# ==&gt; [[ 12.]]# Close the Session when we're done.sess.close() Sessions最后需要关闭，以释放相关的资源；你也可以使用with模块，session在with模块中自动会关闭： 123with tf.Session() as sess: result = sess.run([product]) print(result) TensorFlow的这些节点最终将在计算设备(CPUs,GPus)上执行运算。如果是使用GPU，默认会在第一块GPU上执行，如果你想在第二块多余的GPU上执行： 123456with tf.Session() as sess: with tf.device("/gpu:1"): matrix1 = tf.constant([[3., 3.]]) matrix2 = tf.constant([[2.],[2.]]) product = tf.matmul(matrix1, matrix2) ... device中的各个字符串含义如下： &quot;/cpu:0&quot;: 你机器的CPU； &quot;/gpu:0&quot;: 你机器的第一个GPU； &quot;/gpu:1&quot;: 你机器的第二个GPU； 关于TensorFlow中GPU的使用见这里。 交互环境下的使用以上的python示例中，使用了Session和Session.run()来执行图计算。然而，在一些Python的交互环境下(如IPython中)，你可以使用InteractiveSession类，以及Tensor.eval()、Operation.run()等方法。例如，在交互的Python环境下执行以下代码： 1234567891011121314151617# Enter an interactive TensorFlow Session.import tensorflow as tfsess = tf.InteractiveSession()x = tf.Variable([1.0, 2.0])a = tf.constant([3.0, 3.0])# Initialize 'x' using the run() method of its initializer op.x.initializer.run()# Add an op to subtract 'a' from 'x'. Run it and print the resultsub = tf.sub(x, a)print(sub.eval())# ==&gt; [-2. -1.]# Close the Session when we're done.sess.close() TensorsTensorFlow中使用tensor数据结构（实际上就是一个多维数据）表示所有的数据，并在图计算中的节点之间传递数据。一个tensor具有固定的类型、级别和大小，更加深入理解这些概念可参考Rank, Shape, and Type。 变量(Variables)变量在图执行的过程中，保持着自己的状态信息。下面代码中的变量充当了一个简单的计数器角色： 123456789101112131415161718192021222324252627282930# Create a Variable, that will be initialized to the scalar value 0.state = tf.Variable(0, name="counter")# Create an Op to add one to `state`.one = tf.constant(1)new_value = tf.add(state, one)update = tf.assign(state, new_value)# Variables must be initialized by running an `init` Op after having# launched the graph. We first have to add the `init` Op to the graph.init_op = tf.initialize_all_variables()# Launch the graph and run the ops.with tf.Session() as sess: # Run the 'init' op sess.run(init_op) # Print the initial value of 'state' print(sess.run(state)) # Run the op that updates 'state' and print 'state'. for _ in range(3): sess.run(update) print(sess.run(state))# output:# 0# 1# 2# 3 赋值函数assign()和add()函数类似，直到session的run()之后才会执行操作。与之类似的，一般我们会将神经网络模型中的参数表示为一系列的变量，在模型的训练过程中对变量进行更新操作。 抓取(Fetches)为了抓取ops的输出，需要先执行session的run函数。然后，通过print函数打印状态信息。 123456789101112input1 = tf.constant(3.0)input2 = tf.constant(2.0)input3 = tf.constant(5.0)intermed = tf.add(input2, input3)mul = tf.mul(input1, intermed)with tf.Session() as sess: result = sess.run([mul, intermed]) print(result)# output:# [array([ 21.], dtype=float32), array([ 7.], dtype=float32)] 所有tensors的输出都是一次性 [连贯] 执行的。 填充(Feeds)TensorFlow也提供这样的机制：先创建特定数据类型的占位符(placeholder)，之后再进行数据的填充。例如下面的程序： 123456789input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)output = tf.mul(input1, input2)with tf.Session() as sess: print(sess.run([output], feed_dict=&#123;input1:[7.], input2:[2.]&#125;))# output:# [array([ 14.], dtype=float32)] 如果不对placeholder()的变量进行数据填充，将会引发错误，更多的例子可参考MNIST fully-connected feed tutorial (source code)。 示例：曲线拟合下面是一段使用Python写的，曲线拟合计算。官网将此作为刚开始介绍的示例程序。 1234567891011121314151617181920212223242526272829303132# 简化调用库名import tensorflow as tfimport numpy as np# 模拟生成100对数据对, 对应的函数为y = x * 0.1 + 0.3x_data = np.random.rand(100).astype("float32")y_data = x_data * 0.1 + 0.3# 指定w和b变量的取值范围（注意我们要利用TensorFlow来得到w和b的值）W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))b = tf.Variable(tf.zeros([1]))y = W * x_data + b# 最小化均方误差loss = tf.reduce_mean(tf.square(y - y_data))optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss)# 初始化TensorFlow参数init = tf.initialize_all_variables()# 运行数据流图（注意在这一步才开始执行计算过程）sess = tf.Session()sess.run(init)# 观察多次迭代计算时，w和b的拟合值for step in xrange(201): sess.run(train) if step % 20 == 0: print(step, sess.run(W), sess.run(b))# 最好的情况是w和b分别接近甚至等于0.1和0.3 MNIST手写体识别任务下面我们介绍一个神经网络中的经典示例，MNIST手写体识别。这个任务相当于是机器学习中的HelloWorld程序。 MNIST数据集介绍MNIST是一个简单的图片数据集（数据集下载地址），包含了大量的数字手写体图片。下面是一些示例图片： MNIST数据集是含标注信息的，以上图片分别代表5, 0, 4和1。 由于MNIST数据集是TensorFlow的示例数据，所以我们不必下载。只需要下面两行代码，即可实现数据集的读取工作： 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("MNIST_data/", one_hot=True) MNIST数据集一共包含三个部分：训练数据集(55,000份，mnist.train)、测试数据集(10,000份，mnist.test)和验证数据集(5,000份，mnist.validation)。一般来说，训练数据集是用来训练模型，验证数据集可以检验所训练出来的模型的正确性和是否过拟合，测试集是不可见的（相当于一个黑盒），但我们最终的目的是使得所训练出来的模型在测试集上的效果（这里是准确性）达到最佳。 MNIST中的一个数据样本包含两块：手写体图片和对于的label。这里我们用xs和ys分别代表图片和对应的label，训练数据集和测试数据集都有xs和ys，我们使用 mnist.train.images 和 mnist.train.labels 表示训练数据集中图片数据和对于的label数据。 一张图片是一个28*28的像素点矩阵，我们可以用一个同大小的二维整数矩阵来表示。如下： 但是，这里我们可以先简单地使用一个长度为28 * 28 = 784的一维数组来表示图像，因为下面仅仅使用softmax regression来对图片进行识别分类（尽管这样做会损失图片的二维空间信息，所以实际上最好的计算机视觉算法是会利用图片的二维信息的）。 所以MNIST的训练数据集可以是一个形状为55000 * 784位的tensor，也就是一个多维数组，第一维表示图片的索引，第二维表示图片中像素的索引（”tensor”中的像素值在0到1之间）。如下图： MNIST中的数字手写体图片的label值在1到9之间，是图片所表示的真实数字。这里用One-hot vector来表述label值，vector的长度为label值的数目，vector中有且只有一位为1，其他为0.为了方便，我们表示某个数字时在vector中所对应的索引位置设置1，其他位置元素为0. 例如用[0,0,0,1,0,0,0,0,0,0]来表示3。所以，mnist.train.labels是一个55000 * 10的二维数组。如下： 以上是MNIST数据集的描述及TensorFlow中表示。下面介绍Softmax Regression模型。 Softmax Regression模型数字手写体图片的识别，实际上可以转化成一个概率问题，如果我们知道一张图片表示9的概率为80%，而剩下的20%概率分布在8，6和其他数字上，那么从概率的角度上，我们可以大致推断该图片表示的是9. Softmax Regression是一个简单的模型，很适合用来处理得到一个待分类对象在多个类别上的概率分布。所以，这个模型通常是很多高级模型的最后一步。 Softmax Regression大致分为两步（暂时不知道如何合理翻译，转原话）： Step 1: add up the evidence of our input being in certain classes;Step 2: convert that evidence into probabilities. 为了利用图片中各个像素点的信息，我们将图片中的各个像素点的值与一定的权值相乘并累加，权值的正负是有意义的，如果是正的，那么表示对应像素值（不为0的话）对表示该数字类别是积极的；否则，对应像素值(不为0的话)对表示该数字类别是起负面作用的。下面是一个直观的例子，图片中蓝色表示正值，红色表示负值（蓝色区域的形状趋向于数字形状）： 最后，我们在一个图片类别的evidence(不知如何翻译..)中加入偏置(bias)，加入偏置的目的是加入一些与输入独立无关的信息。所以图片类别的evidence可表示为 $$ evidence_{i}=\sum _{j}W_{ij}x_{j}+b_{i} $$ 其中，\( W_i \) 和 \( b_i \) 分别为类别 \( i \) 的权值和偏置，\( j \) 是输入图片 \( x \) 的像素索引。然后，我们将得到的evidence值通过一个”softmax”函数转化为概率值 \( y \) : $$ y = softmax(evidence) $$ 这里softmax函数的作用相当于是一个转换函数，它的作用是将原始的线性函数输出结果以某种方式转换为我们需要的值，这里我们需要0-9十个类别上的概率分布。softmax函数的定义如下： $$ softmax(x) = normalize(exp(x)) ​$$ 具体计算方式如下 $$ softmax(x)_{i} = \dfrac {exp\left( x_{i}\right) } {\Sigma _{j}exp\left( x_{j}\right) } $$ 这里的softmax函数能够得到类别上的概率值分布，并保证所有类别上的概率值之和为1. 下面的图示将有助于你理解softmax函数的计算过程： 如果我们将这个过程公式化，将得到 实际的计算中，我们通常采用矢量计算的方式，如下 也可以简化成 $$ y = softmax( Wx + b ) ​$$ Softmax Regression的程序实现为了在Python中进行科学计算工作，我们常常使用一些独立库函数包，例如NumPy来实现复杂的矩阵计算。但是由于Python的运行效率并不够快，所以常常用一些更加高效的语言来实现。但是，这样做会带来语言转换（例如转换回python操作）的开销。TensorFlow在这方面做了一些优化，可以对你所描述的一系列的交互计算的流程完全独立于Python之外，从而避免了语言切换的开销。 为了使用TensorFlow，我们需要引用该库函数 1import tensorflow as tf 我们利用一些符号变量来描述交互计算的过程，创建如下 1x = tf.placeholder(tf.float32, [None, 784]) 这里的 \( x \) 不是一个特定的值，而是一个占位符，即需要时指定。如前所述，我们用一个1 * 784维的向量来表示一张MNIST中的图片。我们用[None, 784]这样一个二维的tensor来表示整个MNIST数据集，其中None表示可以为任意值。 我们使用Variable(变量)来表示模型中的权值和偏置，这些参数是可变的。如下， 12W = tf.Variable(tf.zeros([784, 10]))b = tf.Variable(tf.zeros([10])) 这里的W和b均被初始化为0值矩阵。W的维数为784 * 10，是因为我们需要将一个784维的像素值经过相应的权值之乘转化为10个类别上的evidence值；b是十个类别上累加的偏置值。 实现softmax regression模型仅需要一行代码，如下 1y = tf.nn.softmax(tf.matmul(x, W) + b) 其中，matmul函数实现了 x 和 W 的乘积，这里 x 为二维矩阵，所以放在前面。可以看出，在TensorFlow中实现softmax regression模型是很简单的。 模型的训练在机器学习中，通常需要选择一个代价函数（或者损失函数），来指示训练模型的好坏。这里，我们使用交叉熵函数（cross-entropy）作为代价函数，交叉熵是一个源于信息论中信息压缩领域的概念，但是现在已经应用在多个领域。它的定义如下： $$ H_{y’}\left( y\right) = -\sum _{i}y_{i}’\log \left( y_{i}\right) $$ 这里 \( y \) 是所预测的概率分布，而 \( y’ \) 是真实的分布(one-hot vector表示的图片label)。直观上，交叉熵函数的输出值表示了预测的概率分布与真实的分布的符合程度。更加深入地理解交叉熵函数，可参考这篇博文。 为了实现交叉熵函数，我们需要先设置一个占位符在存放图片的正确label值， 1y_ = tf.placeholder(tf.float32, [None, 10]) 然后得到交叉熵，即\( -\sum y’\log \left( y\right) \)： 1cross_entropy = -tf.reduce_sum(y_*tf.log(y)) 注意，以上的交叉熵不是局限于一张图片，而是整个可用的数据集。 接下来我们以代价函数最小化为目标，来训练模型以得到相应的参数值(即权值和偏置)。TensorFlow知道你的计算过程，它会自动利用后向传播算法来得到相应的参数变化，对代价函数最小化的影响作用。然后，你可以选择一个优化算法来决定如何最小化代价函数。如下， 1train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) 在这里，我们使用了一个学习率为0.01的梯度下降算法来最小化代价函数。梯度下降是一个简单的计算方式，即使得变量值朝着减小代价函数值的方向变化。TensorFlow也提供了许多其他的优化算法，仅需要一行代码即可实现调用。 TensorFlow提供了以上简单抽象的函数调用功能，你不需要关心其底层实现，可以更加专心于整个计算流程。在模型训练之前，还需要对所有的参数进行初始化： 1init = tf.initialize_all_variables() 我们可以在一个Session里面运行模型，并且进行初始化： 12sess = tf.Session()sess.run(init) 接下来，进行模型的训练 123for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;) 每一次的循环中，我们取训练数据中的100个随机数据，这种操作成为批处理(batch)。然后，每次运行train_step时，将之前所选择的数据，填充至所设置的占位符中，作为模型的输入。 以上过程成为随机梯度下降，在这里使用它是非常合适的。因为它既能保证运行效率，也能一定程度上保证程序运行的正确性。（理论上，我们应该在每一次循环过程中，利用所有的训练数据来得到正确的梯度下降方向，但这样将非常耗时）。 模型的评价怎样评价所训练出来的模型？显然，我们可以用图片预测类别的准确率。 首先，利用tf.argmax()函数来得到预测和实际的图片label值，再用一个tf.equal()函数来判断预测值和真实值是否一致。如下： 1correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) correct_prediction是一个布尔值的列表，例如 [True, False, True, True]。可以使用tf.cast()函数将其转换为[1, 0, 1, 1]，以方便准确率的计算（以上的是准确率为0.75）。 1accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float")) 最后，我们来获取模型在测试集上的准确率， 1print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)) Softmax regression模型由于模型较简单，所以在测试集上的准确率在91%左右，这个结果并不算太好。通过一些简单的优化，准确率可以达到97%，目前最好的模型的准确率为99.7%。（这里有众多模型在MNIST数据集上的运行结果）。 完整代码及运行结果利用Softmax模型实现手写体识别的完整代码如下： 12345678910111213141516171819202122232425262728293031323334353637__author__ = 'chapter'import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("MNIST_data/", one_hot=True)print("Download Done!")x = tf.placeholder(tf.float32, [None, 784])# parasW = tf.Variable(tf.zeros([784, 10]))b = tf.Variable(tf.zeros([10]))y = tf.nn.softmax(tf.matmul(x, W) + b)y_ = tf.placeholder(tf.float32, [None, 10])# loss funccross_entropy = -tf.reduce_sum(y_ * tf.log(y))train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)# initinit = tf.initialize_all_variables()sess = tf.Session()sess.run(init)# trainfor i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)correct_prediction = tf.equal(tf.arg_max(y, 1), tf.arg_max(y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))print("Accuarcy on Test-dataset: ", sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)) 运行结果如下图： 参考资料 TensorFlow官方帮助文档Tensorflow之MNIST解析]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker集群使用文档]]></title>
    <url>%2FDocker%E5%AE%B9%E5%99%A8%2F2019-2-21-Docker%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[用户管理 (管理员权限)添加docker用户组:1sudo groupadd -g 344 docker 添加用户到用户组：1sudo usermod -a -G 用户组 用户 从用户组中删除用户1gpasswd -d 用户 用户组 镜像的基本操作列出本地镜像1docker images 各个选项说明:• REPOSITORY：表示镜像的仓库源（不唯一）• TAG：镜像的标签(不唯一，可以自己设定)• IMAGE ID：镜像ID（唯一）• CREATED：镜像创建时间• SIZE：镜像大小同一个镜像ID可以有多个仓库源和标签，如图中红框所示。 查找镜像 我们可以从Docker Hub网站来搜索镜像，Docker Hub网址为：https://hub.docker.com/我们也可以使用docker search命令来搜索镜像。比如我们需要一个httpd的镜像来作为我们的web服务。我们可以通过docker search命令搜索httpd来寻找适合我们的镜像。 1docker search httpd NAME:镜像仓库源的名称DESCRIPTION:镜像的描述OFFICIAL:是否docker官方发布 下载镜像 当我们在本地主机上使用一个不存在的镜像时 Docker 就会自动下载这个镜像。如果我们想预先下载这个镜像，我们可以使用docker pull命令来下载它。此处以ubuntu:15.10为例,其中15.10为标签，若不写，会默认下载最新的镜像，标签为latest。 1docker pull 镜像名(:标签) 设置镜像标签1docker tag 原始镜像名 新镜像名:标签 发现镜像ID为00a10af6cf18的镜像多了一个新的标签 liufan。 删除镜像 当我们删除某一镜像时，会先尝试删除所有指向该镜像的标签，然后删除该镜像本身。1.若一个镜像有多个标签，我们只想删除已经没用的标签 1docker rmi 仓库源(liufan): 镜像标签(lf) 删除前后我们发现liufan:lf已经被删除 2.彻底删除镜像1docker rmi –f 镜像ID（以8c811b4aec35为例）（不建议-f强制删除） 我们发现8c811b4aec35这个镜像已经被彻底删除（包含所有指向这个镜像的标签） 3.若想删除的镜像有基于它创建的容器存在时，镜像文件是默认无法删除的。（容器会在下面章节有所讲解）1docker run -it --name liufan ubuntu/numpy /bin/bash 我们基于ubuntu/numpy这个镜像创建了一个名为liufan的容器。下面我们退出容器，尝试删除这个镜像，docker会提示有容器在运行，无法删除：若想强制删除，可使用2中的 docker rmi –f 镜像ID，但不建议这样做，因为有容器依赖这个镜像，强制删除会有遗留问题（强制删除的镜像换了新的ID继续存在系统中） 导入导出镜像导出1docker save 镜像(busybox) &gt; 存储位置(/home/lf/aa.tar) 已经在对应目录生成压缩文件 先把本地的busybox镜像删除，然后尝试导入刚刚导出的压缩镜像1docker rmi busybox &amp;&amp; docker images 导入1docker load &lt; (镜像存储位置)/home/lf/aa.tar 我们发现busybox镜像已经成功导入。 注：当已有的镜像不能满足我们的需求时，我们需要自己制作镜像，主要通过下面2中方式：1） 通过Dockerfile文件制作镜像（较难）2） 基于一个原始镜像创建一个容器，在容器里面进行一些操作（安装一些框架或者软件包），然后退出容器，利用commit命令提交生成新的镜像 （简单）### 容器基本操作&gt; 容器是镜像的一个运行实例，它是基于镜像创建的。#### 新建容器12docker create -it --name lf tensorflowdocker ps -a&gt; &gt; 可以看见我们成功创建了一个名为lf，基于tensorflow镜像的容器。使用docker create 命令新建的容器处于停止状态，可以用如下命令来启动并进入它。12docker start lfdocker attach lf#### 启动容器&gt; 启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（stopped）的容器重新启动。1docker run -it --name liufan ubuntu/numpy /bin/bash上述命令等价于先执行docker create,再执行docker start命令。上面我们以交互模式创建了一个基于ubuntu/numpy镜像，名为liufan的容器。&gt; 其中，-i：表示让容器的标准输入保持打开，&gt; -t：让docker分配一个伪终端并绑定到容器的标准输入上，&gt; /bin/bash：不是必要选项，只是在表明创建容器的时候并运行了bash应用，方便我们进入容器内部，不写也可以，不过那就要用其他命令进入容器了。（docker中必须要保持一个进程的运行，要不然整个容器就会退出）我们可以按Ctrl+d或输入exit命令来退出容器。退出后该容器就会处于终止状态（stopped），可通过3.1中的start和attach重新进入容器。#### 查看终止删除容器1docker ps // 查看所有正在运行容器1docker ps -a // 查看所有容器1docker ps -a -q // 查看所有容器ID1docker stop containerId // containerId 是容器的ID或者名字，一个或多个1docker rm containerId // containerId 是容器的ID或者名字，一个或多个1docker rm containerId // containerId 是容器的ID或者名字，一个或多个可以看到lf、wh这两个容器已经被删除1docker stop $(docker ps -a -q) // stop停止所有容器1docker rm $(docker ps -a -q) // remove删除所有容器注：删除容器时必须保证容器是终止态（stopped），若不是先进行docker stop操作再进行docker rm操作，可以-f强制删除但不建议。#### 进入容器&gt; 1.attach命令使用attach命令有时候并不方便。当多个窗口同时attach到同一个容器的时候，所有的窗口都会同步显示。当某个窗口因命令阻塞时，其他窗口就无法执行操作了。&gt; 2.exec命令docker自1.3版本起，提供了一个更加方便的工具exec，可以直接在容器内部运行命令，例如进入到刚创建的容器中，并启动一个bash#### 导入和导出容器1docker run -it --name liufan ubuntu/numpy /bin/bash我们基于ubuntu/numpy镜像创建了一个名为liufan的容器，下面将它&gt; 导出：1docker export 容器名(liufan) &gt; 存储地址(/home/lf/aa.tar)我们将liufan这个容器导出本地并压缩命名为aa.tar文件。&gt; 导入：先将liufan容器删除在尝试导入12docker stop liufan &amp;&amp;docker rm liufan &amp;&amp;docker ps -adocker import /home/lf/aa.tar test/ubuntu:lf我们可以看到刚刚的容器压缩文件已经成功导入，命名为test/ubuntu:lf镜像。前面第一章中的1.6节中，我们介绍过用docker load命令来导入一个镜像文件，其实这边也可以用docker import命令来导入一个容器到本地镜像库。两者的区别是：docker import：丢弃了所有的历史记录和元数据信息，仅保存容器当时的快照状态。在导入的时候可以重新制定标签等元数据信息。docker load：将保存完整记录，体积较大。### 代码实例（以Tensorflow为例）&gt; 上面两章我介绍了镜像和容器的关系和它们的一些基本操作，接下来我将介绍如何在创建的容器里面运行我们的代码。集群上有Tensorflow、Pytorch、Caffe、MXNet等深度学习框架的镜像，此处我已Tensorflow为例，介绍如何在容器里运行我们的代码。#### 创建容器1docker run -it --name liufan bluesliuf/tensorflow /bin/bash我们基于bluesliuf/tensorflow这个镜像创建了一个名为liufan的镜像，进入容器ls查看目录列表，发现此时的容器就类似一个Linux环境，默认的用户权限为root权限。问题：我们的代码和数据集都在本地机器上，如何放到容器内部呢？直接复制困难并且耗时，如果我们的数据集过大。Docker提供了一种方法：挂载。将我们的本地目录挂载到容器内部，实现本机目录文件和容器目录文件共享。#### 挂载本地目录到容器&gt; 注：查询资料发现不能先创建容器，再挂载本地目录，两者必须同时进行，于是我们重新创建容器并挂载本地目录。我的代码和数据集都放在本机/home/lf/lf/catdogs目下，下面将它挂载到容器内。创建容器有2种方式-v:挂载的命令参数红色冒号前：本地目录的绝对路径红色冒号后：容器挂载本地目录的绝对路径蓝色部分表示容器需要使用GPU 时将显卡驱动映射到容器中，默认参数不用修改，如果不使用GPU 可以不加蓝色部分name: 创建的容器名bluesliuf/tensorflow:基于的镜像不调用GPU（本机）：调用GPU（集群）：可以看见我们已经成功将本地目录挂载到了我们指定的容器内部位置。运行代码（本机）：注:本地代码里面通常会有数据集的读取路径，一些生成日志文件的存储路径，我们要对它进行修改，换为容器内读取和存储路径。再去容器内部看，本地的修改已经同步到容器内了。在本地修改文件和容器内修改文件都行，一处修改两者都会同步修改。但建议在本地修改，因为本地修改起来方便，容器内一般用vim编辑器，较为不便。在终端输入命令：python 代码文件名（此处我是tr aining.py） 不调用GPU（本机）： 可以看见代码已经成功运行，并且相应的日志文件也存储到本地目录（容器目录当然也有，两者是同步共享的）此外，docker还提供了类似screen，可以让容器在后台运行的功能，退出时如果想继续运行：按顺序按【ctrl+p】【ctrl+q】，下次再使用docker attach 或者docker exec进入容器，可以看见我们的程序还在继续运行。例如： 调用GPU（本机）：在后台运行和上面一样，也是利用【ctrl+p】【ctrl+q】。 数据卷挂载Docker针对挂载目录还提供了一种高级的用法。叫数据卷。 数据卷：“其实就是一个正常的容器，专门用来提供数据卷供其它容器挂载的”。感觉它就像是由一个容器定义的一个数据挂载信息。其他的容器启动可以直接挂载数据卷容器中定义的挂载信息。示例如下： 1.创建一个普通的容器，名为wuhao，并将本地的文件目录挂载到了容器，接下来把这个容器当做一个数据卷。1docker run -v /home/lf/lf/catdogs:/var/catdogs --name wuhao bluesliuf/tensorflow /bin/bash 2.再创建一个新的容器，来使用这个数据卷。1docker run -it --volumes-from wuhao --name lf bluesliuf/tensorflow /bin/bash –volumes-from用来指定要从哪个数据卷来挂载数据。我们可以发现通过wuhao这个容器（数据卷），我们成功的将本地目录也挂载到了lf这个容器内。 通过数据卷挂载目录更具有优势。1） 我们只需先创建一个容器并挂载本地目录，将其看成数据卷，当我们其他容器也需要挂载同样目录的时候，我们只需要利用–volumes-from就可以实现。2） 当我们需要挂载的本地目录发生改变时，我们只需要修改作为数据卷那个容器挂载的本地目录即可（类似一个全局变量），而无须一个个修改其他容器的本地挂载目录。 挂载成功后。运行代码步骤与上面一样。]]></content>
      <categories>
        <category>Docker容器</category>
      </categories>
      <tags>
        <tag>docker使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的常见模型（GAN）]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-2-20-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B-GAN%2F</url>
    <content type="text"><![CDATA[GAN的来源 14年Goodfellow提出Generative adversarial nets即生成式对抗网络，它要解决的问题是如何从训练样本中学习出新样本，训练样本是图片就生成新图片，训练样本是文章就输出新文章等等。 GANs简单的想法就是用两个模型， 一个生成模型，一个判别模型。判别模型用于判断一个给定的图片是不是真实的图片（从数据集里获取的图片），生成模型的任务是去创造一个看起来像真的图片一样的图片，有点拗口，就是说模型自己去产生一个图片，可以和你想要的图片很像。而在开始的时候这两个模型都是没有经过训练的，这两个模型一起对抗训练，生成模型产生一张图片去欺骗判别模型，然后判别模型去判断这张图片是真是假，最终在这两个模型训练的过程中，两个模型的能力越来越强，最终达到稳态。 GAN的基本组成 GAN 模型中的两位博弈方分别由生成式模型（generative model）和判别式模型（discriminative model）充当。 生成模型： G 捕捉样本数据的分布，用服从某一分布（均匀分布，高斯分布等）的噪声 z 生成一个类似真实训练数据的样本，追求效果是越像真实样本越好； 判别模型: D 是一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率，如果样本来自于真实的训练数据，D 输出大概率，否则，D 输出小概率。 可以做如下类比：生成网络 G 好比假币制造团伙，专门制造假币，判别网络 D 好比警察，专门检测使用的货币是真币还是假币，G 的目标是想方设法生成和真币一样的货币，使得 D 判别不出来，D 的目标是想方设法检测出来 G 生成的假币。 上图是GAN网络的思想导图，我们用1代表真实数据，0来代表生成的假数据。对于判别器D来说，对于真实数据，它要尽可能判别正确输出值1；而对于生成器G，根据随机噪音向量z生成假数据也输入判别器D，对于这些假数据，判别器要尽可能输出值0。 GAN的训练过程可以看成一个博弈的过程，也可以看成2个人在玩一个极大极小值游戏，可以用如下公式表示： $$\min \limits_{G}\max\limits_{D}GAN(D,G)=E_{x\sim P_{data}(x)}[logD(x)]+E_{z\sim p_z(z)}[log(1-D(G(z)))]$$其本质上是两个优化问题，把拆解就如同下面两个公式： 优化D：$$\max\limits_{D}GAN(D,G)=E_{x\sim P_{data}(x)}[logD(x)]+E_{z\sim p_z(z)}[log(1-D(G(z)))]$$优化G：$$\min\limits_{G}GAN(D,G)=E_{z\sim p_z(z)}[log(1-D(G(z)))]$$ 当优化D时，生成器确定,我们要让判别器尽可能输出高的值，所以要最大化公式(2)的值；当优化G的时候，判别器确定，我们要使判别器判断错误，尽可能使D(G(z))的值更大，所以要最小化公式(3)的值。 GAN的训练过程 上图是GAN的训练过程，解析见图中右边文字。 GAN的算法流程和动态求解过程如下图所示： 一开始我们确定G，最大化D，让点沿着D变大的方向移动(红色箭头)，然后我们确定D，最小化G，让点沿着G变小的方向移动(蓝色箭头)。循环上述若干步后，达到期望的鞍点(理想最优解)。 GAN的网络结构判别器(卷积) 卷积层大家应该都很熟悉了,为了方便说明，定义如下： 二维的离散卷积（N=2）方形的特征输入（i1=i2=i）方形的卷积核尺寸（k1=k2=k ）每个维度相同的步长（s1=s2=s）每个维度相同的padding (p1=p2=p)下图(左)表示参数为 (i=5,k=3,s=2,p=1)的卷积计算过程，从计算结果可以看出输出特征的尺寸为 (o1=o2=o=3)；下图(右)表示参为 (i=6,k=3,s=2,p=1)的卷积计算过程，从计算结果可以看出输出特征的尺寸为 (o1=o2=o=3)。 从上述2个例子我们可以总结出卷积层输入特征和输出特征尺寸和卷积核参数的关系为：$$o=\lfloor\frac{i+2p-k}{s}\rfloor+1$$ 生成器(反卷积) 在介绍反卷积之前，我们先来看一下卷积运算和矩阵运算之间的关系。例有如下运算(i=4,k=3,s=1,p=0)，输出为o=2。 通过上述的分析，我们已经知道卷积层的前向操作可以表示为和矩阵C相乘，那么我们很容易得到卷积层的反向传播就是和C的转置相乘。 反卷积和卷积的关系如下： 右上图表示的是参数为( i′=2,k′=3,s′=1,p′=2)的反卷积操作，其对应的卷积操作参数为 (i=4,k=3,s=1,p=0)。我们可以发现对应的卷积和非卷积操作其 (k=k′,s=s′)，但是反卷积却多了p′=2。通过对比我们可以发现卷积层中左上角的输入只对左上角的输出有贡献，所以反卷积层会出现 p′=k−p−1=2。通过示意图，我们可以发现，反卷积层的输入输出在 s=s′=1的情况下关系为：$$o′=i′-k′+2p′+1=i′+(k-1)-2p$$GAN的优点 ●GAN是一种生成式模型，相比较其他生成模型（玻尔兹曼机和GSNs）只用到了反向传播 ●相比其他所有模型, GAN可以产生更加清晰，真实的样本 ●GAN采用的是一种无监督的学习方式训练，可以被广泛用在无监督学习和半监督学习领域GAN的缺点●训练GAN需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到.我们还没有找到很好的达到纳什均衡的方法,所以训练GAN相比VAE或者PixelRNN是不稳定的●GAN不适合处理离散形式的数据，比如文本●GAN存在训练不稳定、梯度消失、模式崩溃的问题实例DCGAN网络网络结构 (判别器) 网络结构 (生成器) 二次元动漫人脸（共50个epoch）数据集：51223张动漫人脸 图左为原始数据集，图右为训练过程训练过程生成效果图如下： 真实人脸（共100个epoch）数据集：CelebA 是香港中文大学的开放数据集，包含10,177个名人身份的202,599张人脸图片。（选取了25600张）,数据集如下：训练过程生成效果图如下：]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
        <tag>GAN model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的发展]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-1-14-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%91%E5%B1%95%2F</url>
    <content type="text"><![CDATA[深度学习的发展历程 人工智能 机器学习 深度学习 人工智能 远在古希腊时期，发明家就梦想着创造能自主思考的机器。当人类第一次构思可编程计算机时，就已经在思考计算机能否变得智能（尽管这距造出第一台计算机还有一百多年）(Lovelace, 1842)。如今，人工智能（artificialintelligence, AI）已经成为一个具有众多实际应用和活跃研究课题的领域，并且正在蓬勃发展。我们期望通过智能软件自动地处理常规劳动、理解语音或图像、帮助医学诊断和支持基础科学研究。一个人的日常生活需要关于世界的巨量知识。很多这方面的知识是主观的、直观的，因此很难通过形式化的方式表达清楚。计算机需要获取同样的知识才能表现出智能。人工智能的一个关键挑战就是如何将这些非形式化的知识传达给计算机。 机器学习 机器学习(Machine Learning)是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构并不断改善自身性能的学科。简单来说，机器学习就是通过算法，使得机器能从大量的历史数据中学习规律，从而对新的样本做智能识别或预测未来。机器学习在图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等很多方面的发展还存在着没有良好解决的问题。 上图是机器学习解决问题的一般流程，即将原始数据划分为训练数据和测试数据，并提取数据的特征用以训练模型，最终测试数据用来测试模型的好坏（泛化能力）。 深度学习 深度学习的概念源于人工神经网络的研究，含多隐层的多层感知机就是一种深度学习结构。深度学习通过组合低层特征形式更加抽象的高层表示属性类别或特征了，来发现数据的分布式特征表示。其动机在于建立、模拟人脑进行分析学习的神经网络，它模拟人脑的机制来解释数据，例如图像、声音和文本，深度学习是无监督学习的一种。其实，神经网络早在八九十年代就被提出过，真正使得深度学习兴起有2个方面的因素： 大数据，用于训练数据的增加； 计算机的算力大大增加，更快的CPU、通用GPU 的出现 上图是深度学习的简单结构图，主要包含三个部分：输入层（Visible layer）、隐藏层（hidden layer）和输出层（Output layer）。图中解决的是图片分类问题。输入层输入图片，即像素矩阵；对于隐藏层，第一层可以轻易地通过比较相邻像素的亮度来识别边缘。有了第一隐藏层描述的边缘，第二隐藏层可以容易地搜索可识别为角和扩展轮廓的边集合。给定第二隐藏层中关于角和轮廓的图像描述，第三隐藏层可以找到轮廓和角的特定集合来检测特定对象的整个部分；最后根据图像描述中包含的对象部分，输出层输出图片中所包含的对象类别。 深度学习常见的编程框架 观察发现，Google、Microsoft、Facebook等巨头都参与了这场深度学习框架大战，此外，还有毕业于伯克利大学的贾扬清主导开发的Caffe，蒙特利尔大学Lisa Lab团队开发的Theano，以及其他个人或商业组织贡献的框架。 另外，可以看到各大主流框架基本都支持Python，目前Python在科学计算和数据挖掘领域可以说是独领风骚。虽然有来自R、Julia等语言的竞争压力，但是Python的各种库实在是太完善了，Web开发、数据可视化、数据预处理、数据库连接、爬虫等无所不能，有一个完美的生态环境。仅在数据挖据工具链上，Python就有NumPy、SciPy、Pandas、Scikit-learn、XGBoost等组件，做数据采集和预处理都非常方便，并且之后的模型训练阶段可以和TensorFlow等基于Python的深度学习框架完美衔接。 深度学习的应用无人驾驶 深度学习在无人驾驶领域主要用于图像处理， 也就是摄像头上面。 当然也可以用于雷达的数据处理， 但是基于图像极大丰富的信息以及难以手工建模的特性， 深度学习能最大限度的发挥其优势。 在做无人车的公司中，他们都会用到三个传感器激光雷达（lidar），测距雷达（radar）和摄像头（camera），但还是会各有侧重。比如 Waymo（前谷歌无人车）以激光雷达为主，而特斯拉和中国的图森互联以摄像头为主。我们可以从特斯拉近期放出的一段无人驾驶的视频中看到特斯拉有三个摄像头传感器，左中右各一个。 从上图我们可以看出，特斯拉成功识别了道路线（红色的线）前方整个路面（右中图），这个过程也可以用深度学习完成。 AlphaGo阿尔法狗 阿尔法狗（AlphaGo）是第一个击败人类职业围棋选手、第一个战胜围棋世界冠军的人工智能程序。它主要的原理就是深度学习。早在1997年，IBM的国际象棋系统深蓝，击败了世界冠军卡斯帕罗夫时，采用的算法是通过暴力搜索的方式尝试更多的下棋方法从而战胜人类，其所依赖的更多是计算机的计算资源优势。但在围棋上，深蓝的方式完全不适用。为了战胜人类围棋选手，AlphaGo需要更加智能且强大的算法。深度学习为其提供了可能。 AlphaGo主要包括三个组成部分： 蒙特卡洛搜索树（MonteCarlo tree search，MCTS） 估值网络（Value network） 策略网络（Policy notebook） AlphaGo的一个大脑——策略网络，通过深度学习在当前给定棋盘条件下，预测下一步在哪里落子。通过大量对弈棋谱获取训练数据，该网络预测人类棋手下一步落子点的准确率可达57%以上（当年数据）并可以通过自己跟自己对弈的方式提高落子水平。AlphaGo的另一个大脑——估值网络，判断在当前棋盘条件下黑子赢棋的概率。其使用的数据就是策略网络自己和自己对弈时产生的。AlphaGo使用蒙特卡罗树算法，根据策略网络和估值网络对局势的评判结果来寻找最佳落子点。 人脸识别 人脸识别的方法有很多，如face++，DeepFace，FaceNet……常规的人脸识别流程为：人脸检测—&gt;对齐—&gt;表达—&gt;分类。 人脸对齐的方法包括以下几步：1.通过若干个特征点检测人脸；2.剪切；3.建立Delaunay triangulation;4.参考标准3d模型；5.讲3d模型比对到图片上；6.进行仿射变形；7.最终生成正面图像。 学习深度学习所需的基础知识]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的常见模型（CNN）]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-1-14-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B-CNN%2F</url>
    <content type="text"><![CDATA[CNN的来源 CNN由纽约大学的Yann LeCun于1998年提出。CNN本质上是一个多层感知机，其成功的原因关键在于它所采用的局部连接和共享权值的方式。 一方面减少了的权值的数量使得网络易于优化，另一方面降低了过拟合的风险。CNN是神经网络中的一种，它的权值共享网络结构使之更类似于生物神经网络，降低了网络模型的复杂度，减少了权值的数量。 权重共享：在卷积神经网络中，卷积层的每一个卷积滤波器重复的作用于整个感受野中，对输入图像进行卷积，卷积结果构成了输入图像的特征图，提取出图像的局部特征。每一个卷积滤波器共享相同的参数，包括相同的权重矩阵和偏置项。共享权重的好处是在对图像进行特征提取时不用考虑局部特征的位置。而且权重共享提供了一种有效的方式，使要学习的卷积神经网络模型参数数量大大降低。 CNN的网络架构 卷积神经网络结构包括：卷积层，降采样层，全链接层。每一层有多个特征图，每个特征图通过一种卷积滤波器提取输入的一种特征，每个特征图有多个神经元。 卷积层（Conv） 再举一个卷积过程的例子如下：我们有下面这个绿色的55输入矩阵，卷积核是一个下面这个黄色的33矩阵。卷积的步幅是一个像素。则卷积的过程如下面的动图。卷积的结果是一个3*3的矩阵。 上面举的例子都是二维的输入，卷积的过程比较简单，那么如果输入是多维的呢？比如在前面一组卷积层+池化层的输出是3个矩阵，这3个矩阵作为输入呢，那么我们怎么去卷积呢？又比如输入的是对应RGB的彩色图像，即是三个分布对应R，G和B的矩阵呢？ 池化层（Pooling） 全连接层（Full Connecting） 总结 一般CNN的结构依次为1、 input2、 ((conv–&gt;relu)N–&gt;pool?)M3、 (fc–&gt;relu)*K4、 fc 卷积神经网络的训练算法 与一般的机器学习算法相比，先定义Loss function,衡量和实际结果之间的差距； 找到最小化损失函数的W（权重）和b（偏置），CNN里面最常见的算法为SGD（随机梯度下降）。 卷积神经网络的优缺点优点 共享卷积核，便于处理高维数据； 不像机器学习人为提取特征，网络训练权重自动提取特征，且分类效果好。 缺点 需要大量训练样本和好的硬件支持（GPU、TPU…）; 物理含义模糊（神经网络是一种难以解释的“黑箱模型”，我们并不知道卷积层到底提取的是什么特征）。 卷积神经网络的典型结构 实战演练猫狗大战，即一个简单的二分类问题，训练出一个自动判别猫狗的模型 训练集（共25000张图片，猫狗各12500张）测试集（共3000张图片，猫狗各1500张） 我们通过Tensorflow这个深度学习框架来构建我们的分类网络。通过其自带的可视化工具Tensorboard我们可以看到网络的详细结构，如下左图所示。模型训练完成后，我们用测试集来测试模型的泛化能力，输入一张测试图片，导入模型，输出分类结果，示例见下右图。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
        <tag>CNN model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[欢迎访问我的博客]]></title>
    <url>%2Funcategorized%2F2019-01-13-FirstBlog%2F</url>
    <content type="text"><![CDATA[Hey 大家好，我是一名计算机领域的在读研究生，现研究方向为Deep Learning、Computer vision,欢迎大家来学习交流。 [查看个人简历][访问主页]]]></content>
      <tags>
        <tag>first</tag>
      </tags>
  </entry>
</search>

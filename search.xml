<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python实现十大排序算法]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-5-6-python%E5%AE%9E%E7%8E%B0%E5%8D%81%E5%A4%A7%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[之前在这C语言实现八大排序算法（一）和C语言实现八大排序算法（二）2篇文章中，已经详细介绍了各种排序算法的思想，参考资料主要是用C语言实现的。本文主要用python语言再次实现十大排序算法。 十大排序算法的复杂度及稳定性分析如下表所示： 插入排序代码1234567891011121314151617181920212223242526272829'''1. 从第一个元素开始，该元素可以认为已经被排序2.取出下一个元素，在已经排序的元素序列中从后向前扫描3.如果该元素（已排序）大于新元素，将该元素移到下一位置4.重复步骤3，直到找到已排序的元素小于或者等于新元素的位置5.将新元素插入到该位置中6.重复步骤2'''def insert_sort(list): for i in range(1,len(list)): print("第&#123;&#125;轮：".format(i)) key=list[i] j=i-1 while j&gt;=0: if list[j]&gt;key: list[j+1]=list[j] list[j]=key j-=1 print(list) return lista=[2,4,3,8,1,5,7,6]result=insert_sort(a)#print(result) 结果1234567891011121314第1轮：[2, 4, 3, 8, 1, 5, 7, 6]第2轮：[2, 3, 4, 8, 1, 5, 7, 6]第3轮：[2, 3, 4, 8, 1, 5, 7, 6]第4轮：[1, 2, 3, 4, 8, 5, 7, 6]第5轮：[1, 2, 3, 4, 5, 8, 7, 6]第6轮：[1, 2, 3, 4, 5, 7, 8, 6]第7轮：[1, 2, 3, 4, 5, 6, 7, 8] 希尔排序代码12345678910111213141516171819202122'''1.希尔排序是把记录按下标的一定量分组，对每组使用直接插入算法排序；2.随着增量逐渐减少，每组包1含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法被终止。'''def shell_sort(list): # 设定步长 step = int(len(list)/2) while step &gt; 0: print(step) for i in range(step, len(list)): # 类似插入排序, 当前值与指定步长之前的值比较, 符合条件则交换位置 while i &gt;= step and list[i-step] &gt; list[i]: list[i], list[i-step] = list[i-step], list[i] i -= step step = int(step/2) print(list) return lista=[2,4,3,8,1,5,7,6]result=shell_sort(a)#print(result) 结果1234564[1, 4, 3, 6, 2, 5, 7, 8]2[1, 4, 2, 5, 3, 6, 7, 8]1[1, 2, 3, 4, 5, 6, 7, 8] 冒泡排序代码123456789101112131415161718192021222324'''1.比较相邻的元素。如果第一个比第二个大，就交换他们两个。2.对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。在这一点，最后的元素应该会是最大的数。3.针对所有的元素重复以上的步骤，除了最后一个。4.持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。'''def bubble_sort(list): global flag for i in range(len(list)-1): print("第&#123;&#125;轮：".format(i)) flag=False #本次冒泡是否发生交换的标志 for j in range(len(list)-i-1): if list[j]&gt;list[j+1]: list[j],list[j+1]=list[j+1],list[j] flag=True print(list) if flag==False: #未交换，数组已经有序，无须冒泡 returna=[2,4,3,8,1,5,7,6]result=bubble_sort(a) 结果12345678910第0轮：[2, 3, 4, 1, 5, 7, 6, 8]第1轮：[2, 3, 1, 4, 5, 6, 7, 8]第2轮：[2, 1, 3, 4, 5, 6, 7, 8]第3轮：[1, 2, 3, 4, 5, 6, 7, 8]第4轮：[1, 2, 3, 4, 5, 6, 7, 8] 快速排序代码1234567891011121314151617181920212223242526272829303132'''1.从数列中挑出一个元素，称为 “基准”（pivot）；2.重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）；3.对所有两个小数列重复第二步，直至各区间只有一个数。'''def quicksort(list,start,end): if start&gt;=end: return low=start high=end base=list[start] while low&lt;high: while low&lt;high and list[high]&gt;=base: #从后向前找第一个小于基准值的数 high-=1 if low&lt;high: list[low]=list[high] low+=1 while low&lt;high and list[low]&lt;base: #从前向后找第一个大于基准值的数 low+=1 if low&lt;high: list[high]=list[low] high-=1 list[low]=base #填坑 quicksort(list,start,low-1) #递归排序 quicksort(list,low+1,end)a=[2,4,3,8,1,5,7,6]quicksort(a,0,len(a)-1)print(a) 结果1[1, 2, 3, 4, 5, 6, 7, 8] 选择排序代码1234567891011121314151617181920212223''' 选择排序的基本思想：比较 + 交换。第一趟，在待排序记录r1 到 r[n]中选出最小的记录，将它与r1交换；第二趟，在待排序记录r2 到 r[n]中选出最小的记录，将它与r2交换；以此类推，第 i 趟，在待排序记录ri 到 r[n]中选出最小的记录，将它与r[i]交换,使有序序列不断增长直到全部排序完毕。'''def select_sort(list): for i in range(0,len(list)): print("第&#123;&#125;轮：".format(i)) min=i for j in range(i+1,len(list)): if list[min]&gt;list[j]: min=j list[min],list[i]=list[i],list[min] #每轮找出最小的交换 print(list) return lista=[2,4,3,8,1,5,7,6]result=select_sort(a)#print(result) 结果12345678910111213141516第0轮：[1, 4, 3, 8, 2, 5, 7, 6]第1轮：[1, 2, 3, 8, 4, 5, 7, 6]第2轮：[1, 2, 3, 8, 4, 5, 7, 6]第3轮：[1, 2, 3, 4, 8, 5, 7, 6]第4轮：[1, 2, 3, 4, 5, 8, 7, 6]第5轮：[1, 2, 3, 4, 5, 6, 7, 8]第6轮：[1, 2, 3, 4, 5, 6, 7, 8]第7轮：[1, 2, 3, 4, 5, 6, 7, 8] 堆排序代码12345678910111213141516171819202122232425262728293031323334353637383940414243'''大根堆：每个节点的值都大于或等于其子节点的值，用于升序排列；小根堆：每个节点的值都小于或等于其子节点的值，用于降序排列。以大根堆为例，每次将最大元素放置在堆顶'''def adjust_heap(lists, i, size): #向下调整大根堆 ''' :param lists: 数组元素 :param i: 当前结点 :param size: 数组长度 ''' # 非叶子结点的左右两个孩子 lchild = 2 * i + 1 rchild = 2 * i + 2 # 在当前结点、左孩子、右孩子中找到最大元素的索引 max = i if i &lt; size // 2: if lchild &lt; size and lists[lchild] &gt; lists[max]: max = lchild if rchild &lt; size and lists[rchild] &gt; lists[max]: max = rchild # 如果最大元素的索引不是当前结点，把大的结点交换到上面，继续调整堆 if max != i: # 第 2 个参数传入 max 的索引是交换前大数字对应的索引 # 交换后该索引对应的是小数字，应该把该小数字向下调整 lists[max], lists[i] = lists[i], lists[max] adjust_heap(lists, max, size)def build_heap(lists, size): #建立大根堆 for i in range(0, (size//2))[::-1]: # 从倒数第一个非叶子结点开始建立大根堆 adjust_heap(lists, i, size) # 对所有非叶子结点进行堆的调整def heap_sort(lists): size = len(lists) build_heap(lists, size) for i in range(0, size)[::-1]: lists[0], lists[i] = lists[i], lists[0] # 每次根结点都是最大的数，最大数放到后面 adjust_heap(lists, 0, i) # 交换完后还需要继续调整堆，只需调整根节点，此时数组的 size 不包括已经排序好的数 print("第&#123;&#125;轮：".format(size-i)) print(lists) # 由于每次大的都会放到后面，因此最后的 lists 是从小到大排列a=[2,4,3,8,1,5,7,6]result=heap_sort(a) 结果12345678910111213141516第1轮：[7, 6, 5, 4, 1, 2, 3, 8]第2轮：[6, 4, 5, 3, 1, 2, 7, 8]第3轮：[5, 4, 2, 3, 1, 6, 7, 8]第4轮：[4, 3, 2, 1, 5, 6, 7, 8]第5轮：[3, 1, 2, 4, 5, 6, 7, 8]第6轮：[2, 1, 3, 4, 5, 6, 7, 8]第7轮：[1, 2, 3, 4, 5, 6, 7, 8]第8轮：[1, 2, 3, 4, 5, 6, 7, 8] 归并排序代码12345678910111213141516171819202122232425262728293031323334353637'''1.申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列；2.设定两个指针，最初位置分别为两个已经排序序列的起始位置3.比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置；4.重复步骤③直到某一指针达到序列尾；5.将另一序列剩下的所有元素直接复制到合并序列尾。'''def merge(left_list,right_list): left_index=0 right_index=0 merge_list=[] while left_index&lt;len(left_list) and right_index&lt;len(right_list): if left_list[left_index]&lt;right_list[right_index]: #哪一部分小，就添加到合并的列表中 merge_list.append(left_list[left_index]) left_index+=1 else: merge_list.append(right_list[right_index]) right_index+=1 merge_list+=left_list[left_index:] #将剩余的部分添加到合并列表中 merge_list+=right_list[right_index:] return merge_listdef merge_sort(list): if len(list)&lt;=1: return list mid_index=len(list)//2 #拆分数组为两部分 left_list=merge_sort(list[:mid_index]) right_list=merge_sort(list[mid_index:]) return merge(left_list,right_list)a=[2,4,3,8,1,5,7,6]result=merge_sort(a)print(result) 结果1[1, 2, 3, 4, 5, 6, 7, 8] 基数排序代码1234567891011121314151617181920212223242526''' 将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。 然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后，数列就变成一个有序序列。'''def radix_sort(list): i = 0 # 记录当前正在排拿一位，最低位为1 max_num = max(list) # 最大值 j = len(str(max_num)) # 记录最大值的位数 while i &lt; j: bucket_list =[[] for _ in range(10)] #初始化桶数组 for x in list: bucket_list[int(x / (10**i)) % 10].append(x) # 找到位置放入桶数组 print("第&#123;&#125;轮：".format(i+1)) print(bucket_list) list.clear() for x in bucket_list: # 放回原序列 for y in x: list.append(y) print(list) i += 1 return lista = [334,5,67,345,7,5345,99,4,23,78,45,1,453,3424]result=radix_sort(a)# print(result) 结果123456789101112第1轮：[[], [1], [], [23, 453], [334, 4, 3424], [5, 345, 5345, 45], [], [67, 7], [78], [99]][1, 23, 453, 334, 4, 3424, 5, 345, 5345, 45, 67, 7, 78, 99]第2轮：[[1, 4, 5, 7], [], [23, 3424], [334], [345, 5345, 45], [453], [67], [78], [], [99]][1, 4, 5, 7, 23, 3424, 334, 345, 5345, 45, 453, 67, 78, 99]第3轮：[[1, 4, 5, 7, 23, 45, 67, 78, 99], [], [], [334, 345, 5345], [3424, 453], [], [], [], [], []][1, 4, 5, 7, 23, 45, 67, 78, 99, 334, 345, 5345, 3424, 453]第4轮：[[1, 4, 5, 7, 23, 45, 67, 78, 99, 334, 345, 453], [], [], [3424], [], [5345], [], [], [], []][1, 4, 5, 7, 23, 45, 67, 78, 99, 334, 345, 453, 3424, 5345] 计数排序代码12345678910111213141516171819202122232425262728293031'''1.找到给定序列的最小值与最大值2.创建一个长度为最大值-最小值+1的数组，初始化都为0然后遍历原序列，记录每个数据出现的次数此时数组中已经记录好每个值的数量，自然也就是有序的了'''def count_sort(list): # 找到最大最小值 min_num = min(list) max_num = max(list) # 计数列表，即桶的个数 count_list = [0]*(max_num-min_num+1) print(count_list) # 将元素值作为键值存储在桶中，记录其出现的次数 for i in list: count_list[i-min_num] += 1 #最小值作为一个偏移量存在，因为数据可能存在负数的 print(count_list) list.clear() # 填回 for ind,i in enumerate(count_list): #ind为索引，i为数据 while i != 0: list.append(ind+min_num) #数据的真实值 i -= 1 #该数据出现的次数 return lista = [34,5,7,34,9,5,23,12,1]result=count_sort(a)print(result) 结果123[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0][1, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2][1, 5, 5, 7, 9, 12, 23, 34, 34] 桶排序代码1234567891011121314151617181920212223242526272829303132333435'''桶排序与计数排序类似，但可以解决非整数的排序1.桶排序相当于把计数数组划分为按顺序的几个部分2.每一部分叫做一个桶，它来存放处于该范围内的数3.然后再对每个桶内部进行排序，可以使用其他排序方法如快速排序4.最后整个桶数组就是排列好的数据，再将其返回给原序列'''def bucket_sort(list): min_num = min(list) max_num = max(list) # 桶的大小 bucket_range = (max_num-min_num) / len(list) # 桶数组 count_list = [ [] for i in range(len(list) + 1)] print(count_list) # 向桶数组填数 for i in list: count_list[int((i-min_num)//bucket_range)].append(i) print(count_list) list.clear() # 回填，这里桶内部排序直接调用了sorted for i in count_list: for j in sorted(i): list.append(j) print(list) return lista = [37,5,7.8,37,9,5,23,12,1]result=bucket_sort(a)#print(result) 结果123456789101112[[], [], [], [], [], [], [], [], [], []][[1], [5, 7.8, 5], [9, 12], [], [], [23], [], [], [], [37, 37]][1][1, 5, 5, 7.8][1, 5, 5, 7.8, 9, 12][1, 5, 5, 7.8, 9, 12][1, 5, 5, 7.8, 9, 12][1, 5, 5, 7.8, 9, 12, 23][1, 5, 5, 7.8, 9, 12, 23][1, 5, 5, 7.8, 9, 12, 23][1, 5, 5, 7.8, 9, 12, 23][1, 5, 5, 7.8, 9, 12, 23, 37, 37]]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opencv100问 (一)]]></title>
    <url>%2FOpencv%2F2019-5-1-Opencv100%E9%97%AE%E9%97%AE1%2F</url>
    <content type="text"><![CDATA[主要介绍opencv中常见的一些图像处理操作。主要参考ImageProcessing100问。 通道交换我们知道，调用cv2.imread()方法读取的图像是按 BGR 顺序排列的！下面我们将其转换成RGB通道顺序，并存储图片 代码123456789101112131415import cv2#cv2.imread() 的系数是按 BGR 顺序排列的，我们交换通道为RGB显示img=cv2.imread('imori.jpg')b=img[:,:,0].copy()g=img[:,:,1].copy()r=img[:,:,2].copy()img[:,:,0]=r #按RGB通道排列img[:,:,1]=gimg[:,:,2]=bcv2.imwrite('1.jpg',img)cv2.imshow('result',img)cv2.waitKey(0)cv2.destroyAllWindows() 结果 输入(imori.jpg) 输出 (1.jpg) 灰度化（Grayscale）将图像灰度化。灰度是一种图像亮度的表示方法，可以通过下式计算： $Y = 0.2126 R + 0.7152 G + 0.0722 B$ 当然也可以直接调用方法cv2.cvtColor直接获得。 代码1234567891011121314151617181920import cv2import numpy as npimg=cv2.imread('imori.jpg').astype(np.float)b=img[:,:,0].copy()g=img[:,:,1].copy()r=img[:,:,2].copy()#方法一gray=0.2126*r+0.7152*g+0.0722*bgray=gray.astype(np.uint8)#方法二img1=cv2.imread('imori.jpg')gray1=cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)cv2.imwrite('22.jpg',gray1)cv2.imshow('result',gray1)cv2.waitKey(0)cv2.destroyAllWindows() 结果 输入 (imori.jpg) 输出一 (2.jpg) 输出二 (22.jpg) 二值化（Thresholding）把图像进行二值化。二值化是将图像使用黑（0）和白（255）两种值表示的方法。这里我们将灰度的阈值设置为 128 来进行二值化，即： 12y = &#123; 0 (if y &lt; 128) 255 (else) 当然也可以直接调用方法实现 代码123456789101112131415161718192021222324import cv2import numpy as npimg=cv2.imread('imori.jpg').astype(np.float)b=img[:,:,0].copy()g=img[:,:,1].copy()r=img[:,:,2].copy()#方法一erzhi=0.2126*r+0.7152*g+0.0722*berzhi=erzhi.astype(np.uint8)th=128erzhi[erzhi&lt;th]=0erzhi[erzhi&gt;=th]=255#方法二img1=cv2.imread('imori.jpg')img1=cv2.cvtColor(img1,cv2.COLOR_BGR2GRAY)ret,mask=cv2.threshold(img1,128,255,cv2.THRESH_BINARY)cv2.imwrite('3.jpg',erzhi)cv2.imshow('result',mask)cv2.waitKey(0)cv2.destroyAllWindows() 结果 输入 (imori.jpg) 输出一 (3.jpg) 输出二 (33.jpg) 大津二值化算法（Otsu’s Method）大津算法，也被称作最大类间方差法，是一种可以自动确定二值化中阈值的算法，从类内方差 和类间方差的比值计算得来： 小于阈值 t 的类记作 0，大于阈值 t 的类记作 1； $w0$ 和 $w1$ 是被阈值 t 分开的两个类中的像素数占总像素数的比率（满足 $w0+w1=1$）； $S0^2$, $S1^2$ 是这两个类中像素值的方差； $M0$, $M1$ 是这两个类的像素值的平均值； 也就是说： 类内方差： $Sw^2 = w0 \times S0^2 + w1 \times S1^2$ 类间方差： $Sb^2 = w0 \times (M0 - Mt)^2 + w1 \times (M1 - Mt)^2 = w0 \times w1 \times (M0 - M1) ^2$ 图像所有像素的方差： $St^2 = Sw^2 + Sb^2 = (const)$根据以上的式子，我们用以下的式子计算分离度：分离度 $X = Sb^2 / Sw^2 = Sb^2 / (St^2 - Sb^2)$ 也就是说：$argmax_{t} X = argmax_{t} Sb^2​$ 换言之，如果使 $Sb^2 = w0 \times w1 \times (M0 - M1) ^2$ 最大，就可以得到最好的二值化阈值 t。 代码12345678910111213141516171819202122232425262728293031323334353637383940import numpy as npimport cv2# img=cv2.imread('imori.jpg').astype(np.float)img=cv2.imread('imori.jpg')H,W,C=img.shape#灰度# huidu=0.2126*img[...,2]+0.7152*img[...,1]+0.0722*img[...,0]# huidu=huidu.astype(np.uint8)huidu=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)#选择最合适的二值化阈值max_sigma=0max_t=0for t in range(1,255): v0=huidu[np.where(huidu&lt;t)] #小于阈值部分比例及均值 m0=np.mean(v0) if len(v0)&gt;0 else 0 w0=len(v0)/(H*W) v1 = huidu[np.where(huidu &gt;= t)] #大于阈值部分比例及均值 m1 = np.mean(v1) if len(v1) &gt; 0 else 0 w1 = len(v1) / (H * W) sigma=w0*w1*((m0-m1)**2) #找到最好的阈值 if sigma&gt;max_sigma: max_sigma=sigma max_t=tprint("阈值为：",max_t)#根据阈值进行二值化th=max_thuidu[huidu&lt;th]=0huidu[huidu&gt;=th]=255cv2.imwrite('4.jpg',huidu)cv2.imshow('result',huidu)cv2.waitKey(0)cv2.destroyAllWindows() 结果 输入 (imori.jpg) 输出 (th = 127) (4.jpg) HSV 变换将使用 HSV 表示色彩的图像的色相反转。 HSV 即使用色相（Hue）、饱和度（Saturation）、明度（Value）来表示色彩的一种方式。 色相：将颜色使用0到360度表示，就是平常所说的颜色名称，如红色、蓝色。色相与数值按下表对应： 红 黄 绿 青色 蓝色 品红 红 0 60 120 180 240 300 360 饱和度：是指色彩的纯度，饱和度越低则颜色越黯淡( 0&lt;= S &lt; 1)； 明度：即颜色的明暗程度。数值越高越接近白色，数值越低越接近黑色 ( 0 &lt;= V &lt; 1)； 从 RGB 色彩表示转换到 HSV 色彩表示通过以下方式计算： R,G,B的值在[0, 1]之间： 1234567891011Max = max(R,G,B)Min = min(R,G,B)H = &#123; 0 (if Min=Max) 60 x (G-R) / (Max-Min) + 60 (if Min=B) 60 x (B-G) / (Max-Min) + 180 (if Min=R) 60 x (R-B) / (Max-Min) + 300 (if Min=G) V = MaxS = Max - Min 从 HSV 色彩表示转换到 RGB 色彩表示通过以下方式计算： 12345678910111213C = SH' = H / 60X = C (1 - |H' mod 2 - 1|)(R,G,B) = (V - C) (1,1,1) + &#123; (0, 0, 0) (if H is undefined) (C, X, 0) (if 0 &lt;= H' &lt; 1) (X, C, 0) (if 1 &lt;= H' &lt; 2) (0, C, X) (if 2 &lt;= H' &lt; 3) (0, X, C) (if 3 &lt;= H' &lt; 4) (X, 0, C) (if 4 &lt;= H' &lt; 5) (C, 0, X) (if 5 &lt;= H' &lt; 6) 当然也可以调用方法cv2.cvtColor实现，不过不能自由转变H、S、V的值。 代码下面是将色相反转（色相值加180），然后再用 RGB 色彩空间表示图片的方法。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import cv2#方法一，但是不能自由转变H、S、V的值# img=cv2.imread('imori.jpg')# img_hsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV)# cv2.imshow('result',img_hsv)# cv2.waitKey(0)import cv2import numpy as np#方法二，自己设定改变img = cv2.imread("imori.jpg").astype(np.float32) / 255.# RGB &gt; HSVout = np.zeros_like(img)max_v = np.max(img, axis=2).copy()min_v = np.min(img, axis=2).copy()min_arg = np.argmin(img, axis=2)H = np.zeros_like(max_v)H[np.where(max_v == min_v)] = 0## if min == Bind = np.where(min_arg == 0)H[ind] = 60 * (img[..., 1][ind] - img[..., 2][ind]) / (max_v[ind] - min_v[ind]) + 60## if min == Rind = np.where(min_arg == 2)H[ind] = 60 * (img[..., 0][ind] - img[..., 1][ind]) / (max_v[ind] - min_v[ind]) + 180## if min == Gind = np.where(min_arg == 1)H[ind] = 60 * (img[..., 2][ind] - img[..., 0][ind]) / (max_v[ind] - min_v[ind]) + 300V = max_v.copy()S = max_v.copy() - min_v.copy()# 色相反转（色相值加180）H = (H + 180) % 360# HSV &gt; RGBC = SH_ = H / 60X = C * (1 - np.abs(H_ % 2 - 1))Z = np.zeros_like(H)vals = [[Z, X, C], [Z, C, X], [X, C, Z], [C, X, Z], [C, Z, X], [X, Z, C]]for i in range(6): ind = np.where((i &lt;= H_) &amp; (H_ &lt; (i + 1))) out[..., 0][ind] = (V - C)[ind] + vals[i][0][ind] out[..., 1][ind] = (V - C)[ind] + vals[i][1][ind] out[..., 2][ind] = (V - C)[ind] + vals[i][2][ind]out[np.where(max_v == min_v)] = 0out = (out * 255).astype(np.uint8)# Save resultcv2.imwrite("5.jpg", out)cv2.imshow("result", out)cv2.waitKey(0)cv2.destroyAllWindows() 结果 输入 (imori.jpg) 输出一 (55.jpg) 输出二 (5.jpg) 减色处理将图像的值由${256}^3$压缩至$4^3$，即将 RGB 的值只取 {32, 96, 160, 224}。这被称作色彩量化。色彩的值按照下面的方式定义： 1234val = &#123; 32 ( 0 &lt;= val &lt; 64) 96 ( 64 &lt;= val &lt; 128) 160 (128 &lt;= val &lt; 192) 224 (192 &lt;= val &lt; 256) 代码实现123456789101112import cv2import numpy as npimg=cv2.imread('imori.jpg')img1=img.copy()img1 = (img1 // 64) * 64 + 32#print(img1)cv2.imwrite("6.jpg", img)cv2.imshow("result", img)cv2.waitKey(0)cv2.destroyAllWindows() 结果 输入 (imori.jpg) 输出 (6.jpg) 平均池化（Average Pooling）将图片按照固定大小网格分割，网格内的像素值取网格内所有像素的平均值。我们将这种把图片使用均等大小网格分割，并求网格内代表值的操作称为池化（Pooling）。池化操作是卷积神经网络（Convolutional Neural Network）中重要的图像处理方式。平均池化按照下式定义： 1v = 1/|R| * Sum_&#123;i in R&#125; v_i 代码请把大小为 128x128 的imori.jpg使用 8x8 的网格做平均池化。123456789101112131415161718192021import cv2import numpy as npimg=cv2.imread('imori.jpg')pool=img.copy()H,W,C=img.shape #获取图片的高，宽，通道数#网格大小G=8num_H=int(H/G)num_W=int(W/G)for y in range(num_H): #三层循环 for x in range(num_W): for c in range(C): pool[G*y:G*(y+1),G*x:G*(x+1),c]=np.mean(pool[G*y:G*(y+1),G*x:G*(x+1),c]) #取均值替换网格内原来的像素值cv2.imwrite("7.jpg", pool)cv2.imshow("result", pool)cv2.waitKey(0)cv2.destroyAllWindows() 结果 输入 (imori.jpg) 输出 (7.jpg) 最大池化（Max Pooling）网格内的值不取平均值，而是取网格内的最大值进行池化操作。只需将平均池化中的np.mean换成np.max即可。 结果 输入 (imori.jpg) 输出 (8.jpg) 高斯滤波（Gaussian Filter）使用高斯滤波器（3x3 大小，标准差 s=1.3​）来对imori_noise.jpg进行降噪处理吧！ 高斯滤波器是一种可以使图像平滑的滤波器，用于去除噪声。可用于去除噪声的滤波器还有中值滤波器，平滑滤波器、LoG（高斯-拉普拉斯）滤波器。 高斯滤波器将中心像素周围的像素按照高斯分布加权平均进行平滑化。这样的（二维）权值通常被称为卷积核或者滤波器。 但是，由于图像的长宽可能不是滤波器大小的整数倍，因此我们需要在图像的边缘补0。这种方法称作 Zero Padding。并且权值（卷积核）要进行归一化操作(sum g = 1)。 12345权值 g(x,y,s) = 1/ (s*sqrt(2 * pi)) * exp( - (x^2 + y^2) / (2*s^2))标准差 s = 1.3 的 8 近邻 高斯滤波器如下： 1 2 1K = 1/16 [ 2 4 2 ] 1 2 1 卷积神经网络中的卷积核参数是通过训练得到的，而这里的参数，以及下面介绍的各种滤波器（卷积核）参数都是按照一定方法预先计算得到的。 代码123456789101112131415161718192021222324252627282930313233343536373839import cv2import numpy as np# Read imageimg = cv2.imread("imori_noise.jpg")H, W, C = img.shape# Gaussian FilterK_size = 3 #卷积核大小sigma = 1.3 #一个标准差，为了下面计算卷积核的权重## Zero padding #0填充，为了使输出图片和原图片保持相同尺寸pad = K_size // 2out = np.zeros((H + pad * 2, W + pad * 2, C), dtype=np.float)out[pad:pad + H, pad:pad + W] = img.copy().astype(np.float) #将原始图片的像素值复制到填充后的全0图片上## KernelK = np.zeros((K_size, K_size), dtype=np.float) #与卷积神经网络训练核参数不同，此处的权重都是基于高斯分布加权平均求得的for x in range(-pad, -pad + K_size): for y in range(-pad, -pad + K_size): K[y + pad, x + pad] = np.exp(-(x ** 2 + y ** 2) / (2 * (sigma ** 2)))K /= (sigma * np.sqrt(2 * np.pi))K /= K.sum()tmp = out.copy()for y in range(H): #滑动卷积核，使卷积核内权重与图片像素值相乘在求和 for x in range(W): for c in range(C): out[pad + y, pad + x, c] = np.sum(K * tmp[y:y + K_size, x:x + K_size, c])out = out[pad:pad + H, pad:pad + W].astype(np.uint8)# Save resultcv2.imwrite("9.jpg", out)cv2.imshow("result", out)cv2.waitKey(0)cv2.destroyAllWindows() 结果 输入 (imori_noise.jpg) 输出 (9.jpg) 中值滤波（Median filter）使用中值滤波器（3x 3大小）来对imori_noise.jpg进行降噪处理。 中值滤波器是一种可以使图像平滑的滤波器。这种滤波器用滤波器范围内（在这里是3x3）像素点的中值进行滤波，在这里也采用 Zero Padding。 与高斯滤波很类似，只不过不再需要计算卷积核内权重，而是直接通过卷积核框住的像素值的中位数来代替滤波后的像素值 代码123456789101112131415161718192021222324252627282930import cv2import numpy as np# Read imageimg = cv2.imread("imori_noise.jpg")H, W, C = img.shape#与高斯滤波很类似，只不过不再需要计算卷积核内权重，而是直接通过卷积核框住的像素值的中位数来代替滤波后的像素值# Median FilterK_size = 3## Zero paddingpad = K_size // 2out = np.zeros((H + pad*2, W + pad*2, C), dtype=np.float)out[pad:pad+H, pad:pad+W] = img.copy().astype(np.float)tmp = out.copy()for y in range(H): for x in range(W): for c in range(C): out[pad+y, pad+x, c] = np.median(tmp[y:y+K_size, x:x+K_size, c])out = out[pad:pad+H, pad:pad+W].astype(np.uint8)# Save resultcv2.imwrite("10.jpg", out)cv2.imshow("result", out)cv2.waitKey(0)cv2.destroyAllWindows() 结果 输入 (imori_noise.jpg) 输出 (10.jpg)]]></content>
      <categories>
        <category>Opencv</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>Image processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FaceNet在FPGA等硬件平台上的实现]]></title>
    <url>%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%2F2019-4-25-FaceNet%E5%9C%A8FPGA%E7%AD%89%E7%A1%AC%E4%BB%B6%E5%B9%B3%E5%8F%B0%E4%B8%8A%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[本文主要介绍将训练好的网络模型，移植到FPGA等硬件平台上所必须的准备工作。不涉及具体的用C语言重新编写卷积操作、RAM存储等设计，本人做的只是辅助工作=-=。 项目简介论文地址：FaceNet: A Unified Embedding for Face Recognition and Clustering 将在服务器上训练好的FaceNet模型移植到FPGA等硬件平台上，实现人脸的检测推断过程。要想实现上述操作，必须先进行下面2个操作： 模型参数的提取：解析 FaceNet 的网络结构， Restore 训练好的模型，提取各网络层参数。 参数的量化压缩：模型参数量巨大(浮点)，为了节省空间及方便计算，将参数量化为 8 位的定点数。 参数的提取在提取参数前，我们先通过可视化工具Tensorboard解析了一下FaceNet的网络结构，它主要包含5个大模块： block35 Branch_0：32个$1 \times 1$卷积 Branch_1：32个$1 \times 1$卷积、32个$3 \times 3$卷积 Branch_2：32个$1 \times 1$卷积、32个$3 \times 3$卷积、32个$3 \times 3$卷积 Mixed：将Branch_0、Branch_1和Branch_2连接起来 Conv：32个$1 \times 1$卷积 block17 Branch_0：128个$1 \times 1$卷积 Branch_1：128个$1 \times 1$卷积、128个$1 \times 7$卷积、128个$7 \times 1$卷积 Mixed：将Branch_0和Branch_1连接起来 Conv：128个$1 \times 1$卷积 block8 Branch_0：192个$1 \times 1$卷积 Branch_1：192个$1 \times 1$卷积、192个$1 \times 3$卷积、192个$3 \times 1$卷积 Mixed：将Branch_0和Branch_1连接起来 Conv：192个$1 \times 1$卷积 reduction_a Branch_0：192个$3 \times 3$（stride=2）卷积 Branch_1：192个$1 \times 1$卷积、256个$3 \times 3$卷积、384个$3 \times 3$（stride=2）卷积 Branch_2：$3 \times 3$，步长为2的最大池化 Mixed：将Branch_0、Branch_1和Branch_2连接起来 reduction_b Branch_0：256个$1 \times 1$卷积、384个$3 \times 3$（stride=2）卷积 Branch_1：256个$1 \times 1$卷积、256个$3 \times 3$（stride=2）卷积 Branch_2：256个$1 \times 1$卷积、256个$3 \times 3$卷积、256个$3 \times 3$（stride=2）卷积 Branch_3：$3 \times 3$，步长为2的最大池化 Mixed：将Branch_0、Branch_1、Branch_2和Branch_3连接起来 总的网络结构如下所示： Conv2d_1a：32个$3 \times 3$，stride=2的卷积 Conv2d_2a：32个$3 \times 3$的卷积 Conv2d_2b：64个$3 \times 3$的卷积 MaxPool_3a：$3 \times 3$，stride=2的最大池化 Conv2d_3b：80个$1 \times 1$的卷积 Conv2d_4a：192个$3 \times 3$的卷积 Conv2d_4b：256个$3 \times 3$，stride=2的卷积 repeat：5个block35模块 Mixed_6a：1个reduction_a模块 repeat1：10个block17模块 Mixed_7a：1个reduction_b模块 repeat2：5个block8模块 block8：1个block8模块 Logits：平均池化、flatten、Dropout 代码实现代码中会用到float_to_bin()这一个量化函数，下面会有所介绍 123456789101112131415161718192021222324252627282930313233343536import osfrom tensorflow.python import pywrap_tensorflowimport numpy as npimport mathimport float_binimport xiaoshu_binMax = 35.004695892333984 #参数的最大值Min = -11.588409423828125 #参数最小值Mean = -0.0007627065894155365 #参数均值#导入模型checkpoint_path = os.path.join('facenet_lmq/20170512-110547', "model-20170512-110547.ckpt-250000")#读取模型参数reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path)#获取参数中所有的key、value值var_to_shape_map = reader.get_variable_to_shape_map()#循环存储每一个key（tensor名字）对应的valuefor key in var_to_shape_map: par_name=str(key) par_final_name =par_name.replace('/','_') #特殊字符替换 file_path = 'D:/PycharmProjects/faceface/bb/'+par_final_name+'.txt' #创建存储路径 par_shape=reader.get_tensor(key).shape # Tensor维度 par_value=reader.get_tensor(key).flatten() #value拉平，方便下面的量化操作 # print(type(par_value)) # print(par_value.shape) list =[] for index in range(len(par_value)): #对每一个tensor的value量化 par_value[index] =(par_value[index]-Mean)/(Max-Min) #归一化 if ('moving_variance' in par_name): # 特殊的tensor需要进行一些处理（BN） # for index in range(len(par_value)): par_value[index] = 1/(math.sqrt(par_value[index])) list.append(float_bin.float_to_bin(par_value[index])) #调用量化函数float_to_bin（） np.savetxt(file_path,np.array(list),fmt='%s',header=str(par_shape)) #存储量化后的参数 print('done') #print(type(par_value)) 参数的量化压缩训练得到的模型参数都是浮点型的，为了节省在硬件上的存储空间并加速计算，我们将参数量化到了8位的定点数。主要包含2个函数：float_to_bin()和xiaoshu_bin()。 float_to_bin() 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npimport xiaoshu_binimport mathdef float_to_bin(innum,n): global innum_abs,res_nint_array list = [] min = 2**(-n) #小数位取n位后，8位定点数能表示的最小值 max = 2**(7-n)-min #小数位取n位后，8位定点数能表示的最大值 innum_abs = abs(innum) #不管正负，都按正数处理 if (innum_abs&lt;min): #如果表示的数小于最小，按最小处理 innum_abs = min if (innum_abs&gt;max): #如果表示的数大于最大，按最大处理 innum_abs =max nint = math.floor(innum_abs) #取整，分割小数部分和整数部分 nf = innum_abs-nint #小数部分 res_nint = bin(int(nint)).replace('0b','') #整数部分直接调用bin函数处理 nint_num = len(res_nint) #整数部分的二进制表示占的位数长度 res_nint_array =np.zeros(nint_num) #创建矩阵 #print(nint_num) res_nf = xiaoshu_bin.xiaoshu(nf,n) #小数部分调用xiaoshu_bin()函数 if (innum&gt;=0): #原数为正数，二进制第一位为0 c =0 num_add =8-n-nint_num #除去小数位和整数位占的二进制位数后，还剩几位 num_add =np.zeros(num_add) #补0 for value in res_nint: res_nint_array[c] =int(value) #整数部分二进制 c= c+1 #@final =[num_add,res_nint_array,n,res_nf] else: #原数为负数，二进制第一位为1 d =0 num_add = 8-n-nint_num num_add = np.zeros(num_add) num_add[0] =1 for value in res_nint: res_nint_array[d] =int(value) d= d+1 #final = [num_add,res_nint_array,n,res_nf] final_bin =np.hstack((num_add,res_nint_array,res_nf)) #最终表示 for bin_value in final_bin: list.append(str(int(bin_value))) #字符串输出 final_bin_value =''.join(list) return final_bin_value#print(float_to_bin(-4.5，3)) xiaoshu_bin() 12345678910111213141516171819202122232425262728import numpy as npdef xiaoshu(innum, n): global N N =n #小数部分占的位数 count =0 temp = innum reco =np.zeros(N) #创建全0矩阵 if (innum&gt;1) or (N==0): #不是小数 print('Error!') return while(N): #未超过小数部分的位数 count =count+1 if (count&gt;N): N = 0 return reco temp =temp*2 #小数部分不断的乘2 if (temp&gt;1): reco[count-1] =1 temp = temp-1 elif (temp==1): reco[count-1] =1 N =0 else: reco[count-1] =0 return (reco)#print(xiaoshu(0.0525,4)) 量化结果展示以InceptionResnetV1/Block8/Branch_0/Conv2d_1x1/BatchNorm_beta这一tensor为例： 量化前后对比：]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>parameter extraction</tag>
        <tag>parameter quantization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉常见问题（含解答）]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-4-18-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[最近忙着找实习，对计算机视觉中常见的问题做了简单梳理，会不定时更新。 CNNCNN在图像上表现好的原因直接将图像数据作为输入，不仅无需人工对图像进行预处理和额外的特征抽取等复杂操作，而且以其特有的细粒度特征提取方式，使得对图像的处理达到了几近人力的水平。 参数和计算量的计算卷积输入为$Ｗ \times Ｈ \times Ｃ$，卷积核$K \times K \times Ｎ$，输出$Ｗ{1} \times Ｈ{1} \times Ｃ_{1}$ 计算量： $Ｗ{1} \times Ｈ{1} \times Ｃ_{1} \times K \times K \times C​$ 参数量：$C_{1} \times K \times K \times C​$ 调试、修改模型的经验 数据层面获取更多的数据、数据扩增或生成、对数据进行归一化或者标准化、重新进行特征选择 算法层面 对算法进行抽样调查。选取性能最好的算法，然后通过进一步的调参和数据准备来提升。重采样方法。可以先在小数据集上完成模型选择和参数调优，然后再将最终的方法扩展到全部数据集上。 调参 诊断。在每个周期， 评估模型在训练集和验证集上的表现， 并作出图表；权重初始化。尝试所有不同的初始化方法，考察是否有一种方法在其他情况不变的情况下(效果)更优；学习率。尝试随周期递减的学习率或增加动量项；激活函数。尝试常见的激活函数，并且重缩放你的数据以满足激活函数的边界；Batch size和周期。尝试不同的批次 batch size 和周期数，batch size 大小会决定最后的梯度， 以及更新权重的频度。正则化。尝试不同的正则化方式，权重衰减（Weight decay） 去惩罚大的权重、激活约束（Activation constraint） 去惩罚大的激活值、分别在输入， 隐藏层和输出层中试验 dropout 方法或者使用L1、L2正则化。优化算法和损失函数。尝试不同的优化算法（SGD、ADAM、RMSprop、、、）。要被优化的损失函数与你要解决的问题高度相关，也得适当调整。Early Stopping/早停法。一旦训练过程中出现(验证集)性能开始下降， 你可以停止训练与学习，是避免模型在训练数据上的过拟合的正则化方式。 通过嵌套模型提升性能通过组合多个“足够好的”模型来得到优秀的预测能力， 而不是通过组合多个高度调参的（脆弱的）模型。 简述 Inception v1-v4区别、改进V1 采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合； 将CNN中常用的卷积（1x1，3x3，5x5）、池化操作（3x3）堆叠在一起（卷积、池化后的尺寸相同，将通道相加），一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性； 为了减少计算量，增加了1x1卷积。 V2 卷积分解，将单个的5x5卷积层用2个连续的3x3卷积层组成的小网络来代替，在保持感受野范围的同时又减少了参数量，也加深了网络。 提出了著名的Batch Normalization (BN)方法。BN会对每一个mini-batch数据的内部进行标准化（normalization）,使输出规范到N（0，1）的正态分布，加快了网络的训练速度,还可以增大学习率。 BN某种意义上起到了正则化的作用，所以可以减少或者取消dropout，简化网络结构。V2在训练达到V1准确率时快了14倍，最后收敛的准确率也比V1高。 V3 考虑了nx1卷积核，将一个较大的二维卷积拆成两个较小的一维卷积（7x7拆成了7x1和1x7，3x3拆成了1x3和3x1），一方面节约了大量参数，加速运算并减轻了过拟合），同时网络深度进一步增加，增加了网络的非线性。 优化了Inception Module的结构。 V4利用残差连接（Residual Connection）来改进V3结构。 Inception v1中的inception结构怎么设计的 采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合； 该结构将CNN中常用的卷积（1x1，3x3，5x5）、池化操作（3x3）堆叠在一起（卷积、池化后的尺寸相同，将通道相加），一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性。 然而上面这个Inception原始版本，所有的卷积核都在上一层的所有输出上来做，而那个5x5的卷积核所需的计算量就太大了，约需要1.2亿次的计算量，造成了特征图的厚度很大。 为了避免这种情况，在3x3前、5x5前、max pooling后分别加上了1x1的卷积核，以起到了降低特征图厚度的作用，这也就形成了Inception v1的网络结构，如下图所示： 为什么使用1x1卷积核 1x1卷积的主要目的是为了减少维度，还用于修正线性激活（ReLU）。比如上一层的输出为100x100x128，经过具有256个通道的5x5卷积层之后(stride=1，pad=2)，输出数据为100x100x256，其中，卷积层的参数为128x5x5x256= 819200。而假如上一层输出先经过具有32个通道的1x1卷积层，再经过具有256个输出的5x5卷积层，那么输出数据仍为为100x100x256，但卷积参数量已经减少为128x1x1x32 + 32x5x5x256= 204800，大约减少了4倍。 加深了网络的层次，同时也增强了网络的非线性。 简述 CNN 的演变 LeNet：2个卷积3个全连接，最早用于数字识别 AlexNet：12年ImageNet冠军，5个卷积3个全连接，多个小卷积代替单一大卷积；使用ReLU激活函数，解决梯度小数问题；引入dropout避免模型过拟合；最大池化。 ZF-Net：13年ImageNet冠军，只用了一块 GPU 的稠密连接结构；将AlexNet第一层卷积核由11变成7，步长由4变为2。 VGG-Nets：14年ImageNet分类第二名，更深的网络，卷积层使用更小的filter尺寸和间隔；多个小卷积让网络有更多的非线性，更少的参数。 GoogLeNet：14年ImageNet分类第一名。引入Inception模块，采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合；采用了average pooling来代替全连接层；避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度。 ResNet：引入残差单元，简化学习目标和难度，加快训练速度，模型加深时，不会产生退化问题；能够有效解决训练过程中梯度消失和梯度爆炸问题。 DenseNet：密集连接；加强特征传播，鼓励特征复用，极大的减少了参数量。 讲一下CNN，每个层，及作用CNN的特征检测层通过训练数据进行学习 所以在使用CNN时，避免了显示的特征抽取，而隐式地从训练数据中进行学习； 由于同一特征映射面上的神经元权值相同，所以网络可以并行学习，这也是卷积网络相对于神经元彼此相连网络的一大优势。 卷积神经网络以其局部权值共享的特殊结构在语音识别和图像处理方面有着独特的优越性。权值共享降低了网络的复杂性，特别是多维输入向量的图像可以直接输入网络这一特点避免了特征提取和分类过程中数据重建的复杂度。 卷积神经网络（CNN）主要由卷积层、激活函数、池化层、全连接层组成。 卷积层（Conv）：使用卷积核进行特征提取和特征映射 激活函数（Activation）：由于卷积也是一种线性运算，因此需要增加非线性映射 池化层（Pool）：对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征 全连接层（FC）：连接所有的特征，将输出值送给分类器 Pooling层怎么反向传播CNN网络中另外一个不可导的环节就是Pooling池化操作，因为Pooling操作使得feature map的尺寸变化，假如做2×2的池化（步长也为2），假设那么第l+1层的feature map有16个梯度，那么第l层就会有64个梯度，这使得梯度无法对位的进行传播下去。其实解决这个问题的思想也很简单，就是把1个像素的梯度传递给4个像素，但是需要保证传递的loss（或者梯度）总和不变。根据这条原则，mean pooling和max pooling的反向传播也是不同的 mean poolingmean pooling的前向传播就是把一个patch中的值求取平均来做pooling，那么反向传播的过程也就是把某个元素的梯度等分为n份分配给前一层，这样就保证池化前后的梯度（残差）之和保持不变，图示如下 ： max poolingmax pooling也要满足梯度之和不变的原则，max pooling的前向传播是把patch中最大的值传递给后一层，而其他像素的值直接被舍弃掉。那么反向传播也就是把梯度直接传给前一层某一个像素，而其他像素不接受梯度，也就是为0。所以max pooling操作和mean pooling操作不同点在于需要记录下池化操作时到底哪个像素的值是最大，也就是max id，这个变量就是记录最大值所在位置的，因为在反向传播中要用到，那么假设前向传播和反向传播的过程就如下图所示 ： 解释 ResNet 流行的原因 从上面两个图可以看出，在网络很深的时候（56层相比20层），模型效果却越来越差了（误差率越高），并不是网络越深越好。ResNet创造性的引入了残差单元，很好的解决了这个问题。 引入残差单元，简化学习目标和难度，加快训练速度，模型加深时，不会产生退化问题 引入残差单元，能够有效解决训练过程中梯度消失和梯度爆炸问题 Resnet第二个版本做了哪些改进 ResNet_v2与v1的最大区别就是v2的BN和ReLU是在卷积之前使用的，好处： 反向传播基本符合假设，信息传递无阻碍； BN层作为pre-activation，起到了正则化的作用； mobileNet、shuffleNet知道吗？ MobileNet是为移动和嵌入式设备提出的高效模型。MobileNets基于流线型架构(streamlined)，使用深度可分离卷积(即Xception变体结构)来构建轻量级深度神经网络。宽度因子α用于控制输入和输出的通道数，分辨率因子ρ控制输入的分辨率。例如，对于深度分离卷积，把标准卷积(4,4,3,5)分解为： 深度卷积部分：大小为(4,4,1,3)，作用在输入的每个通道上，输出特征映射为(3,3,3)逐点卷积部分：大小为(1,1,3,5)，作用在深度卷积的输出特征映射上，得到最终输出为(3,3,5) shuffleNet专门应用于计算力受限的移动设备，主要包含2个操作：逐点群卷积（降低计算复杂度）和通道混洗（帮助信息流通）。 讲一下pooling的作用， 为什么max pooling要更常用？哪些情况下，average pooling比max pooling更合适？ 作用：对输入的特征图进行压缩，一方面使特征图变小，简化网络计算复杂度；一方面进行特征压缩，提取主要特征。 通常来讲，max-pooling的效果更好，虽然max-pooling和average-pooling都对数据做了下采样，但是max-pooling感觉更像是做了特征选择，选出了分类辨识度更好的特征，提供了非线性。 pooling的主要作用一方面是去掉冗余信息，一方面要保留feature map的特征信息，在分类问题中，我们需要知道的是这张图像有什么object，而不大关心这个object位置在哪，在这种情况下显然max pooling比average pooling更合适。在网络比较深的地方，特征已经稀疏了，从一块区域里选出最大的，比起这片区域的平均值来，更能把稀疏的特征传递下去。 average-pooling更强调对整体特征信息进行一层下采样，在减少参数维度的贡献上更大一点，更多的体现在信息的完整传递这个维度上，在一个很大很有代表性的模型中，比如说DenseNet中的模块之间的连接大多采用average-pooling，在减少维度的同时，更有利信息传递到下一个模块进行特征提取。average-pooling在全局平均池化操作中应用也比较广，在ResNet和Inception结构中最后一层都使用了平均池化。有的时候在模型接近分类器的末端使用全局平均池化还可以代替Flatten操作，使输入数据变成一位向量。 哪些领域（数据集）不能使用深度学习？ 数据集太小，数据样本不足时，深度学习相对其它机器学习算法，没有明显优势 数据集没有局部相关特性，目前深度学习表现比较好的领域主要是图像／语音／自然语言处理等领域，这些领域的一个共性是局部相关性。图像中像素组成物体，语音信号中音位组合成单词，文本数据中单词组合成句子，这些特征元素的组合一旦被打乱，表示的含义同时也被改变。对于没有这样的局部相关性的数据集，不适于使用深度学习算法进行处理。举个例子：预测一个人的健康状况，相关的参数会有年龄、职业、收入、家庭状况等各种元素，将这些元素打乱，并不会影响相关的结果。 细粒度分类LSTM&amp;RNN解释LSTM结构（相对于RNN）的好处RNN的梯度消失原因和解决办法Object DetectionUnet的介绍FCN和Unet的区别RCNN系列的算法流程和区别Fast RCNN中 bbox 回归的损失函数什么解释 ROI Pooling 和 ROI AlignMask RCNN中 mask branch 如何接入 Faster RCNN中解释 FPN解释 ROI Align简述 YOLO 和 SSD简述 Hough 直线检测、Sobel 边缘检测算法流程Mask RCNN中的anchors如何判定为正负样本简述 NMS 算法流程attention起源是用在哪里？pixel还是frame，是soft还是hardanchor的正负样本比是多少算法和激活函数等BN的原理和作用###BN层反向传播，怎么求导 BN 的作用和缺陷，以及针对batch_size小的情况的改进（GN）BN层，先加BN还是激活，有什么区别手推BP优化算法举例和他们的区别（SGD、SGDM、RMSprop、Adam）随机梯度下降和梯度下降训练不收敛的原因有哪些简述 SVM 流程、核函数寻参及常见的核函数举例batch_size 和 learning rate 的关系（怎么平衡和调整二者）解释过拟合和欠拟合，以及解决方法激活函数有哪些，各自区别损失函数有哪些Sigmoid 和 ReLu 对比（各自优缺点）为什么不用sigmoid而用relu？做出了哪些改进？梯度消失和梯度爆炸的原因和解决方法Precision 和 Recall 的定义精确率高、召回率低是为什么SVM，线性回归和逻辑回归的原理及区别PCA原理，PCA和SVD的区别和联系正则化怎么选择，有哪些方式L1、L2范数，区别boost、Adaboostdropout和batch normalization讲一下决策树和随机森林讲一下GBDT的细节，写出GBDT的目标函数。 GBDT和Adaboost的区别与联系偏差、方差距离度量公式哪些，区别多标签识别怎么做data argumentation怎么处理的数据不均衡怎么处理、只有少量带标签怎么处理权重初始化方法都有哪些权值衰减这个参数怎么设置分类问题有哪些评价指标？每种的适用场景。无监督学习了解哪些图像处理Opencv边缘检测算子有哪些霍夫变换直方图是什么canny算子是怎么做的图像的特征提取有哪些算法，适用范围、优缺点]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
        <tag>interview question</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN网络架构演进：从LeNet到DenseNet]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-4-18-CNN%E5%8F%91%E5%B1%95%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[本文主要介绍卷积神经网络（CNN）的发展演变，包含对每个网络的结构分析，创新点总结。内容来自自己的收集整理，还有网易云课堂吴恩达的卷积神经网络教学视频。 LeNet-5LeNet是LeCun在1998年提出，用于解决手写数字识别（0-9）的视觉任务。自那时起，CNN的最基本的架构就定下来了：卷积层、池化层、全连接层。如今各大深度学习框架中所使用的LeNet都是简化改进过的LeNet-5。和原始的LeNet有些许不同，比如把激活函数改为了现在很常用的ReLu。LeNet-5跟现有的conv-&gt;pool-&gt;ReLU的套路不同，它使用的方式是conv1-&gt;pool-&gt;conv2-&gt;pool2再接全连接层，但是不变的是，卷积层后紧接池化层的模式依旧不变。 模型结构 LeNet-5一共有7个层，不包含输入层，分别是 2个卷积层 （5x5，stride=1,num1=6,num2=16） 2个池化层 （2x2，stride=2,type=average） 2个全连接层 (120,84) 1个输出层10个类别（数字0-9的概率） 卷积大小的计算公式如下 模型分析以下图改进后的LeNet-5，对其进行深入分析（原始LeNet5中图像大小是32*32） 首先输入图像是单通道的28*28大小的图像，用矩阵表示就是[1,28,28] 第一个卷积层Conv1所用的卷积核尺寸为5*5，滑动步长为1，卷积核数目为20，那么经过该层后图像尺寸变为24，28-5+1=24，输出矩阵为[20,24,24]。 第一个池化层Pool1核尺寸为2*2，步长2，这是没有重叠的max pooling，池化操作后，图像尺寸减半，变为12×12，输出矩阵为[20,12,12]。 第二个卷积层Conv2的卷积核尺寸为5*5，步长1，卷积核数目为50，卷积后图像尺寸变为8,这是因为12-5+1=8，输出矩阵为[50,8,8]. 第二个池化层Pool2核尺寸为2*2，步长2，这是没有重叠的max pooling，池化操作后，图像尺寸减半，变为4×4，输出矩阵为[50,4,4]。 pool2后面接全连接层FC1，神经元数目为500，再接relu激活函数。 再接FC2，神经元个数为10，得到10维的特征向量，用于10个数字的分类训练，送入softmax分类，得到分类结果的概率output。 模型特性 使用三个层作为一个系列：卷积，池化，非线性 使用卷积提取空间特征 使用映射到空间均值下采样（subsample） 双曲线（tanh）或S型（sigmoid）形式非线性 多层神经网络（MLP）作为最后的分类器 层与层之间的稀疏连接矩阵避免大的计算成本 AlexNetAlexNet在2012年ImageNet竞赛中以超过第二名10.9个百分点的绝对优势一举夺冠，从此深度学习和卷积神经网络名声鹊起，深度学习的研究如雨后春笋般出现，AlexNet的出现可谓是卷积神经网络的王者归来。 模型结构 一共有8个层，前5层是卷积层，后三层是全连接层，最终softmax输出是1000类。(此处层数不算池化层)1) Conv1（11x11，stride=4,num=96）2) Conv2（5x5，stride=1,num=256，pad=2）3) Conv3-5（3x3，stride=1,num3=num4=384，num5=256,pad=1）4) FC6-8 (num6=num7=4096,num8=1000) 模型分析Conv1 第一层输入数据为原始图像的227x227x3的图像（最开始是224x224x3，为后续处理方便必须进行调整）,这个图像被11x11x3（3代表深度，例如RGB的3通道）的卷积核进行卷积运算，卷积核对原始图像的每次卷积都会生成一个新的像素。卷积核的步长为4个像素，朝着横向和纵向这两个方向进行卷积。由此，会生成新的像素；（227-11）/4+1=55个像素，由于第一层有96个卷积核，所以就会形成555596个像素层，系统是采用双GPU处理，因此分为2组数据：55x55x48的像素层数据。重叠pool池化层：这些像素层还需要经过pool运算（池化运算）的处理，池化运算的尺度由预先设定为3x3，运算的步长为2，则池化后的图像的尺寸为：（55-3）/2+1=27。即经过池化处理过的规模为27x27x96。局部响应归一化层(LRN)：最后经过局部响应归一化处理，归一化运算的尺度为5x5；第一层卷积层结束后形成的图像层的规模为27x27x96。分别由96个卷积核对应生成，这96层数据氛围2组，每组48个像素层，每组在独立的GPU下运算。 Conv2 第二层输入数据为第一层输出的27x27x96的像素层（为方便后续处理，这对每幅像素层进行像素填充pad=2），分为2组像素数据，两组像素数据分别在两个不同的GPU中进行运算。每组像素数据被5x5x48的卷积核进行卷积运算，同理按照第一层的方式进行：（27-5+2x2）/1+1=27个像素，一共有256个卷积核，这样也就有了27x27x128两组像素层。重叠pool池化层：同样经过池化运算，池化后的图像尺寸为（27-3）/2+1=13，即池化后像素的规模为2组13x13x128的像素层。局部响应归一化层(LRN)：最后经归一化处理，分别对应2组128个卷积核所运算形成。每组在一个GPU上进行运算。即共256个卷积核，共2个GPU进行运算。 Conv3 第三层输入数据为第二层输出的两组13x13x128的像素层（为方便后续处理，这对每幅像素层进行像素填充pad=1），分为2组像素数据，两组像素数据分别在两个不同的GPU中进行运算。每组像素数据被3x3x128的卷积核（两组，一共也就有3x3x256）进行卷积运算，同理按照第一层的方式进行：（13-3+1x2）/1+1=13个像素，一共有384个卷积核，这样也就有了13x13x192两组像素层。 Conv4 第四层输入数据为第三层输出的两组13x13x192的像素层（为方便后续处理，这对每幅像素层进行像素填充pad=1），分为2组像素数据，两组像素数据分别在两个不同的GPU中进行运算。每组像素数据被3x3x192的卷积核进行卷积运算，同理按照第一层的方式进行：（13-3+1x2）/1+1=13个像素，一共有384个卷积核，这样也就有了13x13x192两组像素层。 Conv5 第五层输入数据为第四层输出的两组13x13x192的像素层（为方便后续处理，这对每幅像素层进行像素填充pad=1），分为2组像素数据，两组像素数据分别在两个不同的GPU中进行运算。每组像素数据被3x3x192的卷积核进行卷积运算，同理按照第一层的方式进行：（13-3+1x2）/1+1=13个像素，一共有256个卷积核，这样也就有了13x13x128两组像素层。重叠pool池化层：进过池化运算，池化后像素的尺寸为（13-3）/2+1=6，即池化后像素的规模变成了两组6x6x128的像素层，共6x6x256规模的像素层。 FC6 第6层输入数据的尺寸是6x6x256，采用6x6x256尺寸的滤波器对第六层的输入数据进行卷积运算；每个6x6x256尺寸的滤波器对第六层的输入数据进行卷积运算生成一个运算结果，通过一个神经元输出这个运算结果；共有4096个6x6x256尺寸的滤波器对输入数据进行卷积，通过4096个神经元的输出运算结果；然后通过ReLU激活函数以及dropout运算输出4096个本层的输出结果值。 FC7 第6层输出的4096个数据与第7层的4096个神经元进行全连接，然后经由ReLU和Dropout进行处理后生成4096个数据。 FC8 第7层输入的4096个数据与第8层的1000个神经元进行全连接，经过训练后输出被训练的数值。 各层训练参数前五层：卷积层 后三层：全连接层 新技术点 ReLU作为激活函数ReLU为非饱和函数，论文中验证其效果在较深的网络超过了Sigmoid，成功解决了Sigmoid在网络较深时的梯度弥散问题。 Dropout避免模型过拟合在训练时使用Dropout随机忽略一部分神经元，以避免模型过拟合。在Alexnet的最后几个全连接层中使用了Dropout。 重叠的最大池化之前的CNN中普遍使用平均池化，而Alexnet全部使用最大池化，避免平均池化的模糊化效果。并且，池化的步长小于核尺寸，这样使得池化层的输出之间会有重叠和覆盖，提升了特征的丰富性。 提出LRN层对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。 GPU加速 数据增强随机从256*256的原始图像中截取224x224大小的区域（以及水平翻转的镜像），相当于增强了（256-224）x（256-224）x2=2048倍的数据量。使用了数据增强后，减轻过拟合，提升泛化能力。避免因为原始数据量的大小使得参数众多的CNN陷入过拟合中。 ZF-NetZFNet是2013 ImageNet分类任务的冠军，其网络结构没什么改进，只是调了调参，性能较Alex提升了不少。差异表现在 AlexNet用了两块GPU的稀疏连接结构，而ZFNet只用了一块 GPU 的稠密连接结构。 除此之外，ZF-Net将AlexNet第一层卷积核由11变成7，步长由4变为2。 模型结构 VGG-NetsVGG-Nets是由牛津大学VGG（Visual Geometry Group）提出，是2014年ImageNet竞赛定位任务的第一名和分类任务的第二名的中的基础网络。VGG可以看成是加深版本的AlexNet. 都是conv layer + FC layer，在当时看来这是一个非常深的网络了，因为层数高达十多层，当然以现在的目光看来VGG真的称不上是一个very deep的网络。 模型结构 上面表格是描述的是VGG-Net的网络结构以及诞生过程。为了解决初始化（权重初始化）等问题，VGG采用的是一种Pre-training的方式，这种方式在经典的神经网络中经常见得到，就是先训练一部分小网络，然后再确保这部分网络稳定之后，再在这基础上逐渐加深。表1从左到右体现的就是这个过程，并且当网络处于D阶段的时候，效果是最优的，即VGG-16。E阶段得到的网络就是VGG-19。VGG-16的16指的是conv+fc的总层数是16，是不包括max pool的层数！ VGG16网络使用的统一的卷积核大小：3x3，stride=1，padding=same，统一的Max-Pool： 2x2，stride=2。 由上图看出，VGG-16的结构非常整洁，深度较AlexNet深得多，里面包含多个conv-&gt;conv-&gt;max_pool这类的结构,VGG的卷积层都是same的卷积，即卷积过后的输出图像的尺寸与输入是一致的，它的下采样完全是由max pooling来实现。VGG网络后接3个全连接层，filter的个数（卷积后的输出通道数）从64开始，然后每进行一个pooling后其成倍的增加，128、512，VGG的主要贡献是使用小尺寸的filter，及有规则的卷积-池化操作。 闪光点 卷积层使用更小的filter尺寸和间隔3×3卷积核的优点： 多个3×3的卷基层比一个大尺寸filter卷基层有更多的非线性，使得判决函数更加具有判决性 多个3×3的卷积层比一个大尺寸的filter有更少的参数，假设卷基层的输入和输出的特征图大小相同为C，那么三个3×3的卷积层参数个数3×（3×3×C×C）=27CC；一个7×7的卷积层参数为49CC；所以可以把三个3×3的filter看成是一个7×7filter的分解（中间层有非线性的分解） C中1*1卷积核的优点：在不影响输入输出维数的情况下，对输入进行线性形变，然后通过Relu进行非线性处理，增加网络的非线性表达能力。 使用了Multi-Scale的方法做数据增强，将原始图像缩放到不同尺寸S，然后再随机裁切224´224的图片，这样能增加很多数据量，对于防止模型过拟合有很不错的效果。 Network in NetworkNIN的结构和传统的神经网络中多层的结构有些类似，后者的多层是跨越了不同尺寸的感受野（通过层与层中间加pool层），从而在更高尺度上提取出特征；NIN结构是在同一个尺度上的多层（中间没有pool层），从而在相同的感受野范围能提取更强的非线性。使用 1×1 卷积为卷积层的特征提供更组合性的能力，这个想法之后被用到一些最近的架构中，例如 ResNet、Inception 及其衍生技术。 模型结构 创新点 提出了抽象能力更高的 Mlpconv 层一般用 CNN 进行特征提取时，其实就隐含地假设了特征是线性可分的， 可实际问题往往是难以线性可分的。一般来说我们所要提取的特征一般是高度非线性的。NIN提出在每个局部感受野中进行更加复杂的运算，提出了对卷积层的改进算法：MLP 卷积层。 提出了 Global Average Pooling（全局平均池化）层另一方面，传统的 CNN 最后一层都是全连接层，参数个数非常之多，容易引起过拟合。NIN提出采用了全局均值池化替代全连接层。与传统的全连接层不同，NIN对每个特征图一整张图片进行全局均值池化，这样每张特征图都可以得到一个输出。这样采用均值池化，连参数都省了，可以大大减小网络参数，避免过拟合。 上图左侧是是传统的卷积层结构（线性卷积），在一个尺度上只有一次卷积；右图是Network in Network结构（NIN结构），先进行一次普通的卷积（比如3x3），紧跟再进行一次1x1的卷积，对于某个像素点来说1x1卷积等效于该像素点在所有特征上进行一次全连接的计算，所以右侧图的1x1卷积画成了全连接层的形式，需要注意的是NIN结构中无论是第一个3x3卷积还是新增的1x1卷积，后面都紧跟着激活函数（比如relu）。将两个卷积串联，就能组合出更多的非线性特征。 GoogLeNet（从Inception v1到v4的演进）GoogLeNet在2014的ImageNet分类任务上击败了VGG-Nets夺得冠军。跟AlexNet,VGG-Nets这种单纯依靠加深网络结构进而改进网络性能的思路不一样，它另辟幽径，在加深网络的同时（22层），也在网络结构上做了创新，引入Inception结构代替了单纯的卷积+激活的传统操作。 获得高质量模型最保险的做法就是增加模型的深度（层数）或者是其宽度（层核或者神经元数），但是这里一般设计思路的情况下会出现如下的缺陷： 参数太多，若训练数据集有限，容易过拟合； 网络越大计算复杂度越大，难以应用； 网络越深，梯度越往后穿越容易消失，难以优化模型。 解决上述问题的根本方法是将全连接甚至一般的卷积都转化为稀疏连接。为了打破网络对称性和提高学习能力，传统的网络都使用了随机稀疏连接。但是，计算机软硬件对非均匀稀疏数据的计算效率很差，所以在AlexNet中又重新启用了全连接层，目的是为了更好地优化并行运算。现在的问题是有没有一种方法：既能保持网络结构的稀疏性，又能利用密集矩阵的高计算性能。 Inception模块（V1）Inception架构的主要思想是设计一个稀疏网络结构，但是能够产生稠密的数据，既能增加神经网络表现，又能保证计算资源的使用效率。 采用不同大小的卷积核意味着不同大小的感受野，最后拼接意味着不同尺度特征的融合； 之所以卷积核大小采用1x1、3x3和5x5，主要是为了方便对齐。设定卷积步长stride=1之后，只要分别设定padding =0、1、2，采用same卷积可以得到相同维度的特征，然后这些特征直接拼接在一起； 该结构将CNN中常用的卷积（1x1，3x3，5x5）、池化操作（3x3）堆叠在一起（卷积、池化后的尺寸相同，将通道相加），一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性。 然而上面这个Inception原始版本，所有的卷积核都在上一层的所有输出上来做，而那个5x5的卷积核所需的计算量就太大了，约需要1.2亿次的计算量(下图左边所示)，造成了特征图的厚度很大。如果在5x5卷积前加一个1x1卷积，计算量将大大减少，如下图所示： 1x1卷积的主要目的是为了减少维度，还用于修正线性激活（ReLU）。比如，上一层的输出为100x100x128，经过具有256个通道的5x5卷积层之后(stride=1，pad=2)，输出数据为100x100x256，其中，卷积层的参数为128x5x5x256= 819200。而假如上一层输出先经过具有32个通道的1x1卷积层，再经过具有256个输出的5x5卷积层，那么输出数据仍为为100x100x256，但卷积参数量已经减少为128x1x1x32 + 32x5x5x256= 204800，大约减少了4倍。为了避免这种情况，在3x3前、5x5前、max pooling后分别加上了1x1的卷积核，以起到了降低特征图厚度的作用，这也就形成了Inception v1的网络结构，如下图所示： InceptionV1创新点 GoogLeNet采用了Inception（V1）模块化（9个）的结构，共22层，方便增添和修改； 网络最后采用了average pooling来代替全连接层，想法来自NIN,参数量仅为AlexNet的1/12,性能优于AlexNet，。但是，实际在最后还是加了一个全连接层，主要是为了方便finetune； 虽然移除了全连接，但是网络中依然使用了Dropout ; 为了避免梯度消失，网络额外增加了2个辅助的softmax用于向前传导梯度。辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益。而在实际测试的时候，这两个额外的softmax会被去掉。 InceptionV2大尺寸的卷积核可以带来更大的感受野，也意味着更多的参数，比如5x5卷积核参数是3x3卷积核的25/9=2.78倍。为此，作者提出可以用2个连续的3x3卷积层(stride=1)组成的小网络来代替单个的5x5卷积层，这便是Inception V2结构，保持感受野范围的同时又减少了参数量，如下图： InceptionV2创新点 卷积分解，将单个的5x5卷积层用2个连续的3x3卷积层组成的小网络来代替，在保持感受野范围的同时又减少了参数量，也加深了网络。 提出了著名的Batch Normalization (BN)方法。BN会对每一个mini-batch数据的内部进行标准化（normalization）,使输出规范到N（0，1）的正态分布，加快了网络的训练速度,还可以增大学习率。 BN某种意义上起到了正则化的作用，所以可以减少或者取消dropout，简化网络结构。V2在训练达到V1准确率时快了14倍，最后收敛的准确率也比V1高。 InceptionV3大卷积核完全可以由一系列的3x3卷积核来替代，那能不能分解的更小一点呢。文章考虑了nx1卷积核，如下图所示的取代3x3卷积：于是，任意nxn的卷积都可以通过1xn卷积后接nx1卷积来替代。实际上，作者发现在网络的前期使用这种分解效果并不好，还有在中度大小的feature map上使用效果才会更好，对于mxm大小的feature map,建议m在12到20之间。用nx1卷积来代替大卷积核，这里设定n=7来应对17x17大小的feature map。该结构被正式用在GoogLeNet V2中。 InceptionV3创新点 将一个较大的二维卷积拆成两个较小的一维卷积（7x7拆成了7x1和1x7，3x3拆成了1x3和3x1），一方面节约了大量参数，加速运算并减轻了过拟合），同时网络深度进一步增加，增加了网络的非线性（每增加一层都要进行ReLU）。 网络输入从224x224变为了299x299 优化了Inception Module的结构，主要有三种不同的结构（35x35、17x17、8x8），见下图。 InceptionV4Inception V4研究了Inception模块与残差连接的结合。Inception V4主要利用残差连接（Residual Connection）来改进V3结构，得到Inception-ResNet-v1，Inception-ResNet-v2，Inception-v4网络。 ResNet的残差结构 ——————————-&gt; 与Inception相结合 ResNet理论上，深的网络一般会比浅的网络效果好，如果要进一步地提升模型的准确率，最直接的方法就是把网络设计得越深越好，这样模型的准确率也就会越来越准确。事实上呢？看下面一个例子，对常规的网络（plain network，也称平原网络）直接堆叠很多层次，经对图像识别结果进行检验，训练集、测试集的误差结果如下图： 从上面两个图可以看出，在网络很深的时候（56层相比20层），模型效果却越来越差了（误差率越高），并不是网络越深越好。通过实验可以发现：随着网络层级的不断增加，模型精度不断得到提升，而当网络层级增加到一定的数目以后，训练精度和测试精度迅速下降，这说明当网络变得很深以后，深度网络就变得更加难以训练了。 残差模块——Residual bloack前面描述了一个实验结果现象，在不断加神经网络的深度时，模型准确率会先上升然后达到饱和，再持续增加深度时则会导致准确率下降。那么我们作这样一个假设：假设现有一个比较浅的网络（Shallow Net）已达到了饱和的准确率，这时在它后面再加上几个恒等映射层（Identity mapping，也即y=x，输出等于输入），这样就增加了网络的深度，并且起码误差不会增加，也即更深的网络不应该带来训练集上误差的上升。而这里提到的使用恒等映射直接将前一层输出传到后面的思想，便是著名深度残差网络ResNet的灵感来源。 a[l+2] 加上了 a[l]的残差块，即：残差网络中，直接将a[l]向后拷贝到神经网络的更深层，在ReLU非线性激活前面加上a[l]，a[l]的信息直接达到网络深层。使用残差块能够训练更深层的网络，构建一个ResNet网络就是通过将很多这样的残差块堆积在一起，形成一个深度神经网络。 上图中是用5个残差块连接在一起构成的残差网络，用梯度下降算法训练一个神经网络，若没有残差，会发现随着网络加深，训练误差先减少后增加，理论上训练误差越来越小比较好。而对于残差网络来讲，随着层数增加，训练误差越来越减小，这种方式能够到达网络更深层，有助于解决梯度消失和梯度爆炸的问题，让我们训练更深网络同时又能保证良好的性能。 ResNet引入了残差网络结构（residual network），通过这种残差网络结构，可以把网络层弄的很深（据说目前可以达到1000多层），并且最终的分类效果也非常好，残差网络的基本结构如下图所示，很明显，该图是带有跳跃结构的： 假定某段神经网络的输入是x，期望输出是H(x)，即H(x)是期望的复杂潜在映射，如果是要学习这样的模型，则训练难度会比较大；在上图的残差网络结构图中，通过“shortcut connections（捷径连接）”的方式，直接把输入x传到输出作为初始结果，输出结果为H(x)=F(x)+x，当F(x)=0时，那么H(x)=x，也就是上面所提到的恒等映射。于是，ResNet相当于将学习目标改变了，不再是学习一个完整的输出，而是目标值H(X)和x的差值，也就是所谓的残差F(x) := H(x)-x，因此，后面的训练目标就是要将残差结果逼近于0，使到随着网络加深，准确率不下降。 残差网络“shortcut connections（捷径连接）”是实线，有一些是虚线，有什么区别呢？ 因为经过“shortcut connections（捷径连接）”后，H(x)=F(x)+x，如果F(x)和x的通道相同，则可直接相加，那么通道不同怎么相加呢。上图中的实线、虚线就是为了区分这两种情况的： 实线的Connection部分，表示通道相同，如左图的第一个粉色矩形和第三个粉色矩形，都是3364的特征图，由于通道相同，所以采用计算方式为H(x)=F(x)+x。 虚线的Connection部分，表示通道不同，如左图的第一个绿色矩形和第三个绿色矩形，分别是3364和33128的特征图，通道不同，采用的计算方式为H(x)=F(x)+Wx，其中W是卷积操作，用来调整 x维度的。 残差学习单元除了上面提到的两层残差学习单元，还有三层的残差学习单元，如下图所示： 实际中，考虑计算的成本，对残差块做了计算优化，即将两个3x3的卷积层替换为1x1 + 3x3 + 1x1, 如上图。新结构中的中间3x3的卷积层首先在一个降维1x1卷积层下减少了计算，然后在另一个1x1的卷积层下做了还原，既保持了精度又减少了计算量。 创新点 引入残差单元，简化学习目标和难度，加快训练速度，模型加深时，不会产生退化问题 引入残差单元，能够有效解决训练过程中梯度消失和梯度爆炸问题 DenseNet模型结构 创新点 密集连接，具体来说就是每个层都会接受其前面所有层作为其额外的输入，缓解梯度消失问题 加强特征传播，鼓励特征复用，极大的减少了参数量。 CNN发展总结 CNN 的演化路径可以总结为以下几个方向： 进化之路一： 网络结构加深 进化之路二： 加强卷积功能 进化之路三： 从分类到检测 进化之路四： 新增功能模块 本文主要介绍的CNN用在分类上的经典网络，关于CNN在目标检测（R-CNN系列）和图像分割（FCN）的应用，以后会有所介绍。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer（五）]]></title>
    <url>%2F%E5%9C%A8%E7%BA%BF%E7%BC%96%E7%A8%8B%2F2019-4-16-%E5%89%91%E6%8C%87offer5%2F</url>
    <content type="text"><![CDATA[剑指offer（21-25）。 栈的压入和弹出序列题目描述输入两个整数序列，第一个序列表示栈的压入顺序，请判断第二个序列是否可能为该栈的弹出顺序。假设压入栈的所有数字均不相等。例如序列1,2,3,4,5是某栈的压入顺序，序列4,5,3,2,1是该压栈序列对应的一个弹出序列，但4,3,5,1,2就不可能是该压栈序列的弹出序列。（注意：这两个序列的长度是相等的） 思路 借用一个辅助栈 代码实现12345678910111213class Solution: def IsPopOrder(self, pushV, popV): if len(pushV)!=len(popV): #如果入栈序列和弹出序列长度不一致，返回False return False stack=[] #创建一个辅助栈 for i in pushV: #循环将入栈序列的值压入辅助栈 stack.append(i) while stack and stack[-1]==popV[0]: #如果辅助栈不为空并且辅助栈顶元素等于弹出序列第一个值 stack.pop() #辅助栈栈顶元素出栈 popV.pop(0) #弹出序列第一个值弹出 if stack: #如果入栈序列循环结束，辅助栈不为空，说明弹出序列不是该栈的弹出顺序 return False return True 从上往下打印二叉树题目描述从上往下打印出二叉树的每个节点，同层节点从左至右打印。 思路 其实就是二叉树的层次遍历，借用一个队列就可以实现 代码实现12345678910111213141516171819202122class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None#其实就是对二叉树进行层次遍历，借用一个队列就可以实现class Solution: # 返回从上到下每个节点值列表，例：[1,2,3] def PrintFromTopToBottom(self, root): if not root: #树为空，返回空列表 return [] result=[] #存储好遍历结点值的列表 queue=[root] #辅助队列 while queue: tmp=queue.pop(0) #队头结点出队 result.append(tmp.val) #队头结点值存入列表 if tmp.left: #左子树非空，入队 queue.append(tmp.left) if tmp.right: #右子树非空，入队 queue.append(tmp.right) return result 二叉搜索树的后序遍历序列题目描述输入一个整数数组，判断该数组是不是某二叉搜索树的后序遍历的结果。如果是则输出Yes,否则输出No。假设输入的数组的任意两个数字都互不相同。 思路 后序遍历的序列中,最后一个数字是树的根节点 ,数组中前面的数字可以分为两部分：第一部分是左子树节点的值都比根节点的值小,第二部分是右子树节点的值都比根节点的值大，后面用递归分别判断前后两部分 是否 符合以上原则 代码实现123456789101112131415161718class Solution: def VerifySquenceOfBST(self, sequence): if not sequence: # 如果数组为空，直接返回False return False length=len(sequence) #数组的长度 root=sequence[-1] #根节点的值 for i in range(length): #寻找二叉搜索树的左子树 if sequence[i]&gt;root: break for j in range(i,length): #判断二叉搜索树的右子树的每一个结点的值是否都比很结点大 if sequence[j]&lt;root: return False left=right=True # 递归调用，分别查看二叉树的左右子树 if i&gt;0: left=self.VerifySquenceOfBST(sequence[0:i]) if i&lt;length-1 and left: right=self.VerifySquenceOfBST(sequence[i:-1]) return left and right #当左右两子树都返回 True 的时候，结果才是 True 二叉树中和为某一值的路径题目描述输入一颗二叉树的根节点和一个整数，打印出二叉树中结点值的和为输入整数的所有路径。路径定义为从树的根结点开始往下一直到叶结点所经过的结点形成一条路径。(注意: 在返回值的list中，数组长度大的数组靠前) 思路 递归 代码实现123456789101112131415161718192021class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: # 返回二维列表，内部每个列表表示找到的路径 def FindPath(self, root, expectNumber): if not root: #如果树为空，返回空列表 return [] result=[] if not root.left and not root.right and expectNumber==root.val: #节点为叶子节点且值等于给定值 return [[root.val]] #注意返回的是二维列表 else: left=self.FindPath(root.left,expectNumber-root.val) #递归查找左右子树中的路径（值要减去根节点的值） right=self.FindPath(root.right,expectNumber-root.val) for i in left+right: #与根节点合并 result.append([root.val]+i) return sorted(result, key=lambda x: -len(x)) #输出按路径长度降序的列表，这里直接调用sorted函数 #也可以自己编写一个排序函数，然后调用 复杂链表的复制题目描述输入一个复杂链表（每个节点中有节点值，以及两个指针，一个指向下一个节点，另一个特殊指针指向任意一个节点），返回结果为复制后复杂链表的head。（注意，输出结果中请不要返回参数中的节点引用，否则判题程序会直接返回空） 思路 递归 非递归。 复制原来的链表，顺次链接成新的链表；利用原来结点的random指向，来为复制的结点指向相应的random将复制好的链表拆分出来，或者说将偶数位的节点重新拆分合成新的链表，得到的就是复制的链表 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class RandomListNode: def __init__(self, x): self.label = x self.next = None self.random = None#方法一，非递归class Solution: # 返回 RandomListNode def Clone(self, pHead): if not pHead: return pHead #复制原来的链表。顺次连接成新的链表 cloNode=pHead while cloNode: node=RandomListNode(cloNode.label) node.next=cloNode.next cloNode.next=node cloNode=node.next #利用原来结点的random指向，来为复制的结点指向相应的random cloNode=pHead while cloNode: node=cloNode.next if cloNode.random: node.random=cloNode.random.next cloNode=node.next #将复制好的链表拆分出来，或者说将偶数位的节点重新拆分合成新的链表，得到的就是复制的链表 cloNode=pHead pHead=pHead.next #必须执行此操作，否则最后得到的pHead就是原链表的头结点了 while cloNode.next: node=cloNode.next cloNode.next=node.next cloNode=node return pHead#方法二，递归class Solution2: # 返回 RandomListNode def Clone(self, pHead): if not pHead: return None cloneHead = RandomListNode(pHead.label) cloneHead.random = pHead.random cloneHead.next = self.Clone(pHead.next) return cloneHead]]></content>
      <categories>
        <category>在线编程</category>
      </categories>
      <tags>
        <tag>jian offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer（四）]]></title>
    <url>%2F%E5%9C%A8%E7%BA%BF%E7%BC%96%E7%A8%8B%2F2019-4-16-%E5%89%91%E6%8C%87offer4%2F</url>
    <content type="text"><![CDATA[剑指offer（16-20）。 合并两个排序的链表题目描述输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 思路 递归 非递归 代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546class ListNode: def __init__(self, x): self.val = x self.next = None#递归方法class Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): list_merge=None #构建一个新的链表存储合并后的链表 if pHead1==None: #一个为空就返回另外一个链表 return pHead2 if pHead2==None: return pHead1 if pHead1.val&lt;=pHead2.val: list_merge=pHead1 list_merge.next=self.Merge(pHead1.next,pHead2) else: list_merge=pHead2 list_merge.next=self.Merge(pHead1,pHead2.next) return list_merge#非递归方法class Solution2: # 返回合并后列表 def Merge(self, pHead1, pHead2): head=ListNode(90) p=head if pHead1==None: return pHead2 if pHead2==None: return pHead1 while pHead1 and pHead2: if pHead1.val&lt;=pHead2.val: head.next=pHead1 pHead1=pHead1.next else: head.next=pHead2 pHead2=pHead2.next head=head.next #后移，方便链接下一次循环中选取的结点 if pHead1: #pHead1和pHead2一个为空，将其链接到合并后的链表中 head.next=pHead1 if pHead2: head.next=pHead2 return p.next 树的子结构题目描述输入两棵二叉树A，B，判断B是不是A的子结构。（ps：我们约定空树不是任意一个树的子结构） 思路 递归 代码实现1234567891011121314151617181920212223242526class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: def HasSubtree(self, pRoot1, pRoot2): result=False if pRoot1!=None and pRoot2!=None: #当Tree1和Tree2都不为空时比较，否则直接返回False if pRoot1.val==pRoot2.val: #如果找到了对应Tree2根节点的点 result=self.doesTree1haveTree2(pRoot1,pRoot2) #以这个根节点为起点判断是否包含Tree2 if not result: result=self.HasSubtree(pRoot1.left,pRoot2) #如果没找到，去root左儿子当作起点，递归判断是否包含Tree2 if not result: result=self.HasSubtree(pRoot1.right,pRoot2) #如果没找到，去root右儿子当作起点，递归判断是否包含Tree2 return result def doesTree1haveTree2(self,node1,node2): if node2==None: #如果Tree2已经遍历完了都能对上，返回True return True if node1==None: #如果Tree2还没遍历完，Tree1却遍历完了，返回False return False if node1.val!=node2.val: #其中只要有一个节点没对上，返回False return False #如果根节点对应的上，再分别去子节点里匹配 return self.doesTree1haveTree2(node1.left,node2.left) and self.doesTree1haveTree2(node1.right,node2.right) 二叉树的镜像题目描述操作给定的二叉树，将其变换为源二叉树的镜像。二叉树的镜像定义：源二叉树 8 / \ 6 10 / \ / \ 5 7 9 11 镜像二叉树 8 / \ 10 6 / \ / \ 11 9 7 5 思路 递归 非递归，借用一个栈或者队列 代码实现12345678910111213141516171819202122232425262728293031323334353637class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = None#方法一，递归class Solution: # 返回镜像树的根节点 def Mirror(self, root): if root==None: return None if root: root.left,root.right=root.right,root.left #翻转根节点的左右孩子结点 if root.left!=None: #递归翻转左子树 self.Mirror(root.left) if root.right!=None: #递归翻转右子树 self.Mirror(root.right) return root#方法二，非递归，借用一个栈实现（其实借用一个队列也可以，思路是一样的）class Solution2: # 返回镜像树的根节点 def Mirror(self, root): if root==None: return None result=[] #创建一个空栈 result.append(root) while result: tree=result[-1] #取栈顶元素 result.pop(-1) #栈顶元素出栈 if (tree.left!=None or tree.right!=None): #交换结点的左右孩子 tree.left,tree.right=tree.right,tree.left if tree.left: #左孩子入栈 result.append(tree.left) if tree.right: result.append(tree.right) #右孩子入栈 return root 顺时针打印矩阵题目描述输入一个矩阵，按照从外向里以顺时针的顺序依次打印出每一个数字，例如，如果输入如下4 X 4矩阵：1 2 3 4 输出：1,2,3,4,8,12,16,15 14,13,9,5,6,7,11,10.5 6 7 89 10 11 1213 14 15 16 思路 可以模拟魔方逆时针旋转的方法，一直做取出第一行的操作例如1 2 34 5 67 8 9输出并删除第一行后，再进行一次逆时针旋转，就变成：6 95 84 7继续重复上述操作即 按照矩阵最上一行、最右一列，最下一行和最左一列分别处理 代码实现1234567891011121314151617181920212223242526272829303132333435363738class Solution: # matrix类型为二维列表，需要返回列表 def printMatrix(self, matrix): result=[] #最终顺时针读取完成后的列表 while matrix: result+=matrix.pop(0) #循环取出矩阵第一行 if not matrix or not matrix[0]: #矩阵是否为空，个人感觉matrix[0]这个判断没有必要 break matrix=self.turn(matrix) #逆时针旋转矩阵 return result def turn(self,matrix): rows=len(matrix) #矩阵的行数 cols=len(matrix[0]) #矩阵列数 new_matrix=[] #最终逆时针旋转后的矩阵 for i in range(cols): new_matrix_cut=[] #存储每一次循环时，每一列的矩阵值 for j in range(rows): new_matrix_cut.append(matrix[j][i]) new_matrix.append(new_matrix_cut) #每一列读取完后，附加到总的矩阵列表 new_matrix.reverse() #列表中的矩阵反转 return new_matrix#方法二class Solution2: def printMatrix(self, matrix): res = [] #最终顺时针读取完成后的列表 while matrix: res += matrix.pop(0) #从前向后读取最上面的一行数据 if matrix and matrix[0]: for row in matrix: #读取最右边的一列（即每一行的最后一个数据） res.append(row.pop()) if matrix: #从后向前读取最下边的一行 res += matrix.pop()[::-1] if matrix and matrix[0]: # 读取最左边一列的数据（即每一行的第一个数据） for row in matrix[::-1]: res.append(row.pop(0)) return res 包含min函数的栈题目描述定义栈的数据结构，请在该类型中实现一个能够得到栈中所含最小元素的min函数（时间复杂度应为O（1））。 思路 设置一个辅助栈来存储最小的元素 代码实现1234567891011121314151617181920class Solution: def __init__(self): self.stack=[] #主栈 self.min_stack=[] #辅助栈，存储最小的元素 def push(self, node): #入栈 self.stack.append(node) #新元素入主栈 if not self.min_stack or node&lt;self.min_stack[-1]: #如果辅助栈为空或者新元素比辅助栈顶元素小，新元素也入辅助栈 self.min_stack.append(node) def pop(self): #出栈 if self.min_stack[-1]==self.stack[-1]: #如果辅助栈顶元素和主栈顶元素相同，辅助栈顶元素出栈 self.min_stack.pop() self.stack.pop() #主栈顶元素出栈 def top(self): #返回栈顶元素 return self.stack[-1] def min(self): #最小值为辅助栈顶元素值 return self.min_stack[-1]]]></content>
      <categories>
        <category>在线编程</category>
      </categories>
      <tags>
        <tag>jian offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer（三）]]></title>
    <url>%2F%E5%9C%A8%E7%BA%BF%E7%BC%96%E7%A8%8B%2F2019-4-16-%E5%89%91%E6%8C%87offer3%2F</url>
    <content type="text"><![CDATA[剑指offer（11-15）。 二进制中1的个数题目描述输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 思路如果是负数，先获取它的补码形式，然后统一为正数处理。发现，当一个数大于0时，不停让它与它的前一位进行按位与操作，即可获得其二进制表示中1的个数。 代码实现12345678910111213141516class Solution: def NumberOf1(self, n): # write code here count = 0 if n &lt; 0: n = n &amp; 0xffffffff #负数的补码表示 while n: count += 1 n = (n - 1) &amp; n #按位与操作，都为1则为1，否则为0 return count'''输入一个整数，输出该数二进制表示中0的个数。其中负数用补码表示。'''#将上述代码中的n = (n - 1) &amp; n 改为 n = n|(n+1) 数值的整数次方题目描述给定一个double类型的浮点数base和int类型的整数exponent。求base的exponent次方。 思路 直接调用函数pow，但非常不推荐这种方法。 累乘，考虑指数的3种情况（小于0，大于0和等于0） 位运算代替递归，同样减少乘法运算，但效率更高（python未实现） 代码实现123456789101112131415161718192021222324252627282930313233#方法一，直接调用库函数，在线刷题可以这样，但是如果是面试编程，千万不要这样class Solution: def Power(self, base, exponent): return (pow(base, exponent))#方法二，累乘，考虑指数的3种情况，26ms，5836kclass Solution2: def Power(self, base, exponent): # write code here if exponent &lt; 0: return 1 / self.Power(base, -exponent) elif exponent == 0: return 1 else: res = [base] for i in range(1, exponent): res.append(res[i - 1] * base) return res[-1]#方法三，位运算代替递归，同样减少乘法运算，但效率更高，才3ms，内存也少，536k# class Solution &#123;# public:# double Power(double base, int exponent) &#123;# long long p = abs((long long)exponent);# double r = 1.0;# while(p)&#123;# if(p &amp; 1) r *= base;# base *= base;# p &gt;&gt;= 1;# &#125;# return exponent &lt; 0 ? 1/ r : r;# &#125;# &#125;; 调整数组顺序使奇数位于偶数的前面题目描述输入一个整数数组，实现一个函数来调整该数组中数字的顺序，使得所有的奇数位于数组的前半部分，所有的偶数位于数组的后半部分，并保证奇数和奇数，偶数和偶数之间的相对位置不变。 思路 遍历，两个列表分别存储奇数和偶数，最后再相链接 排序，前偶数后奇数就交换 代码实现123456789101112131415161718192021222324252627282930313233343536#方法一，两个列表分别存储奇数和偶数，最后再相链接，取余操作也可以按位与操作class Solution: def reOrderArray(self, array): jishu=[] oushu=[] for number in array: if number%2!=0: #if number&amp;1==1: #按位与 jishu.append(number) else: oushu.append(number) return jishu+oushu#方法二，类似冒泡算法，前偶后奇数就交换class Solution2: def reOrderArray(self, array): for i in range(0,len(array)): j = len(array) - 1 while(j&gt;i): if(array[j]%2==1 and array[j-1]%2==0): array[j],array[j-1]=array[j-1],array[j] j-=1 return array#方法三，如果不考虑稳定排序，可以采取类似快排的方法class Solution3: def reOrderArray(self, array): i=0 j=len(array)-1 while i&lt;j: while (i&lt;j and array[i]%2==0): i+=1 while (i&lt;j and array[j]%2==1): j-=1 array[i],array[j]=array[j],array[i] return array 链表中倒数第k个结点题目描述输入一个链表，输出该链表中倒数第k个结点。 思路 借助一个列表，不断取链表结点存入列表，取列表倒数第k个值 设置两个指针，p1，p2，先让p2走k-1步， 然后再一起走，直到p2为最后一个 时，p1即为倒数第k个节点 代码实现1234567891011121314151617181920212223242526272829303132333435363738class ListNode: def __init__(self, x): self.val = x self.next = None#方法一，借助一个列表，不断取链表结点存入列表，取列表倒数第k个值class Solution: def FindKthToTail(self, head, k): result=[] while head!=None: result.append(head) head=head.next if(k&gt;len(result)or k&lt;1): return return result[-k]#方法二，设置两个指针，p1，p2，先让p2走k-1步，# 然后再一起走，直到p2为最后一个 时，p1即为倒数第k个节点class Solution2: def FindKthToTail(self, head, k): # write code here if head==None or k&lt;=0: return None #设置两个指针，p2指针先走（k-1）步，然后再一起走，当p2为最后一个时，p1就为倒数第k个 数 p2=head p1=head #p2先走，走k-1步，如果k大于链表长度则返回 空，否则的话继续走 while k&gt;1: if p2.next!=None: p2=p2.next k-=1 else: return None#两个指针一起 走，一直到p2为最后一个,p1即为所求 while p2.next!=None: p1=p1.next p2=p2.next return p1 反转链表题目描述输入一个链表，反转链表后，输出新链表的表头。 思路 非递归 递归 代码实现123456789101112131415161718192021222324252627class ListNode: def __init__(self, x): self.val = x self.next = None#方法一，非递归方法class Solution: def ReverseList(self, pHead): pre=None if pHead==None or pHead.next==None: #如果当前结点为空或者其后继结点为空，则返回当前结点 return pHead while pHead!=None: tmp=pHead.next #tmp保存当前结点的下一个结点 pHead.next=pre #当前结点指向前一个结点，反转指针 pre=pHead #当前结点变成下一轮的前一个结点 pHead=tmp #当前结点后移一位 return pre#方法二，递归方法class Solution2: def ReverseList(self, pHead): pre=None if pHead==None or pHead.next==None: #如果当前结点为空或者其后继结点为空，则返回当前结点 return pHead p_new=self.ReverseList(pHead.next) #先反转后面的链表，走到链表的末端结点 pHead.next.next=pHead #再将当前结点设置为后面结点的后继结点 pHead.next=None return p_new]]></content>
      <categories>
        <category>在线编程</category>
      </categories>
      <tags>
        <tag>jian offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer（二）]]></title>
    <url>%2F%E5%9C%A8%E7%BA%BF%E7%BC%96%E7%A8%8B%2F2019-4-16-%E5%89%91%E6%8C%87offer2%2F</url>
    <content type="text"><![CDATA[剑指offer编程（6-10） 旋转数组中的最小数字题目描述把一个数组最开始的若干个元素搬到数组的末尾，我们称之为数组的旋转。输入一个非减排序的数组的一个旋转，输出旋转数组的最小元素。例如数组{3,4,5,1,2}为{1,2,3,4,5}的一个旋转，该数组的最小值为1。NOTE：给出的所有元素都大于0，若数组大小为0，请返回0。 思路一直接遍历数组，如果一个数比前一个数小，该数即为最小，复杂度较高O（n） 代码实现123456789class Solution: def minNumberInRotateArray(self, rotateArray): if (len(rotateArray)==0): return 0 min=rotateArray[0] for i in range(len(rotateArray)-1): #如果一个数比前一个数小，该数即为最小 if (rotateArray[i]&lt;min): min = rotateArray[i] return min 思路二二分查找，复杂度为O（logn）需要考虑三种情况： array[mid] &gt; array[high]:出现这种情况的array类似[3,4,5,6,0,1,2]，此时最小数字一定在mid的右边。low = mid + 1 array[mid] == array[high]:出现这种情况的array类似 [1,0,1,1,1] 或者[1,1,1,0,1]，此时最小数字不好判断在mid左边还是右边,这时只好一个一个试 ，high = high - 1 array[mid] &lt; array[high]:出现这种情况的array类似[2,2,3,4,5,6,6],此时最小数字一定就是array[mid]或者在mid的左边。因为右边必然都是递增的。high = mid 代码实现123456789101112131415class Solution2: def minNumberInRotateArray2(self, rotateArray): if(len(rotateArray)==0): return 0 low=0 high=len(rotateArray)-1 while(low&lt;high): mid=low+int((high-low)/2) if(rotateArray[mid]&gt;rotateArray[high]): low=mid+1 elif(rotateArray[mid]&lt;rotateArray[high]): high=mid else: high=high-1 return rotateArray[low] 斐波那契数列题目描述现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。n&lt;=390 ，1 ，1， 2， 3，5，8，13，21，34……. 思路一利用一个列表进行存储递归。 代码实现12345678class Solution: def Fibonacci(self, n): a=[0,1,1] #前3项的值 if n&lt;3: return a[n] for i in range(3,n+1): a.append(a[i-2]+a[i-1]) #循环存入列表 return a[n] 思路二找规律，递归赋值。 代码实现 123456789101112class Solution2: def Fibonacci(self, n): # write code here if n==0: return 0 if n==1: return 1 a,b = 0,1 while n&gt;0: a,b = b,a+b n-=1 return a 跳台阶题目描述一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。 思路与上一题类似，通过列举，可以发现$f[n]=f[n-2]+f[n-1]$ 代码实现123456789101112131415161718class Solution: def jumpFloor(self, number): #f[n]=f[n-2]+f[n-1],与斐波那契额数列类似 result=[1,1] if number&lt;2: return result[number] for i in range(2,number+1): result.append(result[i-2]+result[i-1]) return result[number]#方法二class Solution2: def jumpFloor(self, number): # write code here a=1 b=1 while number&gt;0: a,b = b,a+b number-=1 return a 变态跳台阶题目描述一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 思路通过列举找规律，发现$f[n]=2^{n-1}$ 代码实现123456class Solution: def jumpFloorII(self, number): #找规律，f[n]=2的n-1次方 if number&lt;=0: return 0 else: return pow(2,number-1) 矩形覆盖题目描述我们可以用$2 \times 1$的小矩形横着或者竖着去覆盖更大的矩形。请问用n个$2 \times 1$的小矩形无重叠地覆盖一个$2 \times n$的大矩形，总共有多少种方法？ 思路与跳台阶一样，通过列举，可以发现$f[n]=f[n-2]+f[n-1]​$ 代码实现1234567891011121314151617181920212223class Solution: def rectCover(self, number): #还是斐波那契数列，f[n]=f[n-1]+f[n-2] if number == 0: return 0 result = [1, 1] if number &lt; 2: return result[number] for i in range(2, number + 1): result.append(result[i - 2] + result[i - 1]) return result[number] #方法二class Solution2: def rectCover(self, number): # write code here if number == 0: return 0 a=1 b=1 while number&gt;0: a,b = b,a+b number-=1 return a]]></content>
      <categories>
        <category>在线编程</category>
      </categories>
      <tags>
        <tag>jian offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[剑指offer（一）]]></title>
    <url>%2F%E5%9C%A8%E7%BA%BF%E7%BC%96%E7%A8%8B%2F2019-4-16-%E5%89%91%E6%8C%87offer1%2F</url>
    <content type="text"><![CDATA[这几天开始刷牛客网上剑指offer的编程题，下面一系列博客将用python实现里面的编程题，很多参考了大神的思路（C实现），然后用python实现的。 二维数组查找题目描述在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。6 14 22 268 16 24 3010 20 30 40输入10 ，返回True 思路观察数组规律，从左下角元素开始找，如果目标值比它大，就查找下一列，比它小，就查找上一行。 代码实现123456789101112131415class Solution: # array 二维列表 def Find(self, target, array): rows=len(array)-1 cols=len(array[0])-1 i=rows j=0 while (j&lt;=cols and i&gt;=0): #从左下角元素开始寻找 if(target&lt;array[i][j]): #比左下角元素小，查找上一行 i=i-1 elif(target&gt;array[i][j]): #比左下角元素大，查找下一列 j=j+1 else: return True return False 替换空格题目描述请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为 We%20Are%20Happy。 思路这题很简单，一个循环遍历加替换就行了，需要注意的最后需要把列表中的值转换为字符串输出。 代码实现12345678class Solution: # s 源字符串 def replaceSpace(self, s): s1=list(s) #转换成列表处理 for i in range(len(s1)): if(s1[i]==' '): s1[i]='%20' return ''.join(s1) #将列表元素以字符串形式输出 从尾到头打印链表题目描述输入一个链表，按链表值从尾到头的顺序返回一个ArrayList。 思路利用一个列表，不断的取链表结点，通过insert方法将元素前插到列表。 代码实现12345678910111213class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: # 返回从尾部到头部的列表值序列，例如[1,2,3] def printListFromTailToHead(self, listNode): result=[] #存储从尾到头输出的元素 head = listNode #链表头结点 while (head is not None): result.insert(0,head.val) #将从头部取出的结点前插到列表中 head=head.next #指向下一个结点 return result 重建二叉树题目描述输入某二叉树的前序遍历和中序遍历的结果，请重建出该二叉树。假设输入的前序遍历和中序遍历的结果中都不含重复的数字。例如输入前序遍历序列{1,2,4,7,3,5,6,8}和中序遍历序列{4,7,2,1,5,3,8,6}，则重建二叉树并返回。 思路因为先序遍历中，根节点是第一个被访问的，所以先序序列中的第一个结点就是根节点，然后根据该结点值在中序遍历中的位置，可以将中序遍历划分为左子树、根节点、右子树。最后结合先序遍历和中序遍历序列中左右子树下标递归即可。 代码实现123456789101112131415class TreeNode: def __init__(self, x): self.val = x self.left = None self.right = Noneclass Solution: # 返回构造的TreeNode根节点 def reConstructBinaryTree(self, preorder, inorder): if not preorder or not inorder: return root = TreeNode(preorder[0]) #前序序列中的第一个元素为根节点 i=inorder.index(preorder[0]) #根节点元素在中序序列中的位置，从而划分得到左子树和右子树中的元素 root.left=self.reConstructBinaryTree(preorder[1:i+1],inorder[:i]) #递归重建左子树 root.right=self.reConstructBinaryTree(preorder[i+1:],inorder[i+1:]) #递归重建右子树 return root 两个栈实现队列题目描述用两个栈来实现一个队列，完成队列的Push和Pop操作。队列中的元素为int类型。 思路队列符合先进先出规则。栈符合先进后出规则。入队时：直接将元素压入stack1出队时：若stack2为空，先将stack1中的元素出栈压入栈2，栈2在出栈 代码实现123456789101112class Solution: def __init__(self): self.stack1=[] self.stack2=[] def push(self, node): self.stack1.append(node) #入队时，直接将元素压入栈1 def pop(self): if self.stack2==[]: #出队时，若栈2为空，先将栈1中的元素出栈压入栈2，栈2再出栈 while self.stack1: self.stack2.append(self.stack1.pop()) return self.stack2.pop() return self.stack2.pop() 用2个队列实现一个栈呢？思路所有元素进入q1，因为我们的目的是栈，也就是最先出c,儿队是从队头开始出，所有先把ab出q1并入q2,此时目标c跑到了队头，出q1。此时q1已经为空，下一个要出的是b,把a从q2出队并进q1,此时目标b在q2队头,出队……..即：把非空队列的n-1个压人空对列，剩的第n个出队…即总有一个队列为空。 代码实现12345678910111213141516class StackWithTwoQueues(object): #定义两个空队列 def __init__(self): self.queue1 = [] self.queue2 = [] #入栈 def push(self, item): self.queue1.append(item) #出栈 def pop(self): if len(self.queue1) == 0: return(None) while(len(self.queue1) != 1): self.queue2.append(self.queue1.pop(0)) self.queue1, self.queue2 = self.queue2, self.queue1 return (self.queue2.pop())]]></content>
      <categories>
        <category>在线编程</category>
      </categories>
      <tags>
        <tag>jian offer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于深度神经网络的图像分类与去噪]]></title>
    <url>%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%2F2019-4-24-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E4%B8%8E%E5%8E%BB%E5%99%AA%2F</url>
    <content type="text"><![CDATA[本篇博文主要介绍噪音类型的分类和去噪。 项目简介现有的去噪方法取决于噪声类型的信息，通常由专家分类。换句话说，那些方法没有应用计算方法来对图像噪声类型进行预分类。此外，这些方法假设图像的噪声类型是像高斯噪声那样的单类噪声类型，这限制了实际应用中去噪方法的选择和能力。与现有方法不同，我们采用一种新框架，不仅可以对单一类型噪声进行分类和去噪，而且可以根据实际需要对混合类型的噪声进行分类和去噪。 我们的方法和现有方法对比 数据集 Mnist：Subset（10000张） cifar10：Subset（10000张） BSD500：Fullset（500张） 方法设计一个包含2个网络的框架：噪音分类网络 去噪网络。 噪音分类网络：对图像所包含的噪音类型进行识别分类 去噪网络：根据噪音分类网络判别的噪音类型选择相应的去噪模型去除噪音，进而得到干净图像 总体框架图如下所示 项目实现过程数据预处理针对原始BSD500数据集，对其进行灰度转换，得到它的灰度图。对于Mnist（gray）、Cifar10（RGB）、BSD500（Gray&amp;RGB）这4个数据集，在每个数据集上添加Gaussian、Salt、Speckle、Poisson噪音（可以组合），最终得到 单类噪音数据集4类：Ga，Sa，Sp ，Po 两类噪音数据集6类：Ga&amp;Sa，Ga&amp;Sp，Ga&amp;Po，Sa&amp;Sp，Sa&amp;Po，Sp&amp;Po 多类噪音数据集5类（3和4）：Ga&amp;Sa&amp;Sp，Ga&amp;Sa&amp;Po，Ga&amp;Sp&amp;Po，Sa&amp;Sp&amp;Po， Ga&amp;Sa&amp;Sp&amp;Po 混合噪音数据集共15类：综合上述1，2，3 噪音分类网络网络模型 详细参数 利用Mnist和Cifar10的噪音数据集，对比几个分类网络，筛选出噪音识别准确率最高的网络（Our）。 为了和去噪网络的数据集匹配，将BSD的混合噪音也输入了我们设计的CNN网络，训练了一个分类模型。结果如下： 去噪网络设计 去噪网络主要由三种类型的层组成，如上图所示，由三种不同的颜色表示。 在第一层，我们使用96个尺寸的3×3×c滤波器进行卷积并生成96个特征图，我们在输入到下一层时使用整流线性单元的变量LReLU 来实现非线性。 这里，c表示训练图像的通道数，c表示灰度1，c表示彩色图3。 从第2层到第（d-1）层，使用96个尺寸为3×3×96的滤波器 并且还使用了LReLU，并在卷积层和LReLU层之间进行了批量归一化操作，从而可以提高训练速度并加快收敛过程。 对于最后一层，它使用大小为3×3×96的c滤波器重建输出。 它还删除了所有下采样层，因为这会大大降低我们模型的去噪效果。 把BSD500的8种单类噪音（Gray和RGB各4种）数据输入设计的去噪网络，得到了8个单类噪音去噪模型。 测试： 如果图片被分类网络识别出仅包含单类噪音，就选择相应的单类去噪模型去除噪音 如果图片被分类网络识别出包含多种噪音，根据所包含的噪音类型，依次选择多个单类去噪模型去除噪音 去噪结果分析去噪好坏的评价标准 PSNR（Peak Signal to Noise Ratio）峰值信噪比 其中，MSE表示当前图像X和参考图像Y的均方误差（Mean Square Error），H、W分别为图像的高度和宽度；n为每像素的比特数，一般取8，即像素灰阶数为256.。PSNR的单位是dB，数值越大表示失真越小越好。PSNR是最普遍和使用最为广泛的一种图像客观评价指标，然而它是基于对应像素点间的误差，即基于误差敏感的图像质量评价。由于并未考虑到人眼的视觉特性（人眼对空间频率较低的对比差异敏感度较高，人眼对亮度对比差异的敏感度较色度高，人眼对一个区域的感知结果会受到其周围邻近区域的影响等），因而经常出现评价结果与人的主观感觉不一致的情况。为此我们还引入了下面一个评价指标 SSIM（structural similarity）结构相似性。它分别从亮度、对比度、结构三方面度量图像相似性。 其中$\mu_X$、$\mu_Y$分别表示图像$X$和$Y$的均值，$\sigma_X$、$\sigma_Y$分别表示图像$X$和$Y$的方差，$\sigma_{XY}$表示图像$X$和$Y$的协方差，即 $C_1$、$C_2$、$C_3$为常数，为了避免分母为0的情况，通常取$C_1=(K_1 \times L)^2$, $C2=(K_2 \times L)^2$, $C_3=C_2/2$, 一般地$K_1=0.01$,$K_2=0.03$, $L=255​$， 则 SSIM取值范围[0,1]，值越大，表示图像失真越小越好。 单类噪音 与其它方法的对比 两类噪音 多类噪音 总结 在单类噪音上取得了不错的效果，彩色图要比灰度图的去噪效果好，而且比已有的方法要好。 多类噪音去噪效果不够好，灰度图要比彩色图好。]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>CNN</tag>
        <tag>denoise</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树和二叉树的应用]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-4-7-%E6%A0%91%E5%92%8C%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文主要介绍树和二叉树的应用，主要包含3个部分：二叉排序树、平衡二叉树和哈夫曼树和哈夫曼编码。 二叉排序树（BST）定义二叉排序树（简称BST），也称二叉查找树。二叉排序树或者是一棵空树，或者是一棵有下列特性的非空二叉树： 若左子树非空，则左子树上所有结点关键字的值均小于根结点的关键字值 若右子树非空，则右子树上所有结点关键字的值均大于根结点的关键字值 左、右子树本身也分别是一棵二叉排序树 查找关于二叉排序的查找，在C语言实现七大查找算法（三）我的这篇博文中，已经详细介绍了，此处不再赘述。 查找效率：平均查找长度主要取决于树的高度。 二叉排序树和二分查找的对比 二叉排序树 二分查找（有序顺序表） 判定树 不唯一 唯一 有序性 无需移动结点，修改指针 插入、删除 $O(log_2^n)$ $O(n)$ 有序表静态 √ 有序表动态 √ 插入二叉排序树作为一种动态集合，特点是树的结构不是一次生成的，而是在查找过程中当树中不存在关键字给定值的结点时在进行插入。 由于二叉排序树是递归定义的，插入结点的过程如下： 若原二叉排序树为空，直接插入结点 若关键字$k$小于根结点的关键字，插入到左子树中 若关键字$k$大于根结点的关键字，插入到右子树中 一个插入示例如下（插入28，58），发现插入的新结点一定是某个叶子结点。 实现代码如下1234567891011121314int BST_InSert(BiTree &amp;T,KeyType k)&#123; if(T==NULL)&#123; //原树为空，新插入的记录为根结点 T=(BiTree) malloc (sizeof(BSTNode)); T-&gt;key=k; T-&gt;lchild=T-&gt;rchild=NULL; return 1; &#125; else if(k==T-&gt;key) //树中存在相同关键字的结点 return 0; else if(k&lt;T-&gt;key) return BST_InSert(T-&gt;lchild,k); //递归插入T的左子树中 else return BST_InSert(T-&gt;rchild,k); //递归插入T的右子树中&#125; 构造123456789void Creat_BST(BiTree &amp;T,KeyType str[],int n)&#123; //用关键字数组str[]建立一个二叉排序树 T=NULL; int i=0; while(i&lt;n)&#123; BST_InSert(T,str[i]); i++; &#125;&#125; 删除在二叉排序树中删除一个结点时，不能把以该结点为根的子树上的结点都删除，必须先把被删除结点从存储二叉排序树的链表上摘下，将因删除结点而断开的二叉链表重新链接起来，同时确保二叉排序树的性质不会丢失。 删除操作实现过程按以下3种情况来处理： 若删除的结点z为叶子结点，直接删除 若结点$z$只有一颗左子树或右子树，则让$z$的子树成为$z$父节点的子树，替代$z$的位置 若$z$有左右2棵子树，则令$z$的直接后继（或直接前驱）替代$z$，然后从$BST$中删去这个直接后继（或直接前驱），这样就转化为了第一种或第二种情况 删除示例如下： 平衡二叉树（AVL）定义为了避免树的高度增长过快，降低二叉排序树的性能，我们规定在插入和删除二叉树结点时，要保证任意结点的左、右子树高度差的绝对值不超过1，将这样的树称为平衡二叉树。 平衡因子：结点左子树和右子树的高度差，取值只可能为$-1,0,1​$。 平衡二叉树示例图 插入 未破坏平衡，无需调整 破坏平衡，先找到插入路径上离插入结点最近的平衡因子绝对值大于1的结点A，再对以A为根的子树，调整各结点的位置关系，使之重新达到平衡。 示例图如下，其中75即为A结点 平衡二叉树的插入过程前半部分与二叉排序树相同，但是在插入新结点后，如果造成了不平衡，要做出相应的调整，可以归纳为以下4种调整情况： LL（右单旋转）在A的左孩子的左子树上插入新结点。将A的左孩子B向右上旋转替代A成为根结点，将A结点向右下旋转成为B的右子树的根结点，而B的原右子树则作为A结点的左子树。 RR（左单旋转）在A的右孩子的右子树上插入新结点。将A的右孩子B向左上旋转替代A成为根结点，将A结点向左下旋转成为B的左子树的根结点，而B的原左子树则作为A结点的右子树。 LR（先左后右）在A的左孩子的右子树上插入新结点。先将A的左孩子B的右子树的根结点C向左上旋转提升到B的位置，然后再把该C结点向右上旋转提升到A的位置。 RL（先右后左）在A的右孩子的左子树上插入新结点。先将A的右孩子B的左子树的根结点C向右上旋转提升到B的位置，然后再把该C结点向左上旋转提升到A的位置。 查找平衡二叉树的查找过程和二叉排序树相同。假设$N_h$表示深度为$h$的$AVL$中含有的最少结点数，显然$N_0=1,N_1=2,N_h=N_{h-1}+N_{h-2}+1$。 结论：$n$个结点的平衡二叉树的最大深度为$log_2^n$，平均查找长度为$O(log_2^n)​$。 哈夫曼（Huffman）树和哈夫曼编码哈夫曼树定义和构造 示例 哈夫曼编码]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线索二叉树]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-4-7-%E7%BA%BF%E7%B4%A2%E4%BA%8C%E5%8F%89%E6%A0%91%2F</url>
    <content type="text"><![CDATA[本文主要介绍线索二叉树和树、二叉树、森林三者之间的相互转换。 对于线索二叉树，这里只做简单介绍，着重还是要理解上篇博文中二叉树的各种遍历算法。 线索二叉树基本概念遍历二叉树的实质就是对一个非线性结构进行线性化操作，使在这个访问序列中每一个结点（除第一个和最后一个）都有一个直接前驱和直接后继。 传统的链式存储仅能体现一种父子关系，不能直接得到结点在遍历中的前驱和后继。通过观察，我们发现在二叉链表表示的二叉树中存在大量的空指针，若利用这些空链域存放指向其直接前驱或后继的指针，则可以方便的运用某些二叉树操作算法。引入线索二叉树的目的是为了加快查找结点前驱和后继的速度。 在二叉树线索化时，通常规定： 若无左子树，令$lchild$指向其前驱结点，若无右子树，令$rchild$指向其后继结点 还需增加两个标志域表明当前指针域所指的对象是指向左（右）子结点还是直接前驱（后继） 线索二叉树存储结构描述如下：12345typedef struct ThreadNode&#123; ElemType data; //数据元素 struct ThreadNode *lchild,*rchild; //左、右孩子指针 int ltag,rtag; //左、右线索标志&#125;ThreadNode,*ThreadTree; 线索二叉树的构造对二叉树的线索化，实质上就是遍历一次二叉树，只是在遍历的过程中，检查当前结点左、右指针域是否为空，若为空，将它们改为指向前驱结点或后继结点的线索。 下面以中序线索二叉树建立为例，指针$pre（suc）$指向中序遍历时上一个刚访问过（下一个访问）的结点，如下图所示（中序遍历为 B D A E C）： 下面给出中序遍历时对二叉树线索化的递归算法123456789101112131415void InThread(ThreadTree &amp;p,ThreadTree &amp;pre)&#123; if(p!=NULL)&#123; InThread(p-&gt;lchild,pre); //递归，线索化左子树 if(p-&gt;lchild==NULL)&#123; //左子树为空，建立前驱线索 p-&gt;lchild=pre; p-&gt;tag=1; &#125; if(pre!=NULL&amp;&amp;pre-&gt;rchild==NULL)&#123; pre-&gt;rchild=p; //建立前驱结点的后继线索 pre-&gt;rtag=1; &#125; pre=p; //标记当前结点成为刚刚访问过的结点 InThread(p-&gt;rchild,pre) //递归，线索化右子树 &#125;&#125; 调用上述方法，建立中序线索二叉树的主过程12345678void CreateInThread(ThreadTree T)&#123; ThreadTree pre=NULL; if(T!=NULL)&#123; //非空二叉树，线索化 InThread(T,pre); //线索化二叉树 pre-&gt;rchild=NULL; //处理遍历的最后一个结点 pre-&gt;rtag=1; &#125;&#125; 树、森林、二叉树的相互转换在介绍转换方法前，我们先来看一下树的存储结构 树 —–&gt; 二叉树 森林 —–&gt; 二叉树 二叉树 —–&gt; 树 二叉树 —–&gt; 森林 树和森林的遍历可采用对应二叉树遍历算法实现，与二叉树遍历的对应关系如下表所示： 树 森林 二叉树 先根遍历 先序遍历 先序遍历 后根遍历 中序遍历 中序遍历]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现二叉树遍历的递归和非递归算法]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-4-6-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%A0%913%2F</url>
    <content type="text"><![CDATA[本文主要用python实现二叉树的4种遍历算法，除层次遍历外，前序、中序和后序遍历分别包含递归和非递归2种实现方式。 前序遍历12345678910111213141516171819202122232425# -----------前序遍历 ------------ # 递归算法 def pre_order_recursive(self, T): if T == None: return print(T.root, end=' ') self.pre_order_recursive(T.lchild) self.pre_order_recursive(T.rchild) # 非递归算法 def pre_order_non_recursive(self, T): """借助栈实现前驱遍历 """ if T == None: return stack = [] while T or len(stack) &gt; 0: if T: stack.append(T) print(T.root, end=' ') T = T.lchild else: T = stack[-1] stack.pop() T = T.rchild 中序遍历123456789101112131415161718192021222324# -----------中序遍历 ------------ # 递归算法 def mid_order_recursive(self, T): if T == None: return self.mid_order_recursive(T.lchild) print(T.root, end=' ') self.mid_order_recursive(T.rchild) # 非递归算法 def mid_order_non_recursive(self, T): """借助栈实现中序遍历 """ if T == None: return stack = [] while T or len(stack) &gt; 0: if T: stack.append(T) T = T.lchild else: T = stack.pop() print(T.root, end=' ') T = T.rchild 后序遍历12345678910111213141516171819202122232425262728# -----------后序遍历 ------------ # 递归算法 def post_order_recursive(self, T): if T == None: return self.post_order_recursive(T.lchild) self.post_order_recursive(T.rchild) print(T.root, end=' ') # 非递归算法 def post_order_non_recursive(self, T): """借助两个栈实现后序遍历 """ if T == None: return stack1 = [] stack2 = [] stack1.append(T) while stack1: node = stack1.pop() if node.lchild: stack1.append(node.lchild) if node.rchild: stack1.append(node.rchild) stack2.append(node) while stack2: print(stack2.pop().root, end=' ') return 层次遍历123456789101112131415# -----------层次遍历 ------------ def level_order(self, T): """借助队列（其实还是一个栈）实现层次遍历 """ if T == None: return stack = [] stack.append(T) while stack: node = stack.pop(0) # 实现先进先出 print(node.root, end=' ') if node.lchild: stack.append(node.lchild) if node.rchild: stack.append(node.rchild) 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183class NodeTree: def __init__(self, root=None, lchild=None, rchild=None): """创建二叉树 Argument: lchild: BinTree 左子树 rchild: BinTree 右子树 Return: Tree """ self.root = root self.lchild = lchild self.rchild = rchildclass BinTree: # -----------前序遍历 ------------ # 递归算法 def pre_order_recursive(self, T): if T == None: return print(T.root, end=' ') self.pre_order_recursive(T.lchild) self.pre_order_recursive(T.rchild) # 非递归算法 def pre_order_non_recursive(self, T): """借助栈实现前驱遍历 """ if T == None: return stack = [] while T or len(stack) &gt; 0: if T: stack.append(T) print(T.root, end=' ') T = T.lchild else: T = stack[-1] stack.pop() T = T.rchild # -----------中序遍历 ------------ # 递归算法 def mid_order_recursive(self, T): if T == None: return self.mid_order_recursive(T.lchild) print(T.root, end=' ') self.mid_order_recursive(T.rchild) # 非递归算法 def mid_order_non_recursive(self, T): """借助栈实现中序遍历 """ if T == None: return stack = [] while T or len(stack) &gt; 0: if T: stack.append(T) T = T.lchild else: T = stack.pop() print(T.root, end=' ') T = T.rchild # -----------后序遍历 ------------ # 递归算法 def post_order_recursive(self, T): if T == None: return self.post_order_recursive(T.lchild) self.post_order_recursive(T.rchild) print(T.root, end=' ') # 非递归算法 def post_order_non_recursive(self, T): """借助两个栈实现后序遍历 """ if T == None: return stack1 = [] stack2 = [] stack1.append(T) while stack1: node = stack1.pop() if node.lchild: stack1.append(node.lchild) if node.rchild: stack1.append(node.rchild) stack2.append(node) while stack2: print(stack2.pop().root, end=' ') return # -----------层次遍历 ------------ def level_order(self, T): """借助队列（其实还是一个栈）实现层次遍历 """ if T == None: return stack = [] stack.append(T) while stack: node = stack.pop(0) # 实现先进先出 print(node.root, end=' ') if node.lchild: stack.append(node.lchild) if node.rchild: stack.append(node.rchild) # ----------- 前序遍历序列、中序遍历序列 —&gt; 重构二叉树 ------------ def tree_by_pre_mid(self, pre, mid): if len(pre) != len(mid) or len(pre) == 0 or len(mid) == 0: return T = NodeTree(pre[0]) index = mid.index(pre[0]) T.lchild = self.tree_by_pre_mid(pre[1:index + 1], mid[:index]) T.rchild = self.tree_by_pre_mid(pre[index + 1:], mid[index + 1:]) return T # ----------- 后序遍历序列、中序遍历序列 —&gt; 重构二叉树 ------------ def tree_by_post_mid(self, post, mid): if len(post) != len(mid) or len(post) == 0 or len(mid) == 0: return T = NodeTree(post[-1]) index = mid.index(post[-1]) T.lchild = self.tree_by_post_mid(post[:index], mid[:index]) T.rchild = self.tree_by_post_mid(post[index:-1], mid[index + 1:]) return Tif __name__ == '__main__': # ----------- 测试：前序、中序、后序、层次遍历 ----------- # 创建二叉树 nodeTree = NodeTree(1, lchild=NodeTree(2, lchild=NodeTree(4, rchild=NodeTree(7))), rchild=NodeTree(3, lchild=NodeTree(5), rchild=NodeTree(6))) T = BinTree() print('前序遍历递归\t') T.pre_order_recursive(nodeTree) # 前序遍历-递归 print('\n') print('前序遍历非递归\t') T.pre_order_non_recursive(nodeTree) # 前序遍历-非递归 print('\n') print('中序遍历递归\t') T.mid_order_recursive(nodeTree) # 中序遍历-递归 print('\n') print('中序遍历非递归\t') T.mid_order_non_recursive(nodeTree) # 中序遍历-非递归 print('\n') print('后序遍历递归\t') T.post_order_recursive(nodeTree) # 后序遍历-递归 print('\n') print('后序遍历非递归\t') T.post_order_non_recursive(nodeTree) # 后序遍历-非递归 print('\n') print('层次遍历\t') T.level_order(nodeTree) # 层次遍历 print('\n') print('==========================================================================') # ----------- 测试：由遍历序列构造二叉树 ----------- T = BinTree() pre = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I'] mid = ['B', 'C', 'A', 'E', 'D', 'G', 'H', 'F', 'I'] post = ['C', 'B', 'E', 'H', 'G', 'I', 'F', 'D', 'A'] newT_pre_mid = T.tree_by_pre_mid(pre, mid) # 由前序序列、中序序列构造二叉树 T.post_order_recursive(newT_pre_mid) # 获取后序序列 print('\n') newT_post_mid = T.tree_by_post_mid(post, mid) # 由后序序列、中序序列构造二叉树 T.pre_order_recursive(newT_post_mid) # 获取前序序列 运行结果12345678910111213141516171819202122232425前序遍历递归 1 2 4 7 3 5 6 前序遍历非递归 1 2 4 7 3 5 6 中序遍历递归 4 7 2 1 5 3 6 中序遍历非递归 4 7 2 1 5 3 6 后序遍历递归 7 4 2 5 6 3 1 后序遍历非递归 7 4 2 5 6 3 1 层次遍历 1 2 3 4 5 6 7 ==========================================================================C B E H G I F D A A B C D E F G H I]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言实现二叉树遍历的递归和非递归算法]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-4-6-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%A0%912%2F</url>
    <content type="text"><![CDATA[本文主要介绍二叉树的各种遍历方法。 二叉树的遍历所谓二叉树的遍历，是指按某条搜索路径访问树中的每个结点，使得每个结点均被访问一次，而且仅被访问一次。 由二叉树的递归定义可知，遍历一棵二叉树便要决定对根结点$N$、左子树$L$和右子树$R$的访问顺序。按照先遍历左子树再遍历右子树的原则，常见的遍历次序有： 前序遍历：（N L R） 中序遍历：（L N R） 后序遍历：（L R L） 这里的序指的是根结点何时被访问 在介绍3种遍历算法前，我们先给出二叉树的存储结构和建立二叉树的代码： 所有代码均来自c实现树（二叉树）的建立和遍历算法（一）（前序，中序，后序） 二叉树的存储结构（二叉链表） 123456//二叉树的二叉链表结构，也就是二叉树的存储结构，1个数据域，2个指针域（分别指向左右孩子）typedef struct BiTNode&#123; ElemType data; struct BiTNode *lchild, *rchild;&#125;BiTNode, *BiTree; 建立二叉树（此处以前序遍历的方式建立） 1234567891011121314151617//二叉树的建立，按前序遍历的方式建立二叉树，当然也可以以中序或后序的方式建立二叉树void CreateBiTree(BiTree *T)&#123; ElemType ch; cin &gt;&gt; ch; if (ch == '#') *T = NULL; //保证是叶结点 else &#123; *T = (BiTree)malloc(sizeof(BiTNode)); //if (!*T) //exit(OVERFLOW); //内存分配失败则退出。 (*T)-&gt;data = ch;//生成结点 CreateBiTree(&amp;(*T)-&gt;lchild);//构造左子树 CreateBiTree(&amp;(*T)-&gt;rchild);//构造右子树 &#125;&#125; 下面给出2种算法，分别实现三种遍历方式 递归算法程序实现12345678910111213141516171819202122232425262728293031323334353637383940/递归方式前序遍历二叉树void PreOrderTraverse(BiTree T, int level)&#123; if (T == NULL) return;/*此处表示对遍历的树结点进行的操作，根据你自己的要求进行操作，这里只是输出了结点的数据*/ //operation1(T-&gt;data); operation2(T-&gt;data, level); //输出了层数 PreOrderTraverse(T-&gt;lchild, level + 1); PreOrderTraverse(T-&gt;rchild, level + 1);&#125;//递归方式中序遍历二叉树void InOrderTraverse(BiTree T,int level)&#123; if(T==NULL) return; InOrderTraverse(T-&gt;lchild,level+1); //operation1(T-&gt;data); operation2(T-&gt;data, level); //输出了层数 InOrderTraverse(T-&gt;rchild,level+1);&#125;//递归方式后序遍历二叉树void PostOrderTraverse(BiTree T,int level)&#123; if(T==NULL) return; PostOrderTraverse(T-&gt;lchild,level+1); PostOrderTraverse(T-&gt;rchild,level+1); //operation1(T-&gt;data); operation2(T-&gt;data, level); //输出了层数&#125; 下面是完整的实现代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#include&lt;iostream&gt;#include&lt;stdlib.h&gt;using namespace std;typedef char ElemType;//二叉树的二叉链表结构，也就是二叉树的存储结构，1个数据域，2个指针域（分别指向左右孩子）typedef struct BiTNode&#123; ElemType data; struct BiTNode *lchild, *rchild;&#125;BiTNode, *BiTree;//二叉树的建立，按前序遍历的方式建立二叉树，当然也可以以中序或后序的方式建立二叉树void CreateBiTree(BiTree *T)&#123; ElemType ch; cin &gt;&gt; ch; if (ch == '#') *T = NULL; //保证是叶结点 else &#123; *T = (BiTree)malloc(sizeof(BiTNode)); //if (!*T) //exit(OVERFLOW); //内存分配失败则退出。 (*T)-&gt;data = ch;//生成结点 CreateBiTree(&amp;(*T)-&gt;lchild);//构造左子树 CreateBiTree(&amp;(*T)-&gt;rchild);//构造右子树 &#125;&#125;//表示对遍历到的结点数据进行的处理操作,此处操作是将树结点前序遍历输出void operation1(ElemType ch)&#123; cout &lt;&lt; ch &lt;&lt; " ";&#125;//此处在输出的基础上，并输出层数void operation2(ElemType ch, int level)&#123; cout &lt;&lt; ch &lt;&lt; "在第" &lt;&lt; level &lt;&lt; "层" &lt;&lt; endl;&#125;//递归方式前序遍历二叉树void PreOrderTraverse(BiTree T, int level)&#123; if (T == NULL) return;/*此处表示对遍历的树结点进行的操作，根据你自己的要求进行操作，这里只是输出了结点的数据*/ //operation1(T-&gt;data); operation2(T-&gt;data, level); //输出了层数 PreOrderTraverse(T-&gt;lchild, level + 1); PreOrderTraverse(T-&gt;rchild, level + 1);&#125;//递归方式中序遍历二叉树void InOrderTraverse(BiTree T,int level)&#123; if(T==NULL) return; InOrderTraverse(T-&gt;lchild,level+1); //operation1(T-&gt;data); operation2(T-&gt;data, level); //输出了层数 InOrderTraverse(T-&gt;rchild,level+1);&#125;//递归方式后序遍历二叉树void PostOrderTraverse(BiTree T,int level)&#123; if(T==NULL) return; PostOrderTraverse(T-&gt;lchild,level+1); PostOrderTraverse(T-&gt;rchild,level+1); //operation1(T-&gt;data); operation2(T-&gt;data, level); //输出了层数&#125;int main()&#123; int level = 1; //表示层数 BiTree T = NULL; cout &lt;&lt; "请以前序遍历的方式输入扩展二叉树："; //类似输入AB#D##C## CreateBiTree(&amp;T);// 建立二叉树，没有树，怎么遍历 cout &lt;&lt; "递归前序遍历输出为：" &lt;&lt; endl; PreOrderTraverse(T, level);//进行前序遍历，其中operation1()和operation2()函数表示对遍历的结点数据进行的处理操作 cout &lt;&lt; endl; cout &lt;&lt; "递归中序遍历输出为：" &lt;&lt; endl; InOrderTraverse(T, level); cout &lt;&lt; endl; cout &lt;&lt; "递归后序遍历输出为：" &lt;&lt; endl; PostOrderTraverse(T, level); cout &lt;&lt; endl; return 0;&#125; 注意： 建立二叉树时，这里是以前序遍历的方式，输入的是扩展二叉树，也就是要告诉计算机什么是叶结点，否则将一直递归，当输入“#”时，指针指向NULL，说明是叶结点。 如下图为扩展二叉树：（前序遍历为：ABDG##H###CE#I##F##） operation1( )函数只是对各个结点的输出； operation2( )函数不仅输出了各个结点，同时输出了结点所在的层数。 运行结果 只是运行了operation2( )函数，有层数输出： 只是运行了operation1( )函数，输出值： 非递归算法 我们可以借助栈，实现3种遍历的非递归算法 除此之外，还给出了二叉树的层次遍历算法，所谓层次遍历，就是自顶向下、自左至右的遍历二叉树中的元素，可以借助队列实现。程序实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128//非递归方式前序遍历/* 思路：将T入栈，遍历左子树；遍历完左子树返回时，栈顶元素应为T，出栈，再先序遍历T的右子树。*/void PreOrder(BiTree T)&#123; stack&lt;BiTree&gt; stack; //p是遍历指针 BiTree p = T; //p不为空或者栈不空时循环 while (p || !stack.empty())&#123; if (p != NULL) &#123; //存入栈中 stack.push(p); //对树中的结点进行操作 operation1(p-&gt;data); //遍历左子树 p = p-&gt;lchild; &#125; else &#123; //退栈 p = stack.top(); stack.pop(); //访问右子树 p = p-&gt;rchild; &#125; &#125; &#125;//非递归中序遍历void InOrder(BiTree T)&#123; stack&lt;BiTree&gt; stack; //p是遍历指针 BiTree p = T; //p不为空或者栈不空时循环 while (p || !stack.empty()) &#123; if (p != NULL) &#123; //存入栈中 stack.push(p); //遍历左子树 p = p-&gt;lchild; &#125; else &#123; //退栈 p = stack.top(); operation1(p-&gt;data); //对树中的结点进行操作 stack.pop(); //访问右子树 p = p-&gt;rchild; &#125; &#125; &#125;//非递归后序遍历typedef struct BiTNodePost&#123; BiTree biTree; char tag;&#125;BiTNodePost, *BiTreePost;void PostOrder(BiTree T)&#123; stack&lt;BiTreePost&gt; stack; //p是遍历指针 BiTree p = T; BiTreePost BT; //栈不空或者p不空时循环 while (p != NULL || !stack.empty()) &#123; //遍历左子树 while (p != NULL) &#123; BT = (BiTreePost)malloc(sizeof(BiTNodePost)); BT-&gt;biTree = p; //访问过左子树 BT-&gt;tag = 'L'; stack.push(BT); p = p-&gt;lchild; &#125; //左右子树访问完毕访问根节点 while (!stack.empty() &amp;&amp; (stack.top())-&gt;tag == 'R') &#123; BT = stack.top(); //退栈 stack.pop(); p=BT-&gt;biTree; cout&lt;&lt;BT-&gt;biTree-&gt;data&lt;&lt;" "; &#125; //遍历右子树 if (!stack.empty()) &#123; BT = stack.top(); //访问过右子树 BT-&gt;tag = 'R'; p = BT-&gt;biTree; p = p-&gt;rchild; &#125; &#125;&#125;//层次遍历 void LevelOrder(BiTree T)&#123; BiTree p = T; queue&lt;BiTree&gt; queue; //根节点入队 queue.push(p); //队列不空循环 while (!queue.empty()) &#123; //对头元素出队 p = queue.front(); //访问p指向的结点 operation1(p-&gt;data); //退出队列 queue.pop(); //左孩子不为空，将左孩子入队 if (p-&gt;lchild != NULL) &#123; queue.push(p-&gt;lchild); &#125; //右孩子不空，将右孩子入队 if (p-&gt;rchild != NULL) &#123; queue.push(p-&gt;rchild); &#125; &#125;&#125; 下面是完整的实现代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243#include&lt;iostream&gt;#include&lt;stdlib.h&gt;#include&lt;stack&gt;#include&lt;queue&gt;using namespace std;typedef char ElemType;//二叉树的二叉链表结构，也就是二叉树的存储结构，1个数据域，2个指针域（分别指向左右孩子）typedef struct BiTNode&#123; ElemType data; struct BiTNode *lchild, *rchild;&#125;BiTNode, *BiTree;//二叉树的建立，按前序遍历的方式建立二叉树，当然也可以以中序或后序的方式建立二叉树void CreateBiTree(BiTree *T)&#123; ElemType ch; cin &gt;&gt; ch; if (ch == '#') *T = NULL; //保证是叶结点 else &#123; *T = (BiTree)malloc(sizeof(BiTNode)); //if (!*T) //exit(OVERFLOW); //内存分配失败则退出。 (*T)-&gt;data = ch;//生成结点 CreateBiTree(&amp;(*T)-&gt;lchild);//构造左子树 CreateBiTree(&amp;(*T)-&gt;rchild);//构造右子树 &#125;&#125;//表示对遍历到的结点数据进行的处理操作,此处操作是将树结点前序遍历输出void operation1(ElemType ch)&#123; cout &lt;&lt; ch &lt;&lt; " ";&#125;//此处在输出的基础上，并输出层数void operation2(ElemType ch, int level)&#123; cout &lt;&lt; ch &lt;&lt; "在第" &lt;&lt; level &lt;&lt; "层" &lt;&lt; " ";&#125;//递归方式前序遍历二叉树void PreOrderTraverse(BiTree T, int level)&#123; if (T == NULL) return;/*此处表示对遍历的树结点进行的操作，根据你自己的要求进行操作，这里只是输出了结点的数据*/ operation1(T-&gt;data); //operation2(T-&gt;data, level); //输出了层数 PreOrderTraverse(T-&gt;lchild, level + 1); PreOrderTraverse(T-&gt;rchild, level + 1);&#125;//递归方式中序遍历二叉树void InOrderTraverse(BiTree T,int level)&#123;if(T==NULL)return;InOrderTraverse(T-&gt;lchild,level+1);operation1(T-&gt;data);//operation2(T-&gt;data, level); //输出了层数InOrderTraverse(T-&gt;rchild,level+1);&#125;//递归方式后序遍历二叉树void PostOrderTraverse(BiTree T,int level)&#123;if(T==NULL)return;PostOrderTraverse(T-&gt;lchild,level+1);PostOrderTraverse(T-&gt;rchild,level+1);operation1(T-&gt;data);//operation2(T-&gt;data, level); //输出了层数&#125;//非递归方式前序遍历/* 思路：将T入栈，遍历左子树；遍历完左子树返回时，栈顶元素应为T，出栈，再先序遍历T的右子树。*/void PreOrder(BiTree T)&#123; stack&lt;BiTree&gt; stack; //p是遍历指针 BiTree p = T; //p不为空或者栈不空时循环 while (p || !stack.empty())&#123; if (p != NULL) &#123; //存入栈中 stack.push(p); //对树中的结点进行操作 operation1(p-&gt;data); //遍历左子树 p = p-&gt;lchild; &#125; else &#123; //退栈 p = stack.top(); stack.pop(); //访问右子树 p = p-&gt;rchild; &#125; &#125; &#125;//非递归中序遍历void InOrder(BiTree T)&#123; stack&lt;BiTree&gt; stack; //p是遍历指针 BiTree p = T; //p不为空或者栈不空时循环 while (p || !stack.empty()) &#123; if (p != NULL) &#123; //存入栈中 stack.push(p); //遍历左子树 p = p-&gt;lchild; &#125; else &#123; //退栈 p = stack.top(); operation1(p-&gt;data); //对树中的结点进行操作 stack.pop(); //访问右子树 p = p-&gt;rchild; &#125; &#125; &#125;//非递归后序遍历typedef struct BiTNodePost&#123; BiTree biTree; char tag;&#125;BiTNodePost, *BiTreePost;void PostOrder(BiTree T)&#123; stack&lt;BiTreePost&gt; stack; //p是遍历指针 BiTree p = T; BiTreePost BT; //栈不空或者p不空时循环 while (p != NULL || !stack.empty()) &#123; //遍历左子树 while (p != NULL) &#123; BT = (BiTreePost)malloc(sizeof(BiTNodePost)); BT-&gt;biTree = p; //访问过左子树 BT-&gt;tag = 'L'; stack.push(BT); p = p-&gt;lchild; &#125; //左右子树访问完毕访问根节点 while (!stack.empty() &amp;&amp; (stack.top())-&gt;tag == 'R') &#123; BT = stack.top(); //退栈 stack.pop(); p=BT-&gt;biTree; cout&lt;&lt;BT-&gt;biTree-&gt;data&lt;&lt;" "; &#125; //遍历右子树 if (!stack.empty()) &#123; BT = stack.top(); //访问过右子树 BT-&gt;tag = 'R'; p = BT-&gt;biTree; p = p-&gt;rchild; &#125; &#125;&#125;//层次遍历 void LevelOrder(BiTree T)&#123; BiTree p = T; queue&lt;BiTree&gt; queue; //根节点入队 queue.push(p); //队列不空循环 while (!queue.empty()) &#123; //对头元素出队 p = queue.front(); //访问p指向的结点 operation1(p-&gt;data); //退出队列 queue.pop(); //左孩子不为空，将左孩子入队 if (p-&gt;lchild != NULL) &#123; queue.push(p-&gt;lchild); &#125; //右孩子不空，将右孩子入队 if (p-&gt;rchild != NULL) &#123; queue.push(p-&gt;rchild); &#125; &#125;&#125;int main()&#123; int level = 1; //表层数 BiTree T = NULL; cout &lt;&lt; "请以前序遍历的方式输入扩展二叉树："; //类似输入AB#D##C## CreateBiTree(&amp;T);// 建立二叉树，没有树，怎么遍历 cout &lt;&lt; "递归前序遍历输出为：" &lt;&lt; endl; PreOrderTraverse(T, level);//进行前序遍历，其中operation1()和operation2()函数表示对遍历的结点数据进行的处理操作 cout &lt;&lt; endl; cout &lt;&lt; "递归中序遍历输出为：" &lt;&lt; endl; InOrderTraverse(T, level); cout &lt;&lt; endl; cout &lt;&lt; "递归后序遍历输出为：" &lt;&lt; endl; PostOrderTraverse(T, level); cout &lt;&lt; endl; cout&lt;&lt;"非递归前序遍历输出为："&lt;&lt;endl; PreOrder(T); cout&lt;&lt;endl; cout&lt;&lt;"非递归前序遍历输出为："&lt;&lt;endl; InOrder(T); cout&lt;&lt;endl; cout&lt;&lt;"非递归前序遍历输出为："&lt;&lt;endl; PostOrder(T); cout&lt;&lt;endl; cout&lt;&lt;"层序遍历输出为："&lt;&lt;endl; LevelOrder(T); cout&lt;&lt;endl; return 0;&#125; 运行结果只是运行了operation1( )函数，输出值： 补充：另外一种实现二叉树后序遍历的非递归算法。 12345678910111213141516171819202122232425void PostOrder(BiTree T)&#123; InitStack(s); p=T; r=NULL; //辅助判断右子树是否被访问过 while(p||!IsEmpty(s))&#123; if(p)&#123; //走到最左边 push(s,p); p=p-&gt;lchild; &#125; else&#123; //向右 GetTop(s,p); //取栈顶结点 if(p-&gt;rchild&amp;&amp;p-&gt;rchild!=r)&#123; //如果右子树存在且未被访问过 p=p-&gt;rchild; //转向右 push(s,p); //压入栈 p=p-&gt;lchild; //再转向左 &#125; else&#123; //否则，弹出结点并访问 pop(s,p); //结点出栈 visit(p-&gt;data); //访问该结点 r=p; //记录最近访问过的结点 p=NULL; //结点访问完后，重置p指针 &#125; &#125; &#125;&#125; 重要结论： 二叉树的前序和中序序列能唯一确定一棵二叉树 二叉树的后序和中序序列能唯一确定一棵二叉树 二叉树的层次和中序序列能唯一确定一棵二叉树 参考资料c实现树（二叉树）的建立和遍历算法（一）（前序，中序，后序）树（二叉树）的建立和遍历算法（二）]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构之树（一）]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-4-6-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%A0%91%2F</url>
    <content type="text"><![CDATA[本部分主要介绍树的相关知识，将分为3篇博文介绍。本文将着重介绍二叉树的一些基本概念，以及在其基础上的一些特殊的树形式：满二叉树、完全二叉树、线索二叉树、二叉排序树、平衡二叉树等。 树的基本概念树的定义树是$N（N \ge 0）​$个结点的有限集合，$N=0$时，称为空树。$N$不为0时，树满足： 有且仅有一个特定的称为根结点的结点 当$N \ge 1$时，其余的结点可以分为$m（m&gt;0）$个互不相交的有限集合$T_1,T_2,…,T_m$，其中每一个集合则是根结点的子树。 树是一种递归的数据结构，具有以下2个特点： 树的根结点没有前驱结点，除根结点之外的所有结点有且只有一个前驱结点 树中所有的结点可以有零个或者多个后继结点 $n$个结点的树中只有$n-1$条边 基本术语 结点的度：一个结点的子结点的个数 树的度：树中结点的最大度数 叶子结点：度为$0$的结点 结点深度：自顶向下逐层累加；结点高度：自底向上逐层累加 树的高度：树中结点的最大层数 路径：两结点之间所经过的结点序列构成；路径长度：路径上所有经过边的个数 森林：$m(m \ge 0)$棵不相交的树的集合。 把树的根结点删去就成了森林。 树的性质 树中的结点数等于所有结点的度数加1 度为$m$的树中第$i$层上至多有$m^{i-1}$个结点，$(i \ge 1)​$ 高度为$h$的$m$叉树至多有$\frac{m^h-1}{m-1}$个结点 具有$n$个结点的$m$叉树最小高度为$\lceil log_m^{(n(m-1)+1)} \rceil$ 二叉树定义根结点的编号为1 二叉树的定义与树类似，二叉树也以递归的形式定义。二叉树是$n(n \ge 0)$个结点的有限集合： 或者为空二叉树，即$n=0$ 或者由一个根结点和两个互不相交的被称为根的左子树和右子树组成。左子树和右子树又分别是一棵二叉树。 二叉树中每个结点至多有2棵子树，其结点次序是确定的，不是相对另一结点而言。 满二叉树一棵高度为$h$，并且含有$2^h-1$个结点的二叉树称为满二叉树。(定义根结点的编号为1)起，对于编号为$i$的结点： 若有双亲为$\lfloor i/2 \rfloor$ 若有孩子结点，左孩子为$2i$，右孩子为$2i+1$ 完全二叉树高为$h$，有$n$个结点的二叉树，当且仅当每一个结点都与高为$h$的满二叉树中编号为$1到n​$的结点一一对应时，称为完全二叉树。 完全二叉树的性质 若$i \le \lfloor \frac{n}{2} \rfloor$，结点$i$为分支结点，否则为叶子结点 如果有高度为1的结点，只可能有1个，且该结点只有左孩子 一旦出现某一结点$i$为叶子结点或者只有左孩子，则编号大于$i$的结点均为叶子结点 若$n$为奇数，每个分支结点都有左右孩子；若$n$为偶数，编号最大的分支结点$\frac{n}{2}$只有左孩子 下面是一张满二叉树和完全二叉树的对比图： 二叉排序树一棵二叉树： 或者空树 或者左子树所有结点的值均小于根结点的值，右子树所有结点的值均大于根结点的值，左子树和右子树又各是一棵二叉排序树 平衡二叉树树上任一结点的左子树和右子树的深度之差不超过1。 满二叉树 —–&gt; 完全二叉树 —–&gt; 平衡二叉树 二叉树的性质 非空二叉树的叶子结点（度为0）数等于度为2的结点数加1，即$N_0 = N_2 +1$ 非空二叉树第$k$层至多有$2^k-1$个结点 结点$i​$所在的层次深度为$\lfloor log_2^i \rfloor +1 ​$ 具有$N$个$(N&gt;0)$结点的完全二叉树的高度为$\lfloor log_2^N \rfloor +1$或$\lceil log_2^{(N+1)} \rceil$ 二叉树的存储结构顺序存储结构二叉树的顺序存储结构就是用一组连续地址的存储单元依次自上而下、自左至右存储完全二叉树上的结点元素，即将完全二叉树上编号为$i$的结点元素存储在某个数组下标为$i-1$的分量中。（数组下标要从1开始） 依据二叉树的性质，完全二叉树和满二叉树采用顺序存储比较合适，因为树中结点的序号可以唯一的反映出结点之间的逻辑关系，这样既能最大的节省存储空间，又可以利用数组元素下标确定结点在二叉树中的位置，以及结点之间的关系。 如果对于一般的二叉树，也采用顺序存储的话，只能添加一些并不存在的空结点，这样才能满足我们上面所讲的逻辑关系。 如下图所示，其中$0$表示并不存在的空结点。 链式存储结构由于顺序存储对空间利用效率低，因此一般二叉树都采用链式存储结构。链式结构是指用一个链表来存储一棵二叉树，二叉树中的每一个结点用链表的一个链结点来存储。在二叉树中，结点结构通常包括若干数据域及若干个指针域。二叉链表至少包含3个域：数据域data、左指针域lchild和右指针域rchild。如下图所示： 二叉树的链式存储结构描述如下：1234typedef struct BiTNode&#123; ElemType data; //数据域 struct BiTNode *lchild,*rchild; //左右孩子指针&#125;BiTNode,*BiTree; 在含有$n$个结点的二叉链表中含有$n+1$个空链域。如上图中含有5个结点的二叉链表有6个空链域。]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言实现八大排序算法（二）]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-4-5-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%8E%92%E5%BA%8F2%2F</url>
    <content type="text"><![CDATA[在这篇博文中，我们介绍了剩下4种排序算法，并将对所有的排序算法做一个总结。代码主要参见数据机构之十大排序，关于拓展里面的桶排序和计数排序，只了解了想法，并未自己实现，不过也给出了参考资料中别人的代码实现。 选择排序（Selection Sort）基本思想在要排序的一组数中，选出最小（或者最大）的一个数与第$1$个位置的数交换；然后在剩下的数当中再找最小（或者最大）的与第$2$个位置的数交换，依次类推，直到第$n-1$个元素（倒数第二个数）和第$n$个元素（最后一个数）比较为止。 算法流程 初始时，数组全为无序区$a[0, … , n-1]$, 令$i=0$; 在无序区$a[i, … , n-1]$中选取一个最小的元素与$a[i]$交换，交换之后$a[0, … , i]$即为有序区； 重复步骤2，直到$i=n-1$，排序完成。 复杂度分析：时间复杂度为$O(n^2)$，空间复杂度为$O(1)$，不稳定 动态实例参见: https://img-blog.csdnimg.cn/20181108191520166.gif 算法实现12345678910111213141516171819//选择排序,以升序为例void SelectSort(int a[],int n)&#123; int i,j,min; for(i=0;i&lt;n-1;i++)&#123; min=i; for(j=i+1;j&lt;n;j++)&#123; if(a[j]&lt;a[min) min=j; &#125; if(min!=i) Swap(a[i],a[min]); &#125;&#125;void Swap(int a,int b)&#123; int temp; temp=a; a=b; b=temp;&#125; 堆排序（Heap Sort）堆排序（HeapSort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。 小根堆：$L(i) \le L(2i)$且$L(i) \le L(2i+1)$ 大根堆：$L(i) \ge L(2i)$且$L(i) \ge L(2i+1)$ 其中（$1 \le i \le \lfloor n/2 \rfloor$） 堆排序常用于解决top-k问题。 基本思想（以大根堆为例） 初始化堆： 将数列$a[1,…,n]$构造成大顶堆（即根结点最大，左右结点均小于根结点）; 交换数据： 将$a[1]$(根结点)和$a[n]$(最后一个数据，不一定是最小值)互换，使得$a[n]$是数列中的最大值; 然后再将$a[1,…,n-1]$构造大顶堆，交换$a[1]和a[n-1]$; 再将$a[1,…,n-2]​$构造成大顶堆，……，直到剩下一个元素，就是序列的最小值; 此时整个序列就按从小到大排列好了。 这里的大顶堆是以数组形式存储的，按层（行）存储，第一个元素是顶堆，第2个元素是左子树第一个分支结点，第3个元素是右子树第一个分支结点，第4个元素是第3层左边第一个结点……，有如下性质（根结点的索引是0）： 性质一：索引为$i$的左孩子的索引是 $(2*i+1)$; 性质二：索引为$i$的右孩子的索引是 $(2*i+2)​$; 复杂度分析：时间复杂度为$O(nlog_2^n)$，空间复杂度为$O(1)​$，不稳定 一个大根堆的建立过程如下： 动态实例参见（大根堆）: https://img-blog.csdnimg.cn/20181108192721466.gif 算法实现要想实现堆排序，必须考虑2个方面： 如何建立大根堆？ 大根堆建立好后，第一次输出堆顶元素后，堆被破坏，如何调整建立新的大根堆？ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107//堆排序方法一，主要包含2部分:创建堆；输出堆顶元素后重新调整堆//交换函数void Swap(int array, int i, int j)&#123; int tmp; tmp = array[j]; array[j] = array[i]; array[i] = tmp;&#125;//创建大根堆void MaxHeapCreat(int array[], int heapSize)&#123; int i; for(i = heapSize/2; i &gt;= 0; i--) &#123; MaxHeapify(array, heapSize, i); &#125;/*大根堆调整*/void MaxHeapify(int array[], int heapSize, int currentNode)&#123; int leftChild, rightChild, largest; leftChild = 2*currentNode + 1; rightChild = 2*currentNode + 2; largest = currentNode; if(leftChild &lt; heapSize &amp;&amp; array[leftChild] &gt; array[currentNode]) largest = leftChild; if(rightChild &lt; heapSize &amp;&amp; array[rightChild] &gt; array[largest]) largest = rightChild; if(largest != currentNode) &#123; Swap(array, largest, currentNode); MaxHeapify(array, heapSize, largest); &#125;&#125;//堆排序void HeapSort(int array[],int heapSize)&#123; MaxHeapCreat(array,heapSize); //直接输出 for(i = 0; i &lt; heapSize; i++) &#123; printf("%d\t", array[i]); &#125;&#125; ------------------------------------------------------------------------------------------- //堆排序方法二#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt; //数据交换void Swap(int *a, int* b)&#123; int temp = *a; *a = *b; *b = temp;&#125; //构造大顶堆void heap_down(int* arr,int i, int N)&#123; int child; int temp; for(temp=arr[i]; 2*i+1&lt;N;i=child)&#123; //temp是当前结点的值 child = 2*i+1; //当前结点i的左孩子的位置（在数组中的下标） if(child!=N-1 &amp;&amp; arr[child+1]&gt;arr[child])&#123; //child+1是当前结点的右孩子的位置，判断右孩子是否大于左孩子 child++; &#125; if(temp&lt;arr[child])&#123; //判断根结点是否小于它的左右两个跟结点，如果小于，则交换大的为根结点 arr[i]=arr[child]; arr[child]=temp; &#125; else&#123; break; &#125; &#125; &#125; //堆排序void heap_Sort(int *arr, int length)&#123; int i; //从length/2到0依次遍历，最终得到的数组是一个大顶堆 //最后叶子结点2×i+2=length,i=length/2-1是最后一个父节点 for(i=length/2;i&gt;=0;i--)&#123; heap_down(arr, i, length); &#125; for(i=length-1;i&gt;0;i--)&#123; Swap(&amp;arr[0],&amp;arr[i]); //交换最大值a[0]到队列末尾 heap_down(arr,0,i); //执行去掉最大值的[0~n-1]个元素的大顶堆， &#125;&#125; //打印元素void printArr(int *arr,int length)&#123; for(int i=0;i&lt;length;i++)&#123; printf("%d\n",arr[i]); &#125;&#125; int main()&#123; int arr[]=&#123;3,5,2,10,8,9,6,4,7,19,5,43,56,3,24,98,76,123,456,76,432,987,12&#125;; int length = sizeof(arr)/sizeof(int); heap_Sort(arr,length); printArr(arr,length); return 0;&#125; 小根堆和大根堆类似，此处就不细说了。 归并排序（Merge Sort）归并概念： 将两个的有序数列合并成一个有序数列，我们称之为”归并”（二路归并）。 基本思想归并（Merge）排序算法采用分治策略，将两个（或两个以上）有序表合并成一个新的有序表，即把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。 复杂度分析：时间复杂度为$O(nlog_2^n)$，空间复杂度为$O(n)​$，稳定。 动态实例参见: https://img-blog.csdnimg.cn/2018110819232047.gif 算法实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104//归并排序思想ElemType *b = (ElemType *) malloc ((n+1)*sizeof(ElemType)); //带排序表有n个记录void Merge(ElemType a[],int low,int mid,int high)&#123; //表a中两段a[low,...,mid]和a[mid+1,...,high]各自有序，将其合并成一个有序表 for(int k=low;k&lt;=high;k++) b[k]=a[k]; //将a中元素复制到辅助数组b中 for(i=low,j=mid+1,k=i;i&lt;=mid&amp;&amp;j&lt;=high;k++)&#123; if(b[i]&lt;=b[j]) //比较b的左右两段中的元素 a[k]=b[i++]; //将较小值复制到a中 else a[k]=b[j++]; &#125; while(i&lt;=mid) //若左半部分表未检测完，复制 a[k++]=b[i++]; while(j&lt;=high) //若右半部分表未检测完，复制 a[k++]=b[j++];&#125;void MergeSort(ElemType a[],int low,int high)&#123; if(low&lt;high)&#123; int mid=(low+high)/2; //从中间划分2个子序列 MergeSort(a,low,mid); //对左侧子序列进行递归排序 MergeSort(a,mid+1,high); //对右侧子序列进行递归排序 Merge(a,low,mid,high); //归并 &#125;&#125;-------------------------------------------------------------------------------------------//递归排序的完整实现#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;string.h&gt; //合并两个有序序列（即归并）// A： 待合并的序列（含两个子序列，排在一起）// Temp： 辅助空间// L: 左边序列起点下标// R: 右边序列起点下标，// RightEnd： 右边序列终点下标void Merge(int A[], int Temp[], int L, int R, int RightEnd)&#123; int LeftEnd = R-1; int p=L,i; int num=RightEnd-L+1; //先合并最短序列的长度的个数个元素 while(L&lt;=LeftEnd&amp;&amp;R&lt;=RightEnd)&#123; if(A[L]&lt;=A[R]) Temp[p++]=A[L++]; else Temp[p++]=A[R++]; &#125; //判断如果是左侧序列还有剩余 while(L&lt;=LeftEnd) Temp[p++]=A[L++]; //判断如果是右侧序列还有剩余 while(R&lt;=RightEnd) Temp[p++]=A[R++]; // 将辅助空间中的值拷贝到原列表中，完成排序 for(i=0;i&lt;num;i++,RightEnd--) A[RightEnd]=Temp[RightEnd];&#125; //递归拆分，递归归并void m_sort(int* arr, int* temp, int L, int right_end)&#123; int center; if(L&lt;right_end)&#123; center = (L+right_end)/2; m_sort(arr,temp,L,center); m_sort(arr,temp,center+1,right_end); Merge(arr,temp,L,center+1,right_end); &#125;&#125; //归并排序void merge_Sort(int* arr,int length)&#123; int *temp=(int* )malloc(length*sizeof(int)); //申请辅助空间 if(temp==NULL)&#123; return; &#125; m_sort(arr,temp,0,length-1); free(temp); temp=NULL;&#125; //打印元素void printArr(int *arr,int length)&#123; for(int i=0;i&lt;length;i++)&#123; printf("%d\n",arr[i]); &#125;&#125; int main()&#123; int arr[10]=&#123;3,5,2,10,8,9,6,4,7,1&#125;; int length = sizeof(arr)/sizeof(int); length = 10; merge_Sort(arr,length); printArr(arr,length); return 0;&#125; 基数排序（Radix Sort）基数排序过程无须比较关键字，而是通过“分配”和“收集”过程来实现排序。 基本思想将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后，数列就变成一个有序序列。 基数排序按照优先从高位或低位来排序有两种实现方案： MSD（Most significant digital） 从最左侧高位开始进行排序。先按k1排序分组, 同一组中记录, 关键码k1相等,再对各组按k2排序分成子组, 之后, 对后面的关键码继续这样的排序分组, 直到按最次位关键码kd对各子组排序后. 再将各组连接起来,便得到一个有序序列。MSD方式适用于位数多的序列。 LSD （Least significant digital）从最右侧低位开始进行排序。先从kd开始排序，再对kd-1进行排序，依次重复，直到对k1排序后便得到一个有序序列。LSD方式适用于位数少的序列。 复杂度分析 : 时间复杂度为$O(d(n+r))$，空间复杂度为$O(r)$，$d$为位数，$r$为基数，稳定。 动态实例参见：https://img-blog.csdnimg.cn/20181108193151498.gif 算法实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;string.h&gt; //求数字位数int bit_num(int num)&#123; if(num/10==0)&#123; return 1; &#125; return 1+bit_num(num/10);&#125; //求序列最大值int max_list(int *arr, int length)&#123; int max_num = arr[0]; for(int i=1;i&lt;length;i++)&#123; if(arr[i]&gt;max_num)&#123; max_num = arr[i]; &#125; &#125; return max_num;&#125; //找到一个num从低位到高位的第pos位的数据，数据最右侧最低位pos=1int get_num_pos(int num, int pos)&#123; if(pos&lt;=0)&#123;return -1;&#125;; int pow_num = 1; for(int i=0;i&lt;pos-1;i++)&#123;pow_num*=10;&#125; return (num/pow_num)%10; &#125; //基数排序void base_Sort(int* arr, int length)&#123; int max_num, key_num; int *base_arr[10]; //十进制的10个桶 max_num = max_list(arr,length); key_num = bit_num(max_num); for(int i=0; i&lt;10;i++)&#123; base_arr[i]=(int *)malloc(sizeof(int)*(length+1)); base_arr[i][0] = 0; //桶中第一个位置记录桶中元素的数量 &#125; for(int pos = 1; pos&lt;= key_num;pos++)&#123; //需要执行最大位数次 for(int i=0;i&lt;length;i++)&#123; int num = get_num_pos(arr[i],pos); int index = ++base_arr[num][0]; base_arr[num][index]=arr[i]; &#125; for(int i=0,j=0;i&lt;10;i++)&#123; for(int k=1; k&lt;=base_arr[i][0];k++)&#123; arr[j++] = base_arr[i][k]; &#125; base_arr[i][0]=0; &#125; &#125; &#125; //打印元素void printArr(int *arr,int length)&#123; for(int i=0;i&lt;length;i++)&#123; printf("%d\n",arr[i]); &#125;&#125; int main()&#123; int arr[] =&#123;3,5,7,2,1,0,4,65,7,89,5,3,2,5,45,3,2,54,4,543,3,33,2,34,45,5&#125;; int length = sizeof(arr)/sizeof(int); base_Sort(arr,length); printArr(arr,length); return 0;&#125; 拓展桶排序（Bucket Sort）基本思想是将一个数据表分割成许多buckets，然后每个bucket各自排序，或用不同的排序算法，或者递归的使用bucket sort算法。也是典型的divide-and-conquer分而治之的策略。它是一个分布式的排序，当要被排序的数组内的数值是均匀分配的时候，桶排序时间复杂度是O(n)，桶排序并不是 比较排序，他不受到 O(n log n) 下限的影响,稳定。 算法流程 建立一定数量的数组当作空桶； 遍历原始数组，并将数据放入到对应的桶中； 对非空的桶进行排序； 按照顺序遍历这些非空的桶并放回到原始数组中即可构成排序后的数组。 桶排序关键要确定当前数据和桶的映射关系函数，即一个数据应该放到哪个桶里边。 举个列子例如要对大小为$[1..1000]$范围内的$n$个整数$A[1..n]$排序 首先，可以把桶设为大小为10的范围，具体而言，设集合$B[1]$存储$[1..10]$的整数，集合$B[2]$存储$(10..20]$的整数，…… ，集合$B[i]$存储$((i-1)10, i10]$的整数，$i=1,2,..100$，总共有100个桶。 然后，对A$[1, … , n]$从头到尾扫描一遍，把每个$A[i]$放入对应的桶$B[j]$中。 再对这100个桶中每个桶里的数字排序，这时可用冒泡，选择，乃至快排，一般来说任何排序法都可以。 最后，依次输出每个桶里面的数字，且每个桶中的数字从小到大输出，这样就得到所有数字排好序的一个序列了。 算法实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;string.h&gt; //桶排序void bucket_Sort(int* arr, int length)&#123; int i,j,max_num=arr[0]; int* bucket; //先求出序列最大值 for(i=1;i&lt;length;i++)&#123; if(arr[i]&gt;max_num)&#123; max_num=arr[i]; &#125; &#125; max_num++; //最大值加1 if(arr==NULL || length&lt;=1)&#123; return; &#125; if((bucket = (int*)malloc(sizeof(int)*max_num))==NULL)&#123;return;&#125; for(i=0;i&lt;max_num;i++)&#123; bucket[i]=0; //空桶数组初始化 &#125; for(i=0;i&lt;length;i++)&#123; // 寻访序列，把元素一个一个放入对应的桶里 bucket[arr[i]]+=1; &#125; for(i=0,j=0;i&lt;max_num;i++)&#123; while((bucket[i])&gt;0)&#123; //对每个不是空的桶子进行排序 arr[j]=i; //从不是空的桶子里把项目再放回原来的序列中 j++; bucket[i]--; &#125; &#125; free(bucket); bucket = NULL; &#125; //打印元素void printArr(int *arr,int length)&#123; for(int i=0;i&lt;length;i++)&#123; printf("%d\n",arr[i]); &#125;&#125; int main()&#123; int arr[] =&#123;3,5,7,2,1,0,4,65,7,89,5,3,2,5,45,3,2,54,4,543,3,33,2,34,45,5&#125;; int length = sizeof(arr)/sizeof(int); bucket_Sort(arr,length); //printf("%d\n\n",length); printArr(arr,length); return 0;&#125; 计数排序（Count Sort）计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 基本思想对于给定的输入序列中的每一个元素x，确定该序列中值小于x的元素的个数（此处并非比较各元素的大小，而是通过对元素值的计数和计数值的累加来确定）。一旦有了这个信息，就可以将x直接存放到最终的输出序列的正确位置上。 例如，如果输入序列中只有17个元素的值小于x的值，则x可以直接存放在输出序列的第18个位置上。当然，如果有多个元素具有相同的值时，我们不能将这些元素放在输出序列的同一个位置上，有重复时需要特殊处理（保证稳定性），需要在最后反向填充目标数组，并将每个数字的统计减去1。复杂度分析计数排序是一个稳定的排序算法。当输入的元素是 n 个 0到 k 之间的整数时，时间复杂度是$O(n+k)$，空间复杂度也是$O(n+k)$，其排序速度快于任何比较排序算法。 当序列的最大值是M时，需要辅助空间的长度是M，所以不太适合数据量很大，或者最大值很大，或者数据分布很离散的场合下。当k不是很大并且序列比较集中时，计数排序是一个很有效的排序算法。 动态实例参见：https://img-blog.csdnimg.cn/20181108192837268.gif 算法实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include&lt;stdio.h&gt;#include&lt;stdlib.h&gt;#include&lt;string.h&gt; //计数排序 length: 带排序元素个数 max_num: 待排序元素中最大值void counting_Sort(int* arr, int length)&#123; int *c, *b; int i, min_num,max_num,range; min_num=max_num=arr[0]; //先求出序列最大最小值 for(i=1;i&lt;length;i++)&#123; if(arr[i]&gt;max_num)&#123; max_num=arr[i]; &#125; if(arr[i]&lt;min_num)&#123; min_num = arr[i]; &#125; &#125; range = max_num - min_num +1; c = (int*)malloc(sizeof(int)*range); //辅助排序数组，长度是元素的最大值-最小值+1 b = (int*)malloc(sizeof(int)*length); if(c==NULL || b == NULL)&#123;return;&#125; for(i=0;i&lt;range;i++)&#123; c[i]=0; //辅助数组初始化 &#125; for(i=0;i&lt;length;i++)&#123; c[arr[i]-min_num]+=1; //统计数组arr中每个值为i的元素出现次数 &#125; for(i =1; i&lt;range;i++)&#123; c[i]=c[i-1]+c[i]; //确定值为i的元素在数组c中出现的位置 &#125; for(i=length-1;i&gt;=0;i--)&#123; //对原序列arr中的每一个元素，从后向前依次确定每个元素所在的最终位置， //先放入辅助数组b中（再拷回原始arr中） c[arr[i]]-=1; b[c[arr[i]-min_num]]=arr[i]; &#125; for(i = 0; i&lt;length;i++)&#123; arr[i]=b[i]; //拷回原始arr中 &#125; free(c); c = NULL; free(b); b = NULL;&#125; //打印元素void printArr(int *arr,int length)&#123; for(int i=0;i&lt;length;i++)&#123; printf("%d\n",arr[i]); &#125;&#125; int main()&#123; int arr[] =&#123;3,5,7,2,1,0,4,65,7,89,5,3,2,5,45,3,2,54,4,543,3,33,2,34,45,5&#125;; int length = sizeof(arr)/sizeof(int); counting_Sort(arr,length); //printf("%d\n\n",length); printArr(arr,length); return 0;&#125; 计数排序、桶排序、基数排序区别 基数排序和计数排序都可以看作是一种特殊的桶排序; 计数排序是按照元素出现的次数划分桶，桶排序是按值区间划分桶，基数排序是按数位来划分桶。 排序算法总结 任何借助“比较”的排序算法，至少需要$O(nlog_2^n)​$空间 记录本身信息量较大时，用链表作为存储结构 排序趟数与原始状态无关：直接插入、简单选择、基数 排序中比较次数的数量级与序列初始状态无关：简单选择、归并 时间复杂度 平方阶$(O(n^2))$排序：直接插入、直接选择和冒泡排序； 线性对数阶$(O(nlogn))$排序：快速排序、堆排序和归并排序； $O(n1+§))$排序,$§$是介于0和1之间的常数：希尔排序 线性阶$(O(n))$排序：基数排序、计数排序、桶排序等。 稳定性 稳定的排序算法：冒泡排序、插入排序、归并排序和基数排序。 不稳定的排序算法：选择排序、快速排序、希尔排序、堆排序。 选择排序算法准则 待排序的记录数目n的大小； 记录本身数据量的大小，也就是记录中除关键字外的其他信息量的大小； 关键字的结构及其分布情况； 对排序稳定性的要求。 针对n的大小选择不同排序算法 当n较大，则应采用时间复杂度为$O(n*logn)$的排序方法：快速排序、堆排序或归并排序。 快速排序：是目前基于比较的内部排序中被认为是最好的方法，当待排序的关键字是随机分布时，快速排序的平均时间最短；堆排序：如果内存空间允许且要求稳定性的；归并排序：它有一定数量的数据移动，所以我们可能过与插入排序组合，先获得一定长度的序列，然后再合并，在效率上将有所提高。 当n较大，内存空间允许，且要求稳定性：归并排序 当n较小，可采用直接插入或直接选择排序。 直接插入排序：当元素分布有序，直接插入排序将大大减少比较次数和移动记录的次数。直接选择排序：当元素分布有序，如果不要求稳定性，选择直接选择排序。 一般不使用或不直接使用传统的冒泡排序。 基数排序它是一种稳定的排序算法，但有一定的局限性： 1、关键字可分解；2、记录的关键字位数较少，如果密集更好；3、如果是数字时，最好是无符号的，否则将增加相应的映射复杂度，可先将其正负分开排序。 参考资料八大排序算法C 数据结构之十大排序排序算法总结]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于深度学习框架的水声信号的扩充和分类识别]]></title>
    <url>%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%2F2019-4-24-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E7%9A%84%E6%B0%B4%E5%A3%B0%E4%BF%A1%E5%8F%B7%E7%9A%84%E6%89%A9%E5%85%85%E5%92%8C%E5%88%86%E7%B1%BB%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[现如今，基于声音信号的海上目标识别是进行海量探测和目标识别的可靠方法，也是水声信号处理领域的重要研究内容。 项目简介数据集原始数据包含15类的水下音频数据，每类里面包含长度不等的一段音频数据。 目的训练一个模型，对这15类目标进行识别分类。 思考方法 最开始尝试直接用语音识别的模型（LSTM）对其分类，后来发现效果很差，考虑到可能是水下环境复杂，原始音频数据包含大量噪音。 于是开始考虑其他方法。最后通过查阅资料发现，在水声领域，通常可以将音频信号数据转换为谱图进行处理，于是尝试将原始音频数据先转换为谱图，再用CNN对其进行分类识别。 CNN的方法相比于LSTM，在准确率上有所提升，但还是不够好。又考虑到原始数据由于安全保密或者是收集困难等原因，数据集数量不多，而大量的训练数据集是保证深度学习方法性能的关键。遂最终又引入了GAN网络。先对转换后的频谱做了扩充，然后再用CNN网络对其识别分类，准确率相较于仅用CNN，又有了不错的提升。 项目实现过程整个实现流程如下图所示 （a）是 .wav格式的原始样本 （b）是通过频谱进行的数据预处理和基于CNN的频谱选择，从而确保样本可以被神经网络识别。 （c）由GAN进行数据集扩展，额外的CNN是为了确保生成频谱的质量 （d）对网络进行评估并选择最佳网络。 数据预处理在水声领域，常见的频谱图有很多，例如Lofar、Audio、Demon、Histogram等，我们尝试将原始音频转换为了各种谱图存储，每种谱图都包含15类。部分转换后的频谱如下图所示： 为了找到最好的频谱图，我们直接选择了AlexNet网络，针对每种转换好的频谱，为其划分好训练集和测试集，各训练了一个分类模型。通过比较在测试集上的准确率，我们发现Lofar频谱的识别率最高，于是选定它为原始音频转换后的频谱。 Lofar频谱的转换 将原始音频转换为Lofar频谱，主要是通过STFT(快速傅里叶变换)实现的。 对于每一类的音频，对其做STFT后，先找到具有特征的频率段， 然后将数据根据能量差值转换为对应的灰度值， 每隔1s选取一段音频，重复步骤1和2，最终每类音频得到1000张灰度的Lofar谱图 CNN网络得到每类的Lofar频谱后，我们设计了一个CNN网络来训练分类模型。网络结构图和参数设置如下所示：网络模型 参数设置 最终测试集在上述分类模型中取得了75.7%的识别率。 GAN网络在引入GAN网络前，我们也尝试了原始的数据增广的方法，例如旋转、反转、亮度增强、添加噪音等，如下图所示，但是这些方法生成的数据都有很大的局限性，即图片缺乏多样性，训练出来的分类模型准确率基本没有提升。 原始GAN为了进一步提升识别准确率，我们引入了GAN生成式对抗网络。生成式对抗网络主要由2部分组成： 生成模型G：一个二分类器，估计一个样本来自训练数据（真实）的概率。若样本来自真实数据，输出大概率值，否则输出小概率值。 判别模型D：捕捉生成数据的分布。用服从某一分布（均匀分布、高斯分布）的噪音向量$z$去生成一个类似真实训练数据的分布。 G要尽量最小化D的输出值，D要最大化输出值，两者相互对抗。所以原始GAN模型函数可以表示如下：$$\min \limits_{G}\max\limits_{D}GAN(D,G)=E_{x\sim P_{data}(x)}[logD(x)]+E_{z\sim p_z(z)}[log(1-D(G(z)))]$$ GAN模型示意图 如果1用于表示实际样本，则0是假样本。 对于来自实际样本的数据，D应尽可能地输出概率值1。 对于来自世代的假样本，D应该尽量输出0的概率值。它们相互竞争并最终达到一定的稳定状态：即G的分布尽可能接近实际样本的分布。 CGAN在本项目中，我们的数据共有15类，如果使用原始的GAN网络，我们需要针对每一个类别的Lofar谱图训练一个生成模型，这样太过麻烦。最终我们选择了CGAN网络。 CGAN网络将原始GAN网络从无监督变为有监督。生成器和判别器都增加了额外信息y为条件，y可以是任意信息，例如类别信息（标签）或者是其他模态的数据。相比于原始GAN，CGAN的模型函数可以表示如下（只是多了标签数据y）：$$\min \limits_{G}\max\limits_{D}GAN(D,G)=E_{x\sim P_{data}(x|y)}[logD(x)]+E_{z\sim p_z(z)}[log(1-D(G(z|y)))]$$ CGAN模型示意图 生成式模型G中，先验输入噪音$P(z)$和条件信息$y$联合组成了联合隐层表征。 CGAN网络设计网络模型 参数设置 我们输入15种类型的LOFAR频谱，每种频谱包含1000个样本到设计的CGAN网络中，并且一些超参设置如下：batch_size=64，learning_rate=0.0001，epoches=40。 训练后，我们的方法可以利用G模型不仅可以生成某种特定的光谱样本，而且还能生成各种光谱样本。 在下图中展示了一些原始和生成的样本，其中（a）和（b）对应于单个类，而（c）和（d）对应于各种类。 相比之下，我们发现由我们的网络生成的数据包含共同的特征线，并且在某些情况下，所有这些特征线都清晰地显示并且具有比原始训练样本更突出的特征线。 除此之外，我们也对原始样本和该图中生成的样本之间进行了更深入和更详细的比较。最后，我们可以发现生成的样本具有丰富的多样性，其中包含与（a）中的原始样本相似的样本，以及包括具有一定噪声的样本，并且所有上述样本都在（b）中示出。 （b）中的顶部样本与原始样本类似，底部包含一些噪声，但都具有明显的光谱特征。 使用生成的样本作为训练数据可以极大地改善训练样本的多样性，从而在一定程度上避免过度拟合问题。 图像质量验证上面只是通过观察得到的结果，为了进一步验证我们的网络生成的样本是高质量的，我们使用G模型为每个类生成3000个样本，然后将它们与原始样本混合并进入AlexNet分类网络进行训练。主要进行了以下实验： 我们的方法使用80％的原始样本进行训练，并验证剩余的20％。 同时，我们还应用上述模型对15种类型的生成样本进行分类和识别。 此外，我们将原始样本和生成样本混合，其中80％用于训练，其余20％用于验证。 实验结果如下所示： 上述实验表明混合数据所获得的性能比仅用原始数据集训练的性能高出15％以上。这证明了我们的CGAN网络可以生成高质量的lofar谱，从而解决了水声信号领域数据样本不足的问题。生成的谱图可以用于提高水声信号分类和识别任务的准确性和稳定性。 CNN网络选取确定了生成的样本具有较高的质量后，我们用G模型为每类Lofar谱图生成了3000个样本，除了将其输入到我们设计的CNN网络之外，我们也和其他的一些CNN网络（LeNet、AlexNet、VGG16）做了对比，最终我们的设计的网络模型识别率较高。 拓展如果希望再进一步提升识别率，可以从以下2方面做出改进： 其他的GAN模型。CGAN只是使用了真实图片的类别信息，可以考虑其他的衍生GAN模型，例如InfoGAN WGAN WGAN-GP BEGAN……，也许能生成质量更高的样本 去噪。水下环境复杂，原始的音频数据包含较多的噪音。可以考虑先对原始数据进行去噪，得到高质量数据后，再转换为谱图用CNN分类 当然，也可以把上述2点结合，先去噪，再尝试其他的GAN模型，也许能得到更好的结果。]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>CNN</tag>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言实现八大排序算法（一）]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-4-5-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[本文主要介绍数据结构中常见的八大排序算法，冒泡排序、快速排序、直接插入排序、希尔排序、简单选择排序、堆排序、归并排序和基数排序。 排序相描述 排序分类：若排序过程中，所有的文件都是放在内存中处理的，不涉及数据的内外存交换，则称该排序算法是内部排序算法; 若排序过程中涉及内外存交换，则是外部排序。内部排序适合小文间，外部排序适用于不能一次性把所有记录放入内存的大文件。常见的分类算法还可以根据排序方式分为两大类：比较排序和非比较排序。 排序算法的稳定性： 若排序对象中存在多个关键字相同的记录，经过排序后，相同关键字的记录之间的相对次序保持不变，则该排序方法是稳定的，若次序发生变化(哪怕只有两条记录之间)，则该排序方法是不稳定的。关于哪些算法是稳定的，哪些不稳定，下面会详细介绍。 时间复杂度：一般情况下，算法中基本操作重复执行的次数是问题规模$n$的某个函数，用$T(n)$表示，若有某个辅助函数$f(n)$，使得$T(n)/f(n)$的极限值（当$n$趋近于无穷大时）为不等于零的常数，则称$f(n)$是$T(n)$的同数量级函数。记作$T(n)=O(f(n))$，称$O(f(n)) ​$为算法的渐进时间复杂度，简称时间复杂度。 空间复杂度：空间复杂度(Space Complexity)是对一个算法在运行过程中临时占用存储空间大小的量度，它是问题规模$n$的函数，记做$S(n)=O(f(n))$。比如直接插入排序的时间复杂度是$O(n^2)$,空间复杂度是$O(1) $。而一般的递归算法就要有$O(n)$的空间复杂度了，因为每次递归都要存储返回信息，需要辅助空间的大小随着$n$的增大线性增大。 就地排序：若一个排序算法所需的辅助空间并不依赖于问题的规模n，即时间复杂度是$O(1)$,则称该排序算法为就地排序。非就地排序算法的时间复杂度一般为$O(n)$。 注：算法是否稳定并不能衡量一个算法的优劣，排序算法主要包含2种操作：比较和移动，但并不是所有的排序都基于比较操作，比如基数排序。 下图为排序算法体系结构图： 各大排序算法的时间复杂度、空间复杂度及稳定性见下表： 直接插入排序（Insertion Sort）基本思想将待排序的无序数列看成是一个仅含有一个元素的有序数列和一个无序数列，将无序数列中的元素逐次插入到有序数列中，从而获得最终的有序数列。 算法流程 初始时， $a[0]$自成一个有序区， 无序区为$a[1, … , n-1]$, 令$i=1$; 将$a[i]$并入当前的有序区$a[0, … , i-1]$; $i++$并重复步骤2，直到$i=n-1$, 排序完成。 详细描述如下： 从第一个元素开始，该元素可以认为已经被排序 取出下一个元素，在已经排序的元素序列中从后向前扫描 如果该元素（已排序）大于新元素，将该元素移到下一位置 重复步骤3，直到找到已排序的元素小于或者等于新元素的位置 将新元素插入到该位置中 重复步骤2 复杂度分析：第1个元素，需要进行 0 次比较;第2个元素，需要进行 1 次比较;第3个元素，需要进行 2 次比较;第n个元素，需要进行n-1次比较; 所以插入排序的时间复杂度是$O(n^2)$，空间复杂度是$O(1)$，稳定。 动态实例见：https://img-blog.csdnimg.cn/20181108191925813.gif 算法实现123456789101112131415161718192021222324252627//直接插入法一void InsertSort1(int a[], int n)&#123; int i, j; for(i=1; i&lt;n; i++) if(a[i] &lt; a[i-1]) &#123; int temp = a[i]; //保存要比较的值 for(j=i-1; j&gt;=0 &amp;&amp; a[j]&gt;temp; j--) //从后向前查找待插入位置 a[j+1] = a[j]; //挪位 a[j+1] = temp; //复制到插入位置 &#125;&#125;//直接插入法二：用数据交换代替法一的数据后移(比较对象只考虑两个元素)void InsertSort2(int a[], int n)&#123; for(int i=1; i&lt;n; i++) for(int j=i-1; j&gt;=0 &amp;&amp; a[j]&gt;a[j+1]; j--) Swap(a[j], a[j+1]);&#125;void Swap(int a,int b)&#123; int temp; temp=a; a=b; b=temp;&#125; 拓展（折半插入排序）仅适用于顺序存储的线性表直接插入排序是边比较，边移动元素；折半插入排序是先折半查找到插入位置，再统一移动元素 1234567891011121314151617void ZhebanInsertSort(int a[],int n)&#123; int i,j,low,high,mid; for(i=1;i&lt;n;i++)&#123; temp=a[i]; low=1,high=i; //设置折半查找范围 while(low&lt;=high)&#123; mid=(low+high)/2; if(a[mid]&gt;temp) high=mid-1; //查找左半部分 else low=mid+1; //查找右半部分 &#125; for(j=i;j&gt;=high+1;--j) a[j+1]=a[j]; //统一后移元素，空出插入位置 a[high+1]=temp; //插入 &#125;&#125; 希尔排序（Shell Sort）希尔排序是第一个突破$O(n^2)$的排序算法，是简单插入排序的优化，实质是分组的简单插入排序。它与插入排序的不同之处在于，它会优先比较距离较远的元素(每次取相隔一定间隔gap的元素作为一组，在组内执行简单插入排序)。希尔排序又叫缩小增量排序（不断减小间隔gap的数组，直到gap=1）。 基本思想 希尔排序是把记录按下标的一定量分组，对每组使用直接插入算法排序； 随着增量逐渐减少，每组包1含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法被终止。 算法流程 选择一个增量序列$t_1，t_2，…，t_k$，其中$t_i&gt;t_j，t_k=1$； 按增量序列个数$k$，对序列进行$k $趟排序； 每趟排序，根据对应的增量$t_i$，将待排序列分割成若干长度为$m$ 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 时间复杂度为$O(n^{1+e})$(其中0&lt;e&lt;1)，空间复杂度$O(1)$，不稳定。 动态实例见：https://img-blog.csdnimg.cn/20181108192122882.gif 算法实现12345678910111213141516171819202122232425//希尔排序法一void shellSort1(int a[],int n)&#123; int i,j,gap,temp; for(gap = n/2;gap&gt;0;gap/=2)&#123; for(i=gap;i&lt;n;i+=gap)&#123; temp = a[i]; for(j = i-gap;j&gt;=0&amp;&amp;a[j]&gt;temp;j-=gap)&#123; a[j+gap] = a[j]; &#125; a[j+gap] = temp; &#125; &#125;&#125;//希尔排序法二，和上面的直接插入排序类似，用数据交换代替法一的数据后移(比较对象只考虑两个元素)void ShellSort2(int a[], int n)&#123; int i, j, gap; //分组 for(gap=n/2; gap&gt;0; gap/=2) //直接插入排序 for(i=gap; i&lt;n; i++) for(j=i-gap; j&gt;=0 &amp;&amp; a[j]&gt;a[j+gap]; j-=gap) Swap(a[j], a[j+gap]);&#125; 冒泡排序（Bubble Sort）基本思想在要排序的一组数中，对当前还未排好序的范围内的全部数，自上而下对相邻的两个数依次进行比较和调整，让较大的数往下沉，较小的往上冒。即：每当两相邻的数比较后发现它们的排序与排序要求相反时，就将它们互换。每一趟排序后的效果都是讲没有沉下去的元素给沉下去。 算法流程 （递增为例） 比较相邻的元素。如果第一个比第二个大，就交换他们两个 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对。在这一点，最后的元素应该会是最大的数 针对所有的元素重复以上的步骤，除了最后一个 持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要比较。 复杂度分析第一次排序的比较次数：$ n-1$ n个元素相邻两两比较，需要n-1次比较;第二次排序的比较次数：$ n-2$ 第一次排序后最后一个元素可以确定为最大值，不再需要参与第二次排序;第三次排序的比较次数： $n-3$ 同理，最后两个数已经确定，不再需要参与排序;第 n-1 次排序的比较次数： $1$ ; 所以冒泡排序的时间复杂度是 $O(n^2)$。 空间复杂度是 $O(1)$，稳定 。 动态实例见：https://img-blog.csdnimg.cn/20181108191123628.gif 算法实现12345678910111213141516//冒泡排序void BubbleSort(int a[], int n)&#123; int i, j; for(i=0; i&lt;n; i++)&#123; bool flag=false; //表示本趟冒泡是否发生交换的标志 for(j=1; j&lt;n-i; j++)&#123; //j的起始位置为1，终止位置为n-i if(a[j]&lt;a[j-1])&#123; Swap(a[j-1], a[j]); flag=true; &#125; &#125; if(flag==false) //未交换，说明已经有序，停止排序 return; &#125; &#125; 拓展 （冒泡排序的改进 —- 鸡尾酒排序）鸡尾酒排序与冒泡排序的不同处在于排序时是以首尾双向在序列中进行排序。 先对数组从左到右进行升序的冒泡排序； 再对数组进行从右到左的降序的冒泡排序； 以此类推，持续的、依次的改变冒泡的方向，并不断缩小没有排序的数组范围； 鸡尾酒排序的优点是能够在特定条件下（如集合中大部分元素有序），减少排序的回合数。 实现鸡尾酒排序需要分别定义一个从最左边开始的index_left和从最右边开始的index_reight，当两个index相等的时候循环结束。 123456789101112131415161718//鸡尾酒排序void CocktailSort(int a[], int n)&#123; int left = 0, right=n-1; while(left&lt;right)&#123; for(int i=left;i&lt;right-1;i++)&#123; //从前往后排 if(a[i]&gt;a[i+1])&#123; Swap(a[i],a[i+1]); &#125; right-=1; for(int j=right;j&gt;left;j--)&#123; //从后往前排 if(a[j]&lt;a[j-1])&#123; Swap(a[j],a[j-1]); &#125; &#125; left+=1; &#125; &#125; &#125; 快速排序（Quick Sort）快速排序是目前所有内部排序算法中平均性能最优的排序算法。 基本思想快速排序算法的基本思想为分治思想。 先从数列中取出一个数作轴值（基准数）pivot； 根据基准数将数列进行分区，小于基准数的放左边，大于基准数的放右边； 重复分区操作，知道各区间只有一个数为止。 算法流程（递归+挖坑填数） $i=L，j=R$，将基准数挖出形成第一个坑$a[i]$； $j–$由后向前找出比它小的数，找到后挖出此数$a[j]$填到前一个坑$a[i]$中； $i++$从前向后找出比它大的数，找到后也挖出此数填到前一个坑$a[j]$中； 再重复2,3，直到$i=j$，将基准数填到$a[i]​$。 复杂度分析：时间复杂度为$O(nlog_{2}^{n})$，空间复杂度为$O(log_2^n)$，不稳定。 下面是一个我手写的详细分析实例： 动态实例见：https://img-blog.csdnimg.cn/20181108192522752.gif 算法实现1234567891011121314151617181920//快速排序void QuickSort(int a[],int left,int right)&#123; if(left&lt;right)&#123; int i=left,j=right； int base=a[left]; //基准 while(i&lt;j)&#123; while(i&lt;j&amp;&amp;a[j]&gt;=base) //从右往左找小于base的元素 j--; if(i&lt;j) a[i++]=a[j]; while(i&lt;j&amp;&amp;a[i]&lt;base) //从左往右找大于base的值 i++; if(i&lt;j) a[j--]=a[i]; &#125; a[i]=base; //将基准base填入最后的坑中 QuickSort(a,left,i-1); //递归调用，分治 QuickSort(a,i+1,right); &#125;&#125; 注意：如果基准不是选第一个数，可以用Swap()函数将基准调整至第一个位置，再执行上述程序。 总结：本文主要介绍了2大类排序算法：插入排序（直接插入、希尔排序）和交换排序（冒泡排序、快速排序）。在下一篇博文中，我将介绍剩下的4种排序（简单选择排序、堆排序、归并排序、基数排序），并对所有的排序算法做一个比较。]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2019春实习-百度-计算机视觉算法研发工程师真题]]></title>
    <url>%2FComputer-vision%2F2019-4-4-%E6%98%A5%E5%AE%9E%E4%B9%A0-%E7%99%BE%E5%BA%A6-%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%AE%97%E6%B3%95%E7%A0%94%E5%8F%91%E5%B7%A5%E7%A8%8B%E5%B8%88%2F</url>
    <content type="text"><![CDATA[前两天刚参加完百度计算机视觉算法实习岗的远程笔试，下面是我记忆中的一些考题，先记录下来，等答案公布再来详细分析。 题型 选择：30道，每题2分，共60分 问答：1道，每题30分，共30分 设计：1道，每题30分，共30分 编程：2道，每题20分，共40分 总分160分，考试时间2h 选择题选择题包含单选和多选，涉及到的考点很广，主要包括数据结构、操作系统、网络、C++程序题、视觉相关题等。数据结构和C++程序题偏多。 数据结构 哈希表 动态规划 时间复杂度的影响因素 哈夫曼编码 程序的三大基本结构 逆波兰表达式 操作系统 批处理、作业调度依据 静态重定位特点 链接器 网络 IPV6协议 ICMP报文作用 视觉相关 图像灰度的直方图 数据平滑技术 图像二维离散沃尔什变换 梯度下降中的微分算子 剩下的都是C++的编程题，让你填写代码，或者让你算出程序运行结果的。 问答题 相机模型的参数有哪些 三维空间坐标到图像坐标的投影公式 镜头畸变系数有哪些 对应的畸变矫正公式 设计题有一些按颗粒度分类好的数据，基于CNN或者其他CV模型，设计一个检测模型 设计包含几个模块，阐述每个模块的功能 怎样训练，如何优化，调整参数 如何提高模型的性能，假设可以增加训练数据，如何重新训练 编程题 求解不重复的字符串数目 题目： 给定一个长度小于$10^6$的字符串，每次将第一个字符移到末尾，然后记录所得到的新字符串，其中不同字符串数目是有限的，问一共记录了多少个不同的字符串？ 例如输入：abab输出：2 （只有abab和baba两种不同的字符串） 关于什么字符匹配的，记不清了=-=]]></content>
      <categories>
        <category>Computer vision</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言实现七大查找算法（三）]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-4-4-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%9F%A5%E6%89%BE3%2F</url>
    <content type="text"><![CDATA[上一篇博文主要介绍了哈希查找算法，本文主要介绍树表查找算法。这是一类算法，主要包含二叉查找树、平衡查找树之2-3查找树、平衡查找树之红黑树（Red-Black Tree）、B树和B+树。本文主要弄懂各种查找树的思想，也附上了部分实现代码。代码有时间在详细研读，此处先记录下来。红黑树、B树和B+树还是有点难懂~ ~ ~，本文只是简要介绍了思想，具体实现见参考资料 二叉树查找算法算法简介二叉查找树是先对待查找的数据进行生成树，确保树的左分支的值小于右分支的值，然后在就行和每个节点的父节点比较大小，查找最适合的范围。 这个算法的查找效率很高，但是如果使用这种查找方法要首先创建树。 算法思想二叉查找树（BinarySearch Tree，也叫二叉搜索树，或称二叉排序树Binary Sort Tree）或者是一棵空树，或者是具有下列性质的二叉树： 若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 任意节点的左、右子树也分别为二叉查找树。 二叉查找树性质：对二叉查找树进行中序遍历，即可得到有序的数列。 不同形态的二叉查找树如下图所示： 复杂度分析它和二分查找一样，插入和查找的时间复杂度均为$O(logn)$，但是在最坏的情况下仍然会有$O(n)$的时间复杂度。原因在于插入和删除元素的时候，树没有保持平衡。 查找过程查找操作和二分查找类似，将key和节点的key比较，如果小于，那么就在Left Node节点查找,如果大于，则在Right Node节点查找，如果相等，直接返回Value。 算法实现本部分主要介绍两种二叉查找树的实现方式：递归和非递归。123456789101112131415161718192021222324252627282930313233//非递归查找算法BSTNode *BST_Search(BiTree T,ElemType key,BSTNode *&amp;p)&#123; //查找函数返回指向关键字值为key的结点指针，不存在则返回NULL p=NULL; while(T!=NULL&amp;&amp;key!=T-&gt;data)&#123; p=T; //指向被查找结点的双亲 if(key&lt;T-&gt;data) T=T-&gt;lchild; //查找左子树 else T=T-&gt;rchild; //查找右子树 &#125; return T;&#125;//递归算法Status Search_BST(BiTree T, int key, BiTree f, BiTree *p)&#123; //查找BST中是否存在key，f指向T双亲，其初始值为NULL //若查找成功，指针p指向数据元素结点，返回true； //若失败，p指向查找路径上访问的最后一个结点并返回false if(!T)&#123; *p=f; return false; &#125; else if(key==T-&gt;data)&#123; //查找成功 *p=T; return true; &#125; else if(key&lt;T-&gt;data) return Search_BST(T-&gt;lchild,key,T,p); //递归查找左子树 else return Search_BST(T-&gt;rchild,key,T,p); //递归查找右子树 &#125; 基于二叉查找树进行优化，可以得到其他的树表查找算法，如平衡树、红黑树等 平衡查找树之2-3查找树平衡二叉树：BST中左右子树高度差的绝对值不超过1，平衡因子取值为｛-1，0，1｝ 2-3查找树定义和二叉树不一样，2-3树运行每个节点保存1个或者两个的值。对于普通的2节点(2-node)，他保存1个key和左右两个自己点。对应3节点(3-node)，保存两个Key，2-3查找树的定义如下： 要么为空，要么： 对于2节点，该节点保存一个key及对应value，以及两个指向左右节点的节点，左节点也是一个2-3节点，所有的值都比key要小，右节点也是一个2-3节点，所有的值比key要大。 对于3节点，该节点保存两个key及对应value，以及三个指向左中右的节点。左节点也是一个2-3节点，所有的值均比两个key中的最小的key还要小；中间节点也是一个2-3节点，中间节点的key值在两个跟节点key值之间；右节点也是一个2-3节点，节点的所有key值比两个key中的最大的key还要大。 2-3查找树的性质 如果中序遍历2-3查找树，就可以得到排好序的序列； 在一个完全平衡的2-3查找树中，根节点到每一个为空节点的距离都相同。 复杂度分析2-3树的查找效率与树的高度息息相关。 在最坏的情况下，也就是所有的节点都是2-node节点，查找效率为$log_2^N$ 在最好的情况下，所有的节点都是3-node节点，查找效率为$log_3^N$约等于0.631$log_2^N$ 对于插入来说，只需要常数次操作即可完成，因为他只需要修改与该节点关联的节点即可，不需要检查其他节点，所以效率和查找类似。 查找过程 算法实现直接实现2-3树比较复杂，因为： 需要处理不同的节点类型，非常繁琐 需要多次比较操作来将节点下移 需要上移来拆分4-node节点 拆分4-node节点的情况有很多种 2-3查找树实现起来比较复杂，在某些情况插入后的平衡操作可能会使得效率降低。在2-3查找树基础上改进的红黑树不仅具有较高的效率，并且实现起来较2-3查找树简单。 平衡查找树之红黑树（Red-Black Tree）2-3查找树能保证在插入元素之后能保持树的平衡状态，最坏情况下即所有的子节点都是2-node，树的高度为lgn，从而保证了最坏情况下的时间复杂度。但是2-3树实现起来比较复杂，于是就有了一种简单实现2-3树的数据结构，即红黑树（Red-Black Tree）。 基本思想红黑树的思想就是对2-3查找树进行编码，尤其是对2-3查找树中的3-nodes节点添加额外的信息。红黑树中将节点之间的链接分为两种不同类型，红色链接，他用来链接两个2-nodes节点来表示一个3-nodes节点。黑色链接用来链接普通的2-3节点。特别的，使用红色链接的两个2-nodes来表示一个3-nodes节点，并且向左倾斜，即一个2-node是另一个2-node的左子节点。这种做法的好处是查找的时候不用做任何修改，和普通的二叉查找树相同。 基本定义红黑树是一种具有红色和黑色链接的平衡查找树，同时满足： 红色节点向左倾斜 一个节点不可能有两个红色链接 整个树完全黑色平衡，即从根节点到所以叶子结点的路径上，黑色链接的个数都相同。 下图可以看到红黑树其实是2-3树的另外一种表现形式：如果我们将红色的连线水平绘制，那么他链接的两个2-node节点（E和J）就是2-3树中的一个3-node节点了。 性质红黑树是每个节点都带有颜色属性的二叉查找树，颜色为红色或黑色。在二叉查找树强制的一般要求以外，对于任何有效的红黑树我们增加了如下的额外要求: 节点是红色或黑色。 根是黑色。 所有叶子都是黑色（叶子是NIL节点）。 每个红色节点必须有两个黑色的子节点。(从每个叶子到根的所有路径上不能有两个连续的红色节点。) 从任一节点到其每个叶子的所有简单路径都包含相同数目的黑色节点。 下面是一个具体的红黑树的图例： 复杂度分析：最坏的情况就是，红黑树中除了最左侧路径全部是由3-node节点组成，即红黑相间的路径长度是全黑路径长度的2倍。 下图是一个典型的红黑树，从中可以看到最长的路径(红黑相间的路径)是最短路径的2倍： 详细的红黑树介绍可参见数据结构中各种树和平衡查找树之红黑树 B树B树（B-tree），也称多路平衡查找树，是一种树状数据结构，能够用来存储排序后的数据。这种数据结构能够让查找数据、循序存取、插入数据及删除的动作，都在对数时间内完成。B树，概括来说是一个一般化的二叉查找树，可以拥有多于2个子节点。 B树的定义B树作为一种多路搜索树（并不是二叉的），B树中所有结点的孩子结点数的最大值成为B树的阶，通常记做M。 树中每个结点至多有M棵子树，（M&gt;2）； 若根结点不是终端结点，则其子树数目为$[2, M]$； 除根结点以外的非叶子结点的子树数目为$[\lceil M/2 \rceil, M]$(向上取整)； 每个结点存放至少$\lceil M/2 \rceil-1$和至多$M-1$个关键字； 非叶子结点的关键字个数=指向子树的指针个数-1； 非叶子结点的关键字：$K[1], K[2], …, K[M-1]$；且$K[i] &lt; K[i+1]$； 非叶子结点的指针：$P[1], P[2], …, P[M]$；其中$P[1]$指向关键字小于$K[1]$的子树，$P[M]$指向关键字大于$K[M-1]$的子树，其它$P[i]$指向关键字属于($K[i-1], K[i]$)的子树； 所有叶子结点位于同一层； 对于任意一棵包含n个关键字，高度为h，阶数为m的B树，其高度满足：B树的高度：$log_{m}^{n+1}\leq h \leq log_{\lceil m/2 \rceil}^{[(n+1)/2]}+1$ 下图是一个M=3 阶的B树: B树的构造可以看到B树是2-3树的一种扩展，他允许一个节点有多于2个的元素。B树的插入及平衡化操作和2-3树很相似，这里就不介绍了。下面是往B树中依次插入 6 10 4 14 5 11 15 3 2 12 1 7 8 8 6 3 6 21 5 15 15 6 32 23 45 65 7 8 6 5 4 的演示动画(https://files-cdn.cnblogs.com/files/yangecnu/btreebuild.gif)： 最后结果如下： B树的查找在B树中查找给定关键字的方法是，首先把根结点取来，在根结点所包含的关键字$K_1,…,K_n$查找给定的关键字（可用顺序查找或二分查找法），若找到等于给定值的关键字，则查找成功；否则，一定可以确定要查找的关键字在$K_i与K_{i+1}$之间，$P_i$为指向子树根节点的指针，此时取指针$P_i$所指的结点继续查找，直至找到，或指针$P_i$为空时查找失败。 B-树为系统最优化大块数据的读和写操作。B-tree算法减少定位记录时所经历的中间过程，从而加快存取速度。这种数据结构常被应用在数据库和文件系统的实作上。 B+树B+树是应数据库所需出现的一种B树的变形树。 定义一棵M阶的B+树需满足以下条件： 其定义基本与B-树相同，除了： 非叶子结点的子树指针与关键字个数相同； 非叶子结点的子树指针$P[i]$，指向关键字值属于$[K[i], K[i+1])$的子树（B-树是开区间）； 为所有叶子结点增加一个链指针； 所有关键字都在叶子结点出现； 下图是一个M=3 阶的B+树: B树的构造(https://files-cdn.cnblogs.com/files/yangecnu/Bplustreebuild.gif) 为一个B+树创建的示意图： 最后结果如下： B+树的查找 B+树的搜索与B树也基本相同，区别是B+树只有达到叶子结点才命中（B树可以在非叶子结点命中），其性能也等价于在关键字全集做一次二分查找； B树与B+树的对比B和B+树的区别在于，B+树的非叶子结点只包含导航信息，不包含实际的值，所有的叶子结点和相连的节点使用链表相连，便于区间查找和遍历。 B树的优点由于B树的每一个节点都包含key和value，因此经常访问的元素可能离根节点更近，因此访问也更迅速。 B+树的优点 由于B+树在内部节点上不好含数据信息，因此在内存页中能够存放更多的key。 数据存放的更加紧密，具有更好的空间局部性。因此访问叶子几点上关联的数据也具有更好的缓存命中率。 B+树的叶子结点都是相链的，因此对整棵树的便利只需要一次线性遍历叶子结点即可。而且由于数据顺序排列并且相连，所以便于区间查找和搜索。而B树则需要进行每一层的递归遍历。相邻的元素可能在内存中不相邻，所以缓存命中性没有B+树好。 下面是B 树和B+树的区别图： 总结二叉查找树平均查找性能不错，为$O(logn)$，但是最坏情况会退化为$O(n)$。在二叉查找树的基础上进行优化，我们可以使用平衡查找树。平衡查找树中的2-3查找树，这种数据结构在插入之后能够进行自平衡操作，从而保证了树的高度在一定的范围内进而能够保证最坏情况下的时间复杂度。但是2-3查找树实现起来比较困难，红黑树是2-3树的一种简单高效的实现，他巧妙地使用颜色标记来替代2-3树中比较难处理的3-node节点问题。红黑树是一种比较高效的平衡查找树，应用非常广泛，很多编程语言的内部实现都或多或少的采用了红黑树。 除此之外，2-3查找树的另一个扩展——B/B+平衡树，在文件系统和数据库系统中有着广泛的应用。 参考资料七大查找算法Data Structure平衡查找树之红黑树平衡查找树之B树]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>searching</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言实现七大查找算法（二）]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-4-3-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%9F%A5%E6%89%BE2%2F</url>
    <content type="text"><![CDATA[在前面的博文中，我们介绍了5种查找算法，本文主要介绍哈希表及哈希查找算法。 在介绍哈希查找算法之前，我们需要详细了解什么是哈希表及其构造实现方法。 哈希表哈希表的基本思想我们知道，数组的最大特点就是：寻址容易，插入和删除困难；而链表正好相反，寻址困难，而插入和删除操作容易。那么如果能够结合两者的优点，做出一种寻址、插入和删除操作同样快速容易的数据结构。这就是哈希表创建的基本思想，哈希表就是这样一个集查找、插入和删除操作于一身的数据结构。 哈希表的一些基本概念 哈希表（Hash Table）：也叫散列表，是根据关键码值（Key-Value）而直接进行访问的数据结构，也就是我们常用到的map。 哈希函数：也称散列函数，是Hash表的映射函数，它可以把查找表中的关键字映射成该关键字对应的地址函数，表示如下：$$Hash(key) = Addr,（地址可以是数组下标、索引、内存地址等）$$哈希函数能使对一个数据序列的访问过程变得更加迅速有效，通过哈希函数数据元素能够被很快的进行定位。 冲突：两个不同的关键字，由于散列函数值相同，因而被映射到同一表位置上。该现象称为冲突(Collision)或碰撞。 尽量减少冲突 冲突不可避免，要设计好处理冲突的方法 散列函数的构造方法 定义域必须包含全部需要存储的关键字 地址等概率，均匀的分布在整个地址空间 函数尽量简单，较短时间内能算出任一关键字地址 哈希表有很多种不同的实现方法，为了实现哈希表的创建，这些所有的方法都离不开两个问题——“定址”和“解决冲突” 哈希表的定址方法 直接定址法：直接取关键字的某个线性函数值作为散列地址，例如：$$H(key) = a \times key + b,(a和b均为常数)$$ 适合关键字分布基本连续的情况，否则容易造成存储空间浪费。 除留余数法：(最常用) 假定表长为m，取一个不大于m但最接近或者等于m的质数P，例如：$$H(key) = key\%P$$ 数字分析法：比如有一组$value1=112233，value2=112633，value3=119033$，针对这样的数我们分析数中间两个数比较波动，其他数不变。那么我们取key的值就可以是$key1=22,key2=26,key3=90$。 适合于已知关键字的集合分布，关键字位数较大的情况。 平方取中法：取关键字的平法值的中间几位作为散列地址。 适合于不是道关键字分布，且关键字每一位取值不均匀或均小于散列地址所需的位数。 折叠法：举个例子，比如$value=135790$，要求key是2位数的散列值。那么我们将$value$变为$13+57+90=160$，然后去掉高位$“1”$,此时$key=60$，这就是他们的哈希关系。这样做的目的就是key与每一位value都相关，来达到“散列地址”尽可能分散的目的。 适合于不需要知道关键字分布，关键字位数较多的情况。 哈希表解决冲突的方法 开放定址法：如果两个数据元素的哈希值相同，则在哈希表中为后插入的数据元素另外选择一个表项。当程序查找哈希表时，如果没有在第一个对应的哈希表项中找到符合查找要求的数据元素，程序就会继续往后查找，直到找到一个符合查找要求的数据元素，或者遇到一个空的表项。 假设已经选定散列函数为$H(key)$，下面用$H_i$表示发生冲突后第$i$次探测的散列地址。$$H_{i} = (H(key)+d_{i})\%m$$其中，$i=1, 2, …,k (k&lt;=1)$；$m$表示散列长度，$d_i$表示增量序列。 增量序列$d_i$通常有4种取法： 线性探测法：$d_i=1,2,…, m-1$，可能造成大量元素在相邻地址聚集，降低查找效率。 平方探测法：$d_i=1^2,-1^2,2^2,-2^2,…,-k^2,(k&lt;=m/2)$，m必须是一个可表示为$4k+3$的质数。避免出现“堆积问题”，但是不能探测全部单元（至少一半）。 再散列法：$d_i=Hash_{2}(key)$，使用2个散列函数。 伪随机序列法：$d_i=$伪随机序列 注意：开放定址情形下，不能随便物理删除表中已有的元素，这样会截断其他具有相同散列地址元素查找地址；应做删除标记，逻辑删除。 以线性探测为例（定址法选择取余法），举例如下： 散列过程如下图所示： 我们可以发现，冲突次数还是比较多的，这是因为P的取值没选好，前面讲过：假定表长为m，取一个不大于m但最接近或者等于m的质数P，这里表长m=10，P最好取7，而图中取了10。 拉链法：将哈希值相同的数据元素存放在一个链表中，在查找哈希表的过程中，当查找到这个链表时，必须采用线性查找方法。适用于经常进行插入、删除的情况。 定址法选择取余法，举例如下：设有一组关键字为(26，36，41，38，44，15，68，12，6，51)，初始情况如下图所示： 最终结果如下图所示： 散列查找及性能分析查找过程与构造过程基本一致 查找效率三因素：散列函数、处理冲突的方法、哈希表装填因子$(\alpha)$$$\alpha = \frac{表中记录数}{散列表长度} = \frac{n}{m}$$平均查找长度依赖于$\alpha$，$\alpha$越大，记录越“满”，发生冲突的可能性越大。 下面贴一张哈希表中查找失败和成功时，平均查找程度的计算例子 拓展：如果关键字是字符串怎么办？ （详见参考资料） 将字符串的所有的字符的ASCII码值进行相加，将所得和作为元素的关键字 假设关键字至少有三个字母构成，散列函数只是取前三个字母进行散列 借助Horner’s 规则，构造一个质数（通常是37）的多项式，（非常的巧妙，不知道为何是37）。计算公式为:$key[keysize-i-1]*37^i, 0&lt;=i&lt;keysize$求和。 哈希查找介绍完如何构造哈希表后，我们来看一下哈希查找算法。 算法简介哈希表就是一种以键-值(key-indexed) 存储数据的结构，只要输入待查找的值即key，即可查找到其对应的索引值（地址）。 算法思想哈希的思路很简单，如果所有的键都是整数，那么就可以使用一个简单的无序数组来实现：将键作为索引，值即为其对应的值，这样就可以快速访问任意键的值。这是对于简单的键的情况，我们将其扩展到可以处理更加复杂的类型的键。 算法流程 用给定的哈希函数构造哈希表； 根据选择的冲突处理方法解决地址冲突； 在哈希表的基础上执行哈希查找。 哈希表（空间换时间）是一个在时间和空间上做出权衡的经典例子。如果没有内存限制，那么可以直接将键作为数组的索引。那么所有的查找时间复杂度为O(1)；如果没有时间限制，那么我们可以使用无序数组并进行顺序查找，这样只需要很少的内存。哈希表使用了适度的时间和空间来在这两个极端之间找到了平衡。只需要调整哈希函数算法即可在时间和空间上做出取舍。 复杂度分析：单纯论查找复杂度：对于无冲突的Hash表而言，查找复杂度为O(1)（注意，在查找之前我们需要构建相应的Hash表）。 算法实现这边找了一个除留余数法加线性开放定址法的代码实例，来源于c实现哈希查找 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293//采用除数取留法确定地址，利用线性开放地址法处理冲突问题，2016.5.28#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;io.h&gt;#include &lt;math.h&gt;#include&lt;time.h&gt; #define HASHSIZE 15#define NULLKEY -32768 typedef struct&#123; int *elem; //数据元素存储地址 int count;//当前元素个数&#125;HashTable;int L = 0; //表的长度 bool init(HashTable *hashTable)//哈希表的初始化&#123; int i; L = HASHSIZE; hashTable-&gt;elem = (int*)malloc(L*sizeof(int));//申请内存 hashTable-&gt;count = L; for (i = 0; i &lt; L; i++) &#123; hashTable-&gt;elem[i]=NULLKEY; &#125; return true;&#125; //哈希函数，除留余数法，最常用的哈希函数，还有其它的。int Hash(int data)&#123; return data%L;&#125; void insert( HashTable *hashTable, int data)&#123; int Addr = Hash(data);//求哈希地址 while (hashTable-&gt;elem[Addr] != NULLKEY)//求得地址不是初始化时的空，则表示有元素已经插入，会有冲突 &#123; Addr = (Addr + 1) % L;//开放地址线性探测，还可以二次探测 &#125; hashTable-&gt;elem[Addr] = data;&#125; int find(HashTable *hashTable, int data)&#123; int Addr = Hash(data); //求哈希地址 while (hashTable-&gt;elem[Addr] != data) //线性开放定址法解决冲突 &#123; Addr = (Addr + 1) % L; if (hashTable-&gt;elem[Addr] == NULLKEY || Addr == Hash(data)) return 0; &#125; return Addr;&#125; void display(HashTable *hashTable) //散列元素显示&#123; int i; printf(".........结果展示.........\n"); for (i = 0; i &lt; hashTable-&gt;count; i++) &#123; printf("%d\n", hashTable-&gt;elem[i]); &#125;&#125; void main()&#123; int i, j, result, x; HashTable hashTable; int arr[HASHSIZE]; printf("请输入少于15个，初始化哈希表的元素：\n"); for (j = 0; j &lt; HASHSIZE; j++) &#123; scanf("%d", &amp;arr[j]); &#125; init(&amp;hashTable); for (i = 0; i &lt; HASHSIZE; i++) &#123; insert(&amp;hashTable, arr[i]); &#125; display(&amp;hashTable); printf("请输入你要查找的元素：\n"); scanf("%d", &amp;x); result = find(&amp;hashTable, x); if (result) printf("查找元素%d在哈希表中的位置为%d\n",x,result); else printf("没找到！\n"); system("pause");&#125; 参考资料：Hash那点事儿七大查找算法]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>searching</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言实现七大查找算法（一）]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-4-3-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[本文主要介绍数据结构中的查找算法，主要介绍顺序查找、折半查找（二分查找）、树表查找、分块查找、哈希查找（散列）。其他的一些查找算法也会有所介绍。 查找（Searching）就是根据给定的某个值，在查找表中确定一个其关键字等于给定值的数据元素。 查找表（Search Table）：由同一类型的数据元素构成的集合 关键字（Key）：数据元素中某个数据项的值，又称为键值 主键（Primary Key）：可唯一的标识某个数据元素或记录的关键字 查找表按照操作方式可分为： 静态查找表（Static Search Table）：只做查找操作的查找表。它的主要操作是： 查询某个“特定的”数据元素是否在表中 检索某个“特定的”数据元素和各种属性 动态查找表（Dynamic Search Table）：在查找中同时进行插入或删除等操作： 查找时插入数据 查找时删除数据 平均查找长度（Average Search Length，ASL）：在所有的查找过程中进行关键字的比较次数的平均值。对于含有n个数据元素的查找表，查找成功的平均查找长度计算公式如下：$$ASL = \sum_{i=1}^{n}P_{i}C_{i}$$ $P_i$：查找表中第i个数据元素的概率,一般等概率为$\frac{1}{n}$ $C_i​$：找到第i个数据元素时已经比较过的次数。 顺序查找算法简介 顺序查找又称为线性查找，是一种最简单的查找方法。适用于线性表的顺序存储结构和链式存储结构。时间复杂度为$O（n）$。 基本思路 从第一个元素m开始逐个与需要查找的元素x进行比较，当比较到元素值相同(即m=x)时返回元素m的下标，如果比较到最后都没有找到，则返回-1。 优缺点 缺点：当n 很大时，平均查找长度较大，效率低； 优点：对表中数据元素的存储没有要求。另外，对于线性链表，只能进行顺序查找。 算法实现123456789//顺序查找，n为数组a的长度int SequenceSearch(int a[], int value, int n)&#123; int i; for(i=0; i&lt;n; i++) if(a[i]==value) return i; return -1;&#125; 折半查找算法简介 折半查找，也称二分查找（Binary Search），是一种在有序数组中查找某一特定元素的查找算法。查找过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则查找过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。这种查找算法每一次比较都使查找范围缩小一半。时间复杂度为$O（log_2^n）$。 基本思路给予一个包含 n个带值元素的数组A1、 令 L为0 ， R为 n-1 ；2、 如果L&gt;R，则搜索以失败告终 ；3、 令 m (中间值元素)为 ⌊(L+R)/2⌋ (向下取整)；4、 如果 $A_m&lt;T$，令 L为 m + 1 并回到步骤二 ；5、 如果 $A_m&gt;T​$，令 R为 m - 1 并回到步骤二； 注意：折半查找的前提条件是需要有序表顺序存储，对于静态查找表，一次排序后不再变化，折半查找能得到不错的效率。但对于需要频繁执行插入或删除操作的数据集来说，维护有序的排序会带来不小的工作量，那就不建议使用。 算法实现1234567891011121314151617181920212223242526272829303132//二分查找（折半查找）,一般方法，n为数组a的长度int BinarySearch1(int a[], int value, int n)&#123; int low, high, mid; low = 0; high = n-1; while(low&lt;=high) &#123; mid = (low+high)/2; if(a[mid]==value) //取中间量 return mid; else if(a[mid]&gt;value) high = mid-1; //从前半部分查找 else low = mid+1; //从后半部分查找 &#125; return -1;&#125;//二分查找，递归方法int BinarySearch2(int a[], int value, int low, int high)&#123; if(low&gt;high) return -1; int mid = (low+high)/2; if(a[mid]==value) return mid; else if(a[mid]&gt;value) return BinarySearch2(a, value, low, mid-1); else return BinarySearch2(a, value, mid+1, high);&#125; 插值查找算法简介在介绍插值查找之前，首先考虑一个新问题，为什么上述算法一定要是折半，而不是折四分之一或者折更多呢？打个比方，在英文字典里面查“apple”，你下意识翻开字典是翻前面的书页还是后面的书页呢？如果再让你查“zoo”，你又怎么查？很显然，这里你绝对不会是从中间开始查起，而是有一定目的的往前或往后翻。经过以上分析，折半查找这种查找方式，不是自适应的（也就是说是傻瓜式的）。二分查找中查找点计算如下： $$mid=(low+high)/2, 即mid=low+1/2(high-low)$$通过类比，我们可以将查找的点改进为如下： mid=low+(key-a[low])/(a[high]-a[low])*(high-low)，也就是将上述的比例参数1/2改进为自适应的，根据关键字在整个有序表中所处的位置，让mid值的变化更靠近关键字key，这样也就间接地减少了比较次数。查找成功或者失败的时间复杂度均为$O(log_2(log_2^n))$,最坏情况可能需要$O（n）$。 算法思想基于二分查找算法，将查找点的选择改进为自适应选择，可以提高查找效率。当然，插值查找也属于有序查找。 注：对于表长较大，而关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比折半查找要好的多。反之，数组中如果分布非常不均匀，那么插值查找未必是很合适的选择。 算法实现 12345678910111213//插值查找，类似二分查找，只是mid计算方式不同，此处给出递归方法，一般方法也和上述二分查找类似int InsertionSearch(int a[], int value, int low, int high)&#123; if(low&gt;high) return -1; int mid = low+(value-a[low])/(a[high]-a[low])*(high-low); if(a[mid]==value) return mid; else if(a[mid]&gt;value) return InsertionSearch(a, value, low, mid-1); else return InsertionSearch(a, value, mid+1, high);&#125; 分块查找算法简介要求是顺序表，分块查找又称索引顺序查找，它是顺序查找的一种改进方法。时间复杂度：$O(log(m)+n/m)$ 算法思想 将n个数据元素”按块有序”划分为m块（m ≤ n）。 每一块中的结点不必有序，但块与块之间必须”按块有序”； 即第1块中任一元素的关键字都必须小于第2块中任一元素的关键字； 而第2块中任一元素又都必须小于第3块中的任一元素，…… 算法流程 先选取各块中的最大关键字构成一个索引表； 查找分两个部分：先对索引表进行二分查找或顺序查找，以确定待查记录在哪一块中； 在已确定的块中用顺序法进行查找。 平均查找长度将长度为n的查找表均匀分为m块，每块有s个记录，在等概率的情况，平均查找长度计算如下： 索引和块内均用顺序查找$$ASL= \frac{m+1}{2} + \frac{s+1}{2} = \frac{s^2+2s+n}{2s},(ms=n)$$特殊的$$当 s =\sqrt{n}时，ASL_{min} = \sqrt{n}+1$$ 索引折半查找，块内顺序查找 $$ASL = \left \lceil log_2^(m+1) \right \rceil + \frac{s+1}{2}$$ 斐波那契查找这部分大致知道过程，先记录下来，以后有时间在认真研究一下=-=，主要参考百度百科 算法简介斐波那契数列，又称黄金分割数列，指的是这样一个数列：$1、1、2、3、5、8、13、21、····$，在数学上，斐波那契被递归方法如下定义：$F(1)=1，F(2)=1，F(n)=f(n-1)+F(n-2) （n&gt;=2）$。该数列越往后相邻的两个数的比值越趋向于黄金比例值（0.618）。 黄金比例又称黄金分割，是指事物各部分间一定的数学比例关系，即将整体一分为二，较大部分与较小部分之比等于整体与较大部分之比，其比值约为1:0.618或1.618:1。 斐波那契查找的时间复杂度还是$O(log_2^n )$，但是与折半查找相比，斐波那契查找的优点是它只涉及加法和减法运算，而不用除法，而除法比加减法要占用更多的时间，因此，斐波那契查找的运行时间理论上比折半查找小，但是还是得视具体情况而定。 算法思想也是二分查找的一种提升算法，通过运用黄金比例的概念在数列中选择查找点进行查找，提高查找效率。同样地，斐波那契查找也属于一种有序查找算法。相对于折半查找，一般将待比较的key值与第$mid=（low+high/2$位置的元素比较，比较结果分三种情况： 相等，mid位置的元素即为所求; 大于，$low=mid+1$; 小于，$high=mid-1$。 斐波那契查找与折半查找很相似，他是根据斐波那契序列的特点对有序表进行分割的。要求开始表中记录的个数为某个斐波那契数小1，及$n=F(k)-1$; 开始将k值与第$F(k-1)$位置的记录进行比较(及$mid=low+F(k-1)-1$),比较结果也分为三种 相等，mid位置的元素即为所求 大于，$low=mid+1,k-=2$; 说明：$low=mid+1$说明待查找的元素在$[mid+1,high]$范围内，$k-=2$ 说明范围$[mid+1,high]$内的元素个数为$n-(F(k-1))= F(k)-1-F(k-1)=F(k)-F(k-1)-1=F(k-2)-1$个，所以可以递归的应用斐波那契查找。 小于，$high=mid-1,k-=1​$。 说明：$low=mid+1$说明待查找的元素在$[low,mid-1]$范围内，$k-=1$ 说明范围$[low,mid-1]$内的元素个数为$F(k-1)-1$个，所以可以递归 的应用斐波那契查找。 $n=F(k)-1$， 表中记录的个数为某个斐波那契数小1。这是为什么呢？ 是为了格式上的统一，以方便递归或者循环程序的编写。表中的数据是$F(k)-1$个，使用$mid$值进行分割又用掉一个，那么剩下$F(k)-2$个。正好分给两个子序列，每个子序列的个数分别是$F(k-1)-1$与$F(k-2)-1$个，格式上与之前是统一的。不然的话，每个子序列的元素个数有可能是$F(k-1)，F(k-1)-1，F(k-2)，F(k-2)-1$个，写程序会非常麻烦。 算法举例对于斐波那契数列：$1、1、2、3、5、8、13、21、34、55、89……$（也可以从0开始），前后两个数字的比值随着数列的增加，越来越接近黄金比值0.618。比如这里的89，把它想象成整个有序表的元素个数，而89是由前面的两个斐波那契数34和55相加之后的和，也就是说把元素个数为89的有序表分成由前55个数据元素组成的前半段和由后34个数据元素组成的后半段，那么前半段元素个数和整个有序表长度的比值就接近黄金比值0.618，假如要查找的元素在前半段，那么继续按照斐波那契数列来看，55 = 34 + 21，所以继续把前半段分成前34个数据元素的前半段和后21个元素的后半段，继续查找，如此反复，直到查找成功或失败，这样就把斐波那契数列应用到查找算法中了。 从图中可以看出，当有序表的元素个数不是斐波那契数列中的某个数字时，需要把有序表的元素个数长度补齐，让它成为斐波那契数列中的一个数值，当然把原有序表截断肯定是不可能的，不然还怎么查找。然后图中标识每次取斐波那契数列中的某个值时(F[k])，都会进行-1操作，这是因为有序表数组位序从0开始的，纯粹是为了迎合位序从0开始。 算法实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172// 斐波那契查找.cpp #include "stdafx.h" #include &lt;memory&gt; #include &lt;iostream&gt; using namespace std; const int max_size=20;//斐波那契数组的长度 /*构造一个斐波那契数组*/ void Fibonacci(int * F) &#123; F[0]=0; F[1]=1; for(int i=2;i&lt;max_size;++i) F[i]=F[i-1]+F[i-2]; &#125; /*定义斐波那契查找法*/ int Fibonacci_Search(int *a, int n, int key) //a为要查找的数组,n为要查找的数组长度,key为要查找的关键字 &#123; int low=0; int high=n-1; int F[max_size]; Fibonacci(F);//构造一个斐波那契数组F int k=0; while(n&gt;F[k]-1)//计算n位于斐波那契数列的位置 ++k; int * temp;//将数组a扩展到F[k]-1的长度 temp=new int [F[k]-1]; memcpy(temp,a,n*sizeof(int)); for(int i=n;i&lt;F[k]-1;++i) temp[i]=a[n-1]; while(low&lt;=high) &#123; int mid=low+F[k-1]-1; if(key&lt;temp[mid]) &#123; high=mid-1; k-=1; &#125; else if(key&gt;temp[mid]) &#123; low=mid+1; k-=2; &#125; else &#123; if(mid&lt;n) return mid; //若相等则说明mid即为查找到的位置 else return n-1; //若mid&gt;=n则说明是扩展的数值,返回n-1 &#125; &#125; delete [] temp; return -1; &#125; int _tmain(int argc, _TCHAR* argv[]) &#123; int a[] = &#123;0,16,24,35,47,59,62,73,88,99&#125;; int key=100; int index=Fibonacci_Search(a,sizeof(a)/sizeof(int),key); cout&lt;&lt;key&lt;&lt;" is located at:"&lt;&lt;index; system("PAUSE"); return 0; &#125; 总结：本文主要介绍了5种常见的查找算法，在接下来的文章中，将继续介绍另外2种更重要的查找算法，树表查找和哈希查找。]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>searching</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python实现栈和队列]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-3-31-Python%E5%AE%9E%E7%8E%B0%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[本文主要用python语言实现栈和队列的基本操作。 python实现栈的基本操作1234567891011121314151617181920212223242526272829"""以列表的形式简单实现栈栈：先进后出"""class Stack: def __init__(self): self.stack = [] # 初始化 def is_empty(self): return not bool(self.stack) # 判空 def push(self, value): self.stack.append(value) # 入栈 return True def pop(self): if self.stack: return self.stack.pop() # 出栈 else: raise LookupError('stack is empty!') def peek(self): if self.stack: return self.stack[-1] # 获取栈顶元素 else: raise LookupError('stack is empty') def length(self): return len(self.stack) # 获取栈内元素个数 python实现队列的基本操作123456789101112131415161718192021222324252627"""以列表的形式简单实现队列队列：先进先出"""class Queue: def __init__(self): self.queue = [] def is_empty(self): return not bool(self.queue) def enqueue(self, items): self.queue.append(items) return True def dequeue(self): if self.is_empty(): raise LookupError('queue is empty!') return self.queue.pop(0) def length(self): return len(self.queue) def show(self): if self.is_empty(): raise LookupError('queue is empty!') return self.queue]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>stack queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构之栈和队列（二）]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-3-31-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%972%2F</url>
    <content type="text"><![CDATA[在上篇博文中，我们了解了栈的节本原理和操作。本文主要介绍另外一种操作受限的线性表，队列（Queue）。 队列（Queue）也是一种操作受限的线性表，它只允许在表的一端进行插入，而在另外一端进行删除，满足先进先出（FIFO）。 队列的基本操作 InitQueue(&amp;Q)：初始化 QueueEmpty(Q)：判断队列是否为空 EnQueue(&amp;Q,x)：入队，若Q未满，将x加入，使之成为新的队尾 DeQueue(&amp;Q,&amp;x)：出队，若Q非空，删除队头元素，并用x返回 GetHead(Q,&amp;x)：读队头元素，若Q非空，将队头元素赋值给x 队列的顺序存储结构分配一块连续的存储单元存放队列中的元素，并附设两个指针front和rear分别指向队头元素和队尾元素的位置。设队头指针指向队头元素，队尾指针指向队尾的下一个位置。 队列的顺序存储类型可描述如下：12345#define MaxSize 50typedef struct&#123; ElemType data[MaxSize]; //存放队列元素 int front,rear; //队头指针和队尾指针&#125;SqQueue; 初始状态（队空条件）$Q.front==Q.rear==0$进队操作：队不满时，先送值到队尾元素，再将队尾指针加1出队操作：队不空时，先取队头元素，再将队头指针加1 如上图所示，我们可以用$Q.front==Q.rear==0$来判断队列是否为空，但是我们不能用$Q.rear==MaxSize$作队满的条件，如上图(d)所示。 循环队列为了克服顺序队列的缺点，引出了循环队列，将顺序队列臆造为一个环状空间。 出队入队时，指针按顺时针方向进1，如下图所示： 为了区分$Q.front==Q.rear$为队空还是队满，有三种处理方式： 入队时，少用一个队列单元，队头指针在队尾指针的下一个位置时队满 队满条件：$(Q.rear+1)$%$MaxSize==Q.front$队空条件：$Q.front==Q.rear$队列中元素个数：$(Q.rear-Q.front+MaxSize)$%$MaxSize​$ 增设表示元素个数的数据成员 队空：$Q.size==0$队满：$Q.size==MaxSize​$ 增设tag成员，区分队满还是队空（tag表示下一个存储空间是否为空） tag=0,出队：因为删除导致$Q.front==Q.rear$ (队空)tag=1,进队：因为插入导致$Q.front==Q.rear$ (队满) 循环队列的基本操作如下：12345678910111213141516171819202122232425262728//初始化void InitQueue(&amp;Q)&#123; Q.rear=Q.front=0;&#125;//判断队列是否为空bool isEmpty(Q)&#123; if(Q.rear==Q.front) return true; else return false;&#125;//入队bool EnQueue(SqQueue &amp;Q,ElemType x)&#123; if((Q.rear+1)%MaxSize==Q.front) return false; else Q.data[Q.rear]=x; Q.rear=(Q.rear+1)%MaxSize; return true;&#125;//出队bool DeQueue(SqQueue &amp;Q,ElemType &amp;x)&#123; if(Q,rear==Q.front) return false; x=Q.data[Q.front]; Q.front=(Q.front+1)%MaxSize; return true;&#125; 队列的链式存储结构本质上是一个同时带有队头指针和队尾指针的单链表。头指针指向队头结点，尾指针指向队尾结点。 队列的链式存储类型可以描述如下：1234567typedef struct&#123; ElemType data; struct LinkNode *next;&#125;LinkNode;typedef struct&#123; LinkNode *front,*rear;&#125;LinkQueue; $Q.front==NULL$且$Q.rear==NULL$,链式队列为空出队：首先判空，若队列不为空，取出队头元素，删除，让$Q.front$指向下一个结点进队：建立新结点，将该结点插入链表尾部，并让$Q.rear$指向这个新插入的结点 通常将链式队列设计成一个带头结点的单链表，这样可以统一插入和删除操作。 链式队列的基本操作1234567891011121314151617181920212223242526272829303132//初始化void InitQueue(LinkQueue &amp;Q)&#123; Q.front=Q.rear=(LinkNode *) malloc (sizeof(LinkNode)); Q.front-&gt;next=NULL;&#125;//判断队列是否为空bool IsEmpty(LinkQueue Q)&#123; if(Q.front==Q.rear) return true; else return false;&#125;//入队void EnQueue(LinkQueue &amp;Q,ElemType x)&#123; s=(LinkNode *) malloc (sizeof(LinkNode)); s-&gt;data=x; s-&gt;next=NULL; Q.rear-&gt;next=s; Q.rear=s;&#125;//出队bool DeQueue(LinkQueue &amp;Q,ElemType &amp;x)&#123; if(Q.front==Q.rear) return false; p=Q.front-&gt;next; x=p-&gt;next; Q.front-&gt;next=p-&gt;next; if(Q.rear==p) Q.rear=Q.front; free(p); return true;&#125; 双端队列双端队列是指允许两端都可以进行入队和出队操作的队列。 进队：前端进的元素在后端进的元素前面出队：无论前端还是后端出队，先出的元素排在后出的元素前面输出受限：允许一端进行插入和删除，另外一端只能插入输入受限：允许一端进行插入和删除，另外一端只能删除 栈和队列的应用：（以后会详细讲解） 栈：括号匹配、表达式求值、递归、进制转换、迷宫求解… 队列：二叉树的层次遍历、缓冲区、页面替换算法、广度优先搜索图 特殊矩阵的压缩存储数组与线性表的关系：数组是线性表的推广。一维数组可以看做是一个线性表，二维数组可以看做元素是线性表的线性表。数组只有存取元素和修改元素操作。 数组的存储结构 矩阵的压缩存储]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>stack queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构之栈和队列（一）]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-3-31-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%A0%88%E5%92%8C%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[本文主要介绍2种操作受限的线性表结构：栈（Stack）和队列（Queue），包括它们的概念和存储结构。除此之外，还会简单介绍一下特殊矩阵的压缩存储。 栈（Stack）栈是只允许在一端进行插入或删除操作的线性表。它满足后进先出（LIFO）。 栈的基本操作： InitStack(&amp;S)：初始化 StackEmpty(S)：判断栈是否为空 Push(&amp;S,x)：进栈，若栈S未满，将x加入使之成为新的栈顶 Pop(&amp;S,x)：出栈，若栈S非空，弹出栈顶元素，并用x返回 GetTop(S,&amp;x)：读栈顶元素，若栈S非空，用x返回栈顶元素 ClearStack(&amp;S)：销毁栈，释放S存储空间 栈的顺序存储栈的顺序存储也称顺序栈，它利用一组地址连续的存储单元存放自栈底到栈顶的数据元素，同时设有一个指针（top）指示当前栈顶的位置。 栈的顺序存储类型描述如下：12345#define MaxSize 50typedef struct&#123; ElemType data[MaxSize]; int top;&#125;SqStack; 栈顶指针（top）初始值不同，进栈出栈操作也有所不同，栈顶指针初始值一般取-1或0，对应操作如下： 初始化栈123void InitStack(&amp;S)&#123; s.top==-1; //初始化栈顶指针&#125; 判栈空123456bool StackEmpty(S)&#123; if(s.top==-1) return true; //栈空 else return false;&#125; 进栈123456bool Push(SqStack &amp;S,ElemType x)&#123; if(s.top==MaxSize-1) //栈满报错 return false; S.data[++S.top]=x; //指针先加1，再进栈 return true;&#125; 出栈123456bool Pop(SqStack &amp;S,ElemType x)&#123; if(s.top==-1) //栈空报错 return false; x=S.data[S.top--]; //先出栈，指针再减1 return true;&#125; 读取栈顶元素123456bool GetTop(SqStack &amp;S,ElemType &amp;x)&#123; if(S.top==-1) //栈空报错 return false; x=S.data[S.top]; //x记录栈顶元素 return true;&#125; 栈的链式存储链式存储的栈又称链栈，优点是便于多个栈共享存储空间，提高效率，并且不存在栈满上溢情况。 常采用单链表实现，规定操作都是在单链表的表头进行 一般规定链栈没有头结点，Lhead指向栈顶元素 栈的链式存储类型描述如下：1234typedef struct Linknode&#123; ElemType data; //数据域 struct Linknode *next; //指针域&#125;*LiStack; 共享栈共享栈是为了高效的利用存储空间。利用栈底的位置相对不变这一特性，我们可以让2个顺序栈共享一个一维数据空间。这2个栈的栈底分别设在共享空间的两端，两个栈顶向共享空间延伸。 2个栈的进出栈操作如下图所示：]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>stack queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构之线性表（链式表示）]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-3-30-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A1%A82%2F</url>
    <content type="text"><![CDATA[在上篇博文中，我们介绍了线性表的顺序存储，本文将介绍其链式表示方式。由于顺序表的插入、删除操作都需要移动大量的元素，这极大的影响了运行效率，所以引进了线性表的链式表示。链式存储线性表时，不需要使用地址连续的存储单元，对线性表的插入删除操作只需要修改指针，不需要移动元素。 我们将介绍4种链表形式： 单链表 双链表 循环链表 静态链表 单链表线性表的链式存储又称单链表。它是通过一组任意的存储单元来存储线性表中的数据元素。对于每个链表结点，除了存放元素自身信息外（数据data），还需要存放一个指向其后继的指针（指针next）。 单链表中结点类型的描述如下：1234typedef struct LNode&#123; ElemType data; //数据域 struct LNode *next; //指针域&#125;LNode,*LinkList; 注意：单链表是非随机存取的存储结构，查找某个特定结点时，需要从表头开始遍历，依次查找。通常用头指针来标识一个单链表。为了操作上的方便，在单链表第一个结点之前附加一个结点，称为头结点。头结点的数据域可以不设任何信息，也可以记录表长等相关信息。头结点的指针域指向线性表的第一个元素结点。 头结点和头指针的区别： 不管带不带头结点，头指针始终指向链表的第一个结点 头结点是带头结点链表的第一个结点，结点内通常不存储信息 引入头结点的优点： 开始结点的位置被放在头结点的指针域中，链表的第一个位置上的操作和其他位置上的操作一致，无须进行特殊处理 无论链表是否为空，头指针都是指向头结点的非空指针（空表中头结点指针域为空），这样空表和非空表处理统一了 头插法建立单链表：O(n)该方法从一个空表开始，生成新结点，将新结点插入到当前链表的表头，如下所示： 1234567891011121314LinkList CreatList1(LinkList &amp;L)&#123; LNode *s;int x; L=(LinkList)malloc(sizeof(LNode)); //创建头结点 L-&gt;next=NULL; scanf("%d",&amp;x); //输入值 while(x!=9999)&#123; s=(LNode*)malloc(sizeof(LNode)); //创建新结点 s-&gt;data=x; s-&gt;next=L-next; L-next=s; //将新结点插入表L中 scanf("%d",&amp;x); &#125; return L;&#125; 采用头插法建立单链表，读入数据的顺序与生成的链表中的元素顺序是相反的。 尾插法建立单链表：O(n)将新结点插入到当前链表的表尾上，为此必须增加一个尾指针r，让它始终指向当前链表的尾结点。如下所示： 123456789101112131415LinkList CreatList2(LinkList &amp;L)&#123; int x; L=(LinkList)malloc(sizeof(LNode)); LNode *s,*r=L; //r为表尾指针 scanf("%d",&amp;x); while(x!=9999)&#123; s=(LNode*)malloc(sizeof(LNode)); s-&gt;data=x; r-&gt;next=s; r=s; //r指向新的表尾结点 scanf("%d",&amp;x); &#125; r-&gt;next=NULL; //尾结点指针置空 return L;&#125; 按序号查找结点值：O(n)从链表第一个结点开始，顺着指针域next向下搜索，直到找到第i个结点为止，否则返回最后一个结点指针域为NULL。12345678910111213LNode *GetElem(LinkList L,int i)&#123; int j=1; LNode *p=L-&gt;next; if(i==0) retuen L; if(i&lt;1) //i无效，返回NULL return NLLL; while(p&amp;&amp;j&lt;i)&#123; //顺序查找 p=p-&gt;next; j++; &#125; return p;&#125; 按值查找结点：O(n)从链表第一个结点开始，顺着指针域next向下搜索，若某结点的值等于给定值e，返回该结点的指针，否则返回NULL。123456LNode *LocateElem(LinkList L,ElemType e)&#123; LNode *p=L-&gt;next; while(p!=NULL&amp;&amp;p-&gt;data!=e) p=p-&gt;next; return p;&#125; 插入结点： （后插，将s插在p后）复杂度为O(n)将值为x的新结点s插入到单链表的第i个位置上。分为3步： 调用按序号查找算法GetElem(L,i-1),找到第i-1个结点，假设为p 让新插入结点s的指针域指向p的后继结点 最后让p结点的指针域指向新插入结点s123p=GetElem(L,i-1);s-&gt;next=p-&gt;next;p-&gt;next=s; 前插操作均可以转化为后插操作，只需要先找到待插入结点的前驱结点即可。时间主要耗费在查找前驱结点上。 插入结点： （前插，将s插在p前）可以通过上面的后插方法，除此之外，下面介绍另外一种方法O(1)。 仍然将s插入p的后面 将p-&gt;data和s-&gt;data交换12345s-&gt;next=p-&gt;next; //修改指针域p-&gt;next=s;temp=p-&gt;data; //交换数据域p-&gt;data=s-&gt;data;s-&gt;data=temp 删除结点操作：O(n)将单链表的第i个结点删除。先查找表中的第i-1个结点，即被删结点（q）的前驱结点（p），再将其删除。1234p=GetElem(L,i-1);q=p-&gt;next;p-&gt;next=q-&gt;next;free(q); 要实现删除某一个给定结点，通常是找到其前驱结点，然后执行删除操作即可，这样的时间复杂度为O(n)，如上面所示。 删除结点操作：O(1)其实，删除某一结点也可以通过删除它的后继结点实现。实质上就是将其后继结点的值赋予自身，然后删除后继结点，这样的时间复杂度为O(1)。 例如：删除给定结点p (q为p的后继结点) 1234q=p-&gt;next;p-&gt;data=p-&gt;next-&gt;data; //和后继结点交换数据p-&gt;next=q-&gt;next; //将结点q从链中断开 free(q); 双链表单链表中结点只有一个指向其后继的指针，这使得单链表只能从头结点依次顺序的向后遍历。如果想要访问某个节点的前驱结点（插入、删除），只能从头遍历。访问前驱结点的时间复杂度为O(n),访问后继结点的时间复杂度为O(1)。 双链表的结点描述如下：1234typedef struct DNode&#123; ElemType data; //数据域 struct DNode *prior,*next; //前驱后后继指针&#125;DNode,*DLinkList; 双链表仅仅在单链表的结点中增加了一个指向其前驱的prior指针，因此，双链表中执行按值查找和按位查找和单链表相同。但双链表在插入和删除操作上和单链表不同，这是因为链变的同时prior也要做出修改。双链表可以很快的找到其前驱结点，插入和删除的时间复杂度为O(1)。 插入操作（后插 ：将结点s插在p结点之后）1234s-&gt;next=p-&gt;next;p-&gt;next-&gt;prior=s;s-&gt;prior=p;p-&gt;next=s; 插入操作（前插 ：将结点q插在p结点之前）1234p-&gt;prior-&gt;next=q;q-&gt;next=p;q-&gt;prior=p-&gt;prior;p-&gt;prior=q; 删除操作：删除p的后继结点q123p-&gt;next=q-&gt;next;q-&gt;next-&gt;prior=p;free(q); 上面2节详细介绍了单链表和双链表的各种操作，接下来简要介绍一下循环链表和静态链表。 循环链表循环单链表：与单链表区别在于，表中最后一个结点的指针不是NULL，而是指向头结点。 判空条件：头结点的指针是否等于头指针。 任何一个位置上的插入、删除操作等价，无须判断是否为表尾。 可以从表中任意一个结点遍历整个链表。 循环双链表： 若结点p为尾结点，p-&gt;next=L 为空表时，头结点的prior和next都为L 静态链表 借助数组来描述线性表的链式存储结构，也有数据域和指针域 以next==-1作为结束标志 适合于不支持指针的高级语言（如Basic） 顺序表和链表的比较 存取方式 顺序表：可以顺序存取，也可以随机存取链表：只能从表头顺序存取 逻辑和物理结构 顺序表：逻辑相邻的，物理存储也相邻链表：逻辑相邻的，物理存储不一定相邻 查找、删除和插入操作 按值查找 按号查找 插入删除 顺序表 无序 O(n)；有序O($log_2^n$) O(1) 平均移动半个表长 链表 O(n) 平均O(n) 只需要修改相关结点指针域 空间分配 顺序表：静态（造成溢出）；动态（效率低）链表：灵活、高效 选取情况 顺序表： 线性表较稳定；按号访问为O(1)链表： 线性表长度、规模难以估计；需要频繁插入、删除操作时]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>linear list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构之线性表（顺序表示）]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-3-30-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E7%BA%BF%E6%80%A7%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[定义线性表是具有相同数据类型的$n(n&gt;=0)$个数据元素的有限序列。其中$n$为表长，当$n=0$时，线性表是一个空表。若用$L$命名线性表，则一般表示如下：$$L = (a_1,a_2,…,a_i,a_{i+1},..,a_n)$$其中，$a_1$是唯一的第一个数据元素，又称为表头元素；$a_n$是唯一的最后一个元素，又称为表尾元素。除第一个元素外，每个元素有且仅有一个直接前驱。除最后一个元素外，每个元素有且仅有一个直接后继。 特点： 表中元素个数有限且具有逻辑上的有序性 表中元素数据类型相同 表中元素具有抽象性，仅讨论元素逻辑关系，不考虑元素内容 划分： 顺序存储： 顺序表 链式存储： 单链表 （指针实现） 双链表（指针实现） 循环链表（指针实现） 静态链表（借助数组实现） 注意： 线性表是逻辑结构，表示元素之间一对一的相邻关系 顺序表和链表是指存储结构 线性表的基本操作 InitList(&amp;L)：初始化表 Length(L)：求表长 LocateElem(L,e)：按值查找 GetElem(L,i)：按位查找 ListInsert(&amp;L,i,e)：插入操作 ListDelete(&amp;L,i,&amp;e)：删除操作 PrintList(L)：输出 Empty(L)：判空 DestroyList(&amp;L)：销毁 线性表的顺序存储线性表的顺序存储又称顺序表，它是一组地址连续的存储单元，它的逻辑顺序和物理顺序相同。注意：线性表中的位序是从1开始的，而数组中元素的下标是从0开始的。 假定线性表的元素类型为ElemType，那么线性表的顺序存储类型可以描述如下：12345#define MaxSize 50 //定义线性表的最大长度typedef struct&#123; ElemType data[MaxSize]; //顺序表的元素 int length; //顺序表的当前长度&#125;SqList; //顺序表的类型定义 一维数组可以是静态分配的（数据可能溢出），也可以是动态分布的。C的初始动态分配语句为：1L.data = (ElemType*)malloc(sizeof(ElemType)*InitSize) 12345#define InitSize 100 //表长的初始定义typedef struct&#123; ElemType *data; // 动态分布数组的指针 int MaxSize,length; // 数组的最大容量和当前个数&#125;SeqList; 插入操作在顺序表L的第i个位置插入新的元素e 1234567891011bool ListInsert(SqList &amp;L,int i,ElemType e)&#123; if(i&lt;1||i&gt;L.length+1) //判断i范围是否有效 return false; if(L.length&gt;=MaxSize) //若当前空间已满，则不能插入 return false; for(int j=L.length;j&gt;=i;j--) //将第i个元素及后面的元素后移 L.data[j]=L.data[j-1] L.data[i-1]=e; //插入元素e L.length++; //表长加1 return true;&#125; 删除操作删除表L中第i个位置的元素，删除的元素用变量e返回 123456789bool ListDelete(SqList &amp;L,int i,&amp;e)&#123; if(i&lt;1||i&gt;L.length) //判断i范围是否有效 return false; e=L.data[i-1]; //将被删除的元素赋值给e for(j=i;j&lt;L.length;j++) //将第i个位置后的元素前移 L.data[j-1]=L.data[j] L.length--; //表长减1 return true;&#125; 按值查找在表L中查找第一个元素值等于e的元素，并返回其位序 1234567int LocateElem(SqList &amp;L, ElemType e)&#123; int i; for(i=0;i&lt;L.length;i++) if(L.data[i]==e) return i+1; return 0;&#125; 总结：上述三种操作的时间复杂度均为O(n)。 顺序表的特点： 随机访问，通过首地址和元素序号可以在O(1)的时间内找到指定元素 存储密度高，每个结点只存储数据元素 逻辑相邻元素物理上也相邻，插入和删除需要移动大量元素]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>linear list</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据结构之绪论]]></title>
    <url>%2F%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F2019-3-30-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E7%BB%AA%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[本文主要介绍数据结构中的一些基本知识，例如数据结构得划分、数据类型、算法等。 接下来的博客将详细介绍数据结构中的链表、栈和队列、树、查找、排序等算法。 数据结构 逻辑结构（算法设计） 线性结构：线性表、栈、队列（一对一） 非线性结构：树、图、集合（一对多、多对多） 存储结构（算法实现） 物理结构 数据的运算 数据元素是数据的基本单位。 数据类型 原子类型： （值不可再分） 结构类型： （值可在分） 抽象数据类型： （数据对象、数据关系、基本操作等） 线性结构 一般线性表 受限线性表 （栈、队列、串） 线性表推广 （数组、广义表） 有序表仅描述元素间的逻辑关系，属于逻辑结构。 二叉树和二叉排序树逻辑结构和存储结构相同，但数据运算不同。 算法 特性： 有穷性 确定性 可行性 输入（随便） 输出（至少一个） 效率： 时间复杂度：一般考虑最坏时间复杂度 空间复杂度：原地工作指算法所需的辅助空间为常量，O(1) 大小关系：$O(1)&lt;O(log_{2}^{n})&lt;O(n)&lt;O(nlog_{2}^{n})&lt;O(n^2)&lt;O(n^3)&lt;O(2^n)&lt;O(n!)&lt;O(n^n)$ 整体内容如下：]]></content>
      <categories>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>data structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用深度学习对医学CT图像(LIDC-IDRI)中的肺结节进行良恶性判断(二)]]></title>
    <url>%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%2F2019-3-29-%E5%88%A9%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AF%B9%E5%8C%BB%E5%AD%A6CT%E5%9B%BE%E5%83%8F%E4%B8%AD%E7%9A%84%E8%82%BA%E7%BB%93%E8%8A%82%E8%BF%9B%E8%A1%8C%E8%89%AF%E6%81%B6%E6%80%A7%E5%88%A4%E6%96%AD2%2F</url>
    <content type="text"><![CDATA[在上篇博文中，我们详细介绍了如何分割肺实质，并根据标注信息提取肺结节，本文主要介绍如何利用CNN网络训练分类模型，辅助医生作出判断。 本文网络结构见论文： http://downloads.hindawi.com/journals/jhe/2017/8314740.pdf 数据集通过上篇博文中的方法，我们最后根据肺结节的良恶性程度（1-5）得到了5类肺结节。数目如下： 良恶性程度为1：1254 良恶性程度为2：1532 良恶性程度为3：1721 良恶性程度为4：1226 良恶性程度为5：1646 之前介绍过，3表示不确定，所以我们训练时舍弃该数据集。我们只需要判断良恶性，因而我们将良恶性程度为1和2归为良性（Malignant），良恶性程度为4和5归为另一类恶性（benign）。主要采取的网络有（CNN、DNN和SAE）。 网络模型 CNN的网络结构和参数 DNN的网络结构和参数 SAE的网络结构和参数 最后在验证集上的测试结果如下： 通过比较可以发现，CNN的分类效果最好。 拓展尽管CNN模型已经取得了不错的分类效果，但还需要提升。考虑到分类数据集还是太少，我们引入了生成式对抗网络（GAN）来扩充我们的肺结节数据集。关于GAN的介绍可以看我之前的一篇博客深度学习的常见模型 （GAN）,此处不详细介绍。 我们通过CGAN网络训练了生成模型，扩充了我们的数据集，生成示例如下： 然后将生成的数据与原始数据混合，重新训练CNN分类模型，在准确率上果然取得了一定的提升。]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用深度学习对医学CT图像(LIDC-IDRI)中的肺结节进行良恶性判断]]></title>
    <url>%2F%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%2F2019-3-29-%E5%88%A9%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AF%B9%E5%8C%BB%E5%AD%A6-CT-%E5%9B%BE%E5%83%8F%E4%B8%AD%E7%9A%84%E8%82%BA%E7%BB%93%E8%8A%82%E8%BF%9B%E8%A1%8C%E8%89%AF%E6%81%B6%E6%80%A7%E5%88%A4%E6%96%AD%2F</url>
    <content type="text"><![CDATA[肺癌是最常见的癌症，目前，CT可用于帮助医生在早期阶段检测肺癌。 在许多情况下，识别肺癌的诊断取决于医生的经验，这可能会忽略一些患者并导致一些问题。 在许多医学影像诊断领域，深度学习已被证明是一种流行且有效的方法。 本文主要基于LIDC-IDRI这一公开数据集，对其进行了肺结节的提取，并利用CNN对其分类训练，从而辅助医生作出判断。由于篇幅较长，将分为2篇博客，这篇主要介绍数据处理，即肺结节的提取。 数据集数据集采用为 LIDC-IDRI （The Lung Image Database Consortium），该数据集由胸部医学图像文件(.dcm)(如CT、X光片)和对应的诊断结果病变标注(.xml)组成。数据是由美国国家癌症研究所(National Cancer Institute)发起收集的，目的是为了研究高危人群早期癌症检测。该数据集中，共收录了1018个研究实例。对于每个实例中的图像，都由4位经验丰富的胸部放射科医师进行两阶段的诊断标注。在第一阶段，每位医师分别独立诊断并标注病患位置，其中会标注三中类别： $&gt;=$3mm的结节 $&lt;$3mm的结节 $&gt;=$3mm的非结节 在随后的第二阶段中，各位医师都分别独立的复审其他三位医师的标注，并给出自己最终的诊断结果。这样的两阶段标注可以在避免forced consensus的前提下，尽可能完整的标注所有结果。 图像信息（.dcm）图像文件为Dicom格式，是医疗图像的标准格式，其中除了图像像素外，还有一些辅助的元数据如图像类型、图像时间等信息。一张CT图像有 512x512 个像素点，在dicom文件中每个像素由2字节表示，所以每张图片约512KB大小。目前测试一共1012个病例数据，对于每个实例，可以看为一个三维矩阵D(slicer rows cols), slicer表示切片的个数(对应每个病例的.dcm文件数)，rows和cols分别表示图片的行数和列数(默认为512)。eg: 对于病例LIDC-IDRI-0001，即为$133 \times 512 \times 512$的矩阵，一共133张切片，每张大小$512 \times 512$。 查看dcm文件： 通过pip或者Anaconda安装pydicom模块，该模块是python专门用来处理dicom格式文件的库。 通过软件MicroDicom viewer 通过上面2种方式，我们可以看出dicom文件中包含了一些图像信息（SOP Instance UID、Study Instance UID，Series Instance UID······）SOP Instance UID：用于唯一区分每一张dcm切片Study Instance UID: 每个病例对应的检查实例号Series Instance UID: 不同检查对应的序列实例号 注释信息（.xml）Xml文件中包含放射科医生对病人CT图像中疑似肺结节的标注信息，主要分为三类： 结节(3mm-33mm):包含结节的特征信息（characteristics）、结节的完整轮廓(roi) 结节（&lt;3mm）：只显示结节的近似三维重心，若不透明则不标记 非结节（&gt;3mm）：只显示其近似的三维重心，指出非结节连接区域 Xml文件大体结构图如下： 其中对于3mm—33mm结节的characteristics，包含了如下信息：1） Subtlety：检测难度（1-5级，1最难，5最明显）2） internalStructure：内部结构（4种，软组织、液体、脂肪、空气）3） calcification：钙化（6种情况）4） sphericity：球形度（5种程度，但只明确3种）5） margin：边缘（5种程度）6） lobulation：分叶征（5种情况，但只明确2种）7） spiculation：毛刺征（5种情况，但只明确2种）8） texture：纹理（5种情况，但只明确3种） 9） maliynancy：恶性程度（1-5，1最低，5最高） 数据预处理本部分主要做的工作是分割肺实质，提取肺结节。 图像存储格式转换原始数据集的图像信息是以dcm格式存储的，但通常我们用作训练数据输入网络的图像大多是jpg或者png格式，所以为了方便以后的训练，我们首先要将原始图像转为jpg格式或者png格式存储，在这里我们是转为jpg格式存储的。此处测试共包含1012个病例，每个病例包含约100—300个dcm文件，我们使用MicroDicom viewer软件对其进行批量转换。以LIDC-IDRI-0001 为例： 原始数据转换后 将原始数据转换为jpg格式的图片后，下面我们会利用matlab编写函数分割肺实质，提取肺结节，主要包含下面6个函数： find_files(): 递归的遍历文件目录 fengefeishizhi(): 分割肺实质 readxml(): 读取标注信息(.xml)文件 readdicom() 存储标注信息中的肺结节信息，方便后面提取 jianqieimage(): 剪切肺结节 jianqie(): 根据肺结节的轮廓信息将其剪切出来，存为图片 分割肺实质将图像转换为jpg格式存储后，我们还要对数据进一步处理。由于我们最终是以肺结节图像作为训练数据输入网络，那么CT图像中除肺部以外的信息是无用的，所以我们要将肺实质分割出来。主要用到1个函数： fengefeishizhi()。代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133clear all;clc;ticfor q =2:3 str1 = num2str(q); %jpg数据格式的存储路径 str2 = 'D:\MATLAB\work\0001-0120\LIDC-IDRI-000'; str3 ='\*.jpg'; str4 ='\'; %分割好肺实质后的图片存储路径 str5='D:\MATLAB\work\0001-0120_fenge\000'; str_imagedir = strcat(str2,str1,str3); str_dirname = strcat(str2,str1,str4); str_write= strcat(str5,str1,str4); %disp(str_imagedir) %disp(str_dirname) %disp(str_write)7 imagelist = dir(str_imagedir); for i = 1:length(imagelist) name = imagelist(i).name; dirname = [str_dirname,name];%B=imread(dirname);%读取原图像% B=rgb2gray(A);%将原图像转换为灰度图像 A=imread(dirname); B=rgb2gray(A);%subplot(2,2,1),imshow(B,[]),title('DICOM图像导入后显示');% figure,imshow(B),title('图像导入后显示');%==================================================== min(min(B)); max(max(B)); t=graythresh(B);%计算阈值t C=im2bw(B,t);%根据阈值二值化图像% figure(),imshow(C,[]),title('显示二值化图像');% C=bwareaopen(C,6000);%去除面积小于T的部分（气管）。%%%%%%%%%在肺实质比较大的时候，而且操作床特殊分段构造，面积为10000 D=imfill(C,4,'holes');%对二值化后的图像填充肺实质% figure(),imshow(D,[]),title('显示填充肺实质图像'); E=D-C;%得到肺实质的图像E% figure(),imshow(E,[]),title('显示肺实质的图像'); F=imfill(E,8,'holes');%填充肺实质空洞% FMask=bwareaopen(F,1000);%去除面积小于T的部分（气管）。%%%%%%%%%在肺实质比较大的时候，而且操作床特殊分段构造，面积为4600 FMask=bwareaopen(F,6000);%去除面积小于T的部分（气管）。%%%%%%%%%在肺实质比较大的时候，而且操作床特殊分段构造，面积为4600% figure(),imshow(FMask,[]),title('显示掩摸');%得到掩膜%-------------------------分开左右肺---------------------------------------- r_ball=90;%可变的，取值为10/15,越小越细致 se_ball=strel('ball',r_ball,10);%椭圆体半径10，高度10 r_disk=ceil(r_ball/6);%圆整r_ball/6得到大于或等于它的最接近整数。ceil取整 if r_disk==0; r_disk=1;%最小为1 end se_erode=strel('disk',r_disk,0); %圆形半径 mask=imopen(FMask,1);%开操作% figure(),imshow(mask,[]); L=bwlabel(FMask); %数学形态重建，基于膨胀运算，用掩摸对二值图像标记，将图像分成多个区域%stat = regionprops(FMask);%,计算图像区域特征，区域连通，object为二值图像， [row,col]=size(B);%im2bw，Convert image to binary image, based on threshold%im2bw默认threshold0.5，得到512*512空矩阵 mask_leftlung=im2bw(zeros(row,col));%左肺掩膜 mask_rightlung=im2bw(zeros(row,col));%右肺掩膜 for i=1:row for j=1:col if L(i,j)==1 %如果是左肺 mask_leftlung(i,j)=1;% 分开左右肺，肺是白色的 end if L(i,j)==2 mask_rightlung(i,j)=1; end end end% figure(),imshow(mask_leftlung,[]);title('左肺掩摸显示')% figure(),imshow(mask_rightlung,[]);title('右肺掩摸显示')%----------------------对左肺修补------------------------------------------- object1=1-mask_leftlung; %左肺反向% figure();imshow(object1,[]);title('左肺反向后显示') object2=imopen(object1,se_ball);%开操作，椭圆体半径30，高度10% figure();imshow(object2,[]);title('反向左肺模糊重影图显示') %得到反向左肺模糊重影图 leftmask1=1-object2;%左肺模糊重影图 % figure();imshow(leftmask1,[]);title('左肺模糊重影图显示') leftmask2=im2bw(leftmask1,0.5);%根据阈值0.5将图像生成二值图像%figure();imshow(leftmask2,[]);title('左肺清晰二值图像显示')%%得到左肺清晰的二值图像，支气管消去了，结节的毛刺也消除，结节变小；对左肺进行了修补 leftmask3=imfill(leftmask2,'hole'); %填充左肺实质空洞% figure();imshow(leftmask3,[]),title('填充左肺实质后显示'); %只是填充了左肺实质，得到不平滑的左肺图像 leftmask4=imerode(leftmask3,se_erode);%腐蚀左肺操作，肺结节大了点，平滑作用% figure();imshow(leftmask4,[]),title('leftlungmask');%得到平滑效果图像%---------------------补回空洞---------------------------------------------- ConvHull=bwconvhull(leftmask4,'object');%对左肺掩摸求凸壳%figure();imshow(ConvHull,[]),title('凸壳图像'); DIF_ConvHull=ConvHull-leftmask4;%将补的缺口部分取出来%figure();imshow(DIF_ConvHull,[]),title('与左肺原图差值图像'); BW1 = bwconncomp(DIF_ConvHull);%利用连通域分析左肺凸壳 stats = regionprops(BW1, 'Area','Eccentricity');%获得每个连通域得面积、离心率 idx = find([stats.Area] &gt; 80 &amp; [stats.Eccentricity] &lt; 0.8); % % % % BW2 = ismember(labelmatrix(BW1), idx);%取出符合要求的区域% % % % figure();imshow(BW2,[]),title('左肺所需要补的部分显示');% % % % leftmask5=BW2+leftmask4;%将符合要求的区域“补”到左肺掩摸中%figure();imshow(leftmask5,[]),title('显示最终的左肺掩摸');%---------------------对右肺修补-------------------------------------------- object1=1-mask_rightlung; %反转右肺轮廓 %figure();imshow(object1,[]);title('右肺反向后显示') object2=imopen(object1,se_ball);%开操作 %figure();imshow(object2,[]);title('反向右肺模糊重影图显示') %得到反向右肺模糊重影图 rightmask1=1-object2;%得到右肺模糊掩膜，反转回来，实质为白色 %figure();imshow(rightmask1,[]);title('右肺模糊重影图显示') rightmask2=im2bw(rightmask1,0.5);%右肺转换为二值图像 %figure();imshow(rightmask2,[]);title('右肺清晰二值图像显示') rightmask3=imfill(rightmask2,'hole');%填充右肺实质空洞 %figure();imshow(rightmask3,[]),title('填充右肺实质后显示'); rightmask4=imerode(rightmask3,se_erode);%腐蚀操作，平滑作用% figure();imshow(rightmask4,[]),title('rightlungmask'); % % % lungmask=im2bw(leftmask5+rightmask4);%将左右肺合并，得到全肺掩膜 lungmask=im2bw(leftmask4+rightmask4);%将左右肺合并，得到全肺掩膜 lung=immultiply(lungmask,B);%相与,得到的是灰度值从0到max-min+1的灰度图像 %dicomwrite(lung,'E:\1_毕业设计\images_CT\S60\I00');%dicomwrite()函数将lung（从源图像提取出来的肺实质）图像保存为dicom文件格式，方便下次使用 %subplot(2,2,2),imshow(lung,[]),title('提取的肺实质'); %figure;imshow(lung,[]),title('提取的肺实质');%name = + name; feishizhi = [str_write,name]; imwrite(lung,feishizhi);%break endend 以LIDC-IDRI-0001 中的部分切片为例，其中左侧为原始CT图像，右侧为分割肺实质后的图像，效果图如下: 读取标注信息并存储我们从医生的标注信息文件（.xml）读取肺结节的位置信息和良恶性程度，然后存储到对应的xls表中。主要用到2个函数： readxml()和readdicom()代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172function [num_mal,sop_text,max_min_xy]=zl_readxml(xml_path)% % function [sop_text,max_min_xy]=zl_readxml(xml_path)% clear all% clc%xml_path = 'H:\肺结节\数据\LIDC-IDRI\900-300\LIDC-IDRI\LIDC-IDRI-0060\1.3.6.1.4.1.14519.5.2.1.6279.6001.203745372924354240670222118382\1.3.6.1.4.1.14519.5.2.1.6279.6001.463214953282361219537913355115\191.xml';%% 跳转到内层标签unblindedReadNoduledocNode = xmlread(xml_path); %读取XML文件返回一个文件模型节点* document = docNode.getDocumentElement();readingSession = document.getElementsByTagName('readingSession'); %返回与给定的元素所有子节点的Nodelist对象*%% 最后返回的三个值%% 最后返回的三个值num_mal = []; %每个结节的恶性度和属于该类别的图片的数量sop_text = &#123; &#125;; %每个图片的标号max_min_xy = []; %每个图像中肺结节的x和y的最小值和最大值sop_num = 0; %总结节个数？*%%for r = 0:readingSession.getLength()-1 unblinded_nodule = readingSession.item(r).getElementsByTagName('unblindedReadNodule'); %unblindedReadNodule一个节点标记，&lt;unblindedReadNodule&gt;节点数据包括在&lt;/unblindedReadNodule&gt;* for u = 0 : unblinded_nodule.getLength()-1 roi = unblinded_nodule.item(u).getElementsByTagName('roi'); %item() 方法可返回节点列表中处于指定索引号的节点。*&lt;roi&gt;结节轮廓&lt;/roi&gt;* mal = unblinded_nodule.item(u).getElementsByTagName('malignancy'); %&lt;malignancy&gt;结节恶性度&lt;/malignancy&gt;* %如果xml文件中没有malignancy或者roi标签直接跳过 if isempty(roi.item(0)) continue; end if isempty(mal.item(0)) continue; end Num_roi = roi.getLength(); %该类别的图片的数量 mal_int = str2num(char(mal.item(0).getTextContent())); num_mal = [num_mal();mal_int,Num_roi]; for i = 0 : Num_roi-1 %遍历* sop_id = roi.item(i).getElementsByTagName('imageSOP_UID'); %图片编号* sop_text&#123;sop_num + i + 1&#125; = char(sop_id.item(0).getTextContent()); %数组* edgeMap = roi.item(i).getElementsByTagName('edgeMap'); %边界* xy = []; for j = 0 :edgeMap.getLength()-1 %获得坐标* xCoord = edgeMap.item(j).getElementsByTagName('xCoord'); xCoord_int = str2num(char(xCoord.item(0).getTextContent())); yCoord = edgeMap.item(j).getElementsByTagName('yCoord'); yCoord_int = str2num(char(yCoord.item(0).getTextContent())); xy=[xy();xCoord_int,yCoord_int]; end %找到结节轮廓* if edgeMap.getLength()==1 max_min_xy = [max_min_xy();xy,xy]; continue; end [maxr,max_index] = max(xy); [minr,min_index] = min(xy); max_min_xy = [max_min_xy();minr,maxr]; end sop_num = sop_num + Num_roi; %总个数 end if isempty(num_mal) continue; end num_mal = [num_mal();0,0]; %扩展维数*endend 上述是辅助函数，提取函数如下所示：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384clear;clc; %% Each treatment 100 or 200 %处理数据导入到表格中%LIDC_path = 'E:\zhaolei\深度学习\肺结节\400-499\LIDC-IDRI\'; 原路径LIDC_path = 'D:\MATLAB\tiqu\LIDC-IDRI';%IDRI_path = 'H:\肺结节\数据\LIDC-IDRI\'; 原路径%XLS_path = 'H:\肺结节\数据\excel\excel_all'; 原路径IDRI_path = 'D:\MATLAB\tiqu\LIDC-IDRI';XLS_path = 'D:\MATLAB\tiqu\xls';IDRI_child_path = dir(IDRI_path); %打开文件目录并返回文件结构体*num_IDRI_child = size(IDRI_child_path); %返回列和行数的数组* %for n = 8 :num_IDRI_child 原版 %为啥从8开始？？？for n = 3 :num_IDRI_child %非原版 % child_idri_path = [IDRI_path,IDRI_child_path(n).name];原版（可能有错） child_idri_path = [IDRI_path,'\',IDRI_child_path(n).name]; %非原版 child_idri_path_temp = dir(child_idri_path); %打开文件* LIDC_path = [child_idri_path,'\',child_idri_path_temp(3).name]; %文件目录* LIDC_child_path = dir(LIDC_path); %打开 num_child = size(LIDC_child_path); %返回文件的列和行数的数组* for i = 3 : num_child(1) %从3开始（前两个是. ..） %% find dicom file list child_path = [LIDC_path,'\',LIDC_child_path(i).name]; %一步步打开文件夹 child_path_temp = dir(child_path); child_path1 = [child_path,'\',child_path_temp(3).name]; child_path_temp = dir(child_path1); %xml_path = [child_path1,'\',child_path_temp(3).name]; xml_path = [child_path,'\']; %获取单个文件夹中的dicom和xml文件 dcm_files = find_files(xml_path, '.dcm'); % 获得文件列表 xml_files = find_files(xml_path, '.xml'); xml_path = char(xml_files); [num_mal,sop_text,max_min_xy]=zl_readxml(xml_path); %函数调用 % num_mal = []; %每个结节的恶性度和属于该类别的图片的数量 % sop_text = &#123; &#125;; %每个图片的标号 % max_min_xy = []; %每个图像中肺结节的x和y的最小值和最大值 sop_num = size(sop_text); % 获得行列数，行：？ 列：图片数* mal_num = size(num_mal); %行： 图片数？* dcm_number = [ ]; %图片编号* %?? if sop_num(2)&gt;mal_num(1) %要根据他们两个的差值来决定补多少个0 for m = 1 : sop_num(2)-mal_num(1) num_mal = [num_mal();0,0]; %添加扩展维度* end end if sop_num(2)&lt; mal_num(1) for m = 1 : mal_num(1) - sop_num(2) % 只有数据维度一样才能被写入到文件中！所以少的要补上四个0 dcm_number= [dcm_number;0]; %添加扩展维度 max_min_xy = [max_min_xy;0,0,0,0]; %添加扩展维度 end end %?? %% Get the number and file name of the image In a single folder for md = 1 : sop_num(2) %??? dcm_number= [dcm_number;0]; end for j = 1:numel(dcm_files) %遍历文件 dicomInformation = dicominfo(dcm_files&#123;j&#125;); %存储图片信息 instance = dicomInformation.SOPInstanceUID; imagenum = dicomInformation.InstanceNumber; % Make sure that the StudyInstanceUID matches that found in % the XML annotations for s = 1 : sop_num(2) %对比 if strcmpi(instance,sop_text(1,s)) dcm_number(s) = imagenum; %编号？？?* end end end total = [num_mal,dcm_number,max_min_xy]; if isempty(total) continue; end child_path = [XLS_path,'\',LIDC_child_path(i).name] xlswrite(child_path,total); %导入到表格中 2017/4/10 endend 提取肺结节读取到肺结节的位置信息和良恶性程度后，我们要根据该信息提取肺结节。主要用到2个函数： jianqieimage()和jianqie()代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546function zl_jianqie(img_path,dir,times,size_center,col4xy) %dir 为患病可能程度，col4xy为剪切区域 train_path = 'I:\肺结节\数据\result2\train23jpg\'; result_name = [img_path(24:37),'_',char(num2str(dir)),'_',char(num2str(times)),img_path(38:46)]; train_path = [train_path,char(num2str(dir)),'\',result_name]; %剪切路径* img=imread(img_path); %读取图片文件* img1=imcrop(img,col4xy); %返回图像的一个裁剪区域* I2=imcrop(I,[a b c d]);%利用裁剪函数裁剪图像，其中， %（a,b）表示裁剪后左上角像素在原图像中的位置；c表示裁剪后图像的宽，d表示裁剪后图像的高 %% 分割肺结节实质 img1_size = size(img1); min(min(img1)); % 找到最小值，最大值 max(max(img1)); t=graythresh(img1); %使用最大类间方差法找到图片的一个合适的阈值threshold C=im2bw(img1,t); %转换为二值图像* D=imfill(C,4,'holes');%对二值化后的图像填充肺实质 if dir &gt;=4 %大概率为肺癌* FMask=bwareaopen(D,10); % 除二值图像中面积小于10的对象 D = FMask; end total = 0; for i = 1:img1_size(1) %行数 for j = 1:img1_size(2) %列数 if D(i,j) == 0 %二值图像当值为0时 （黑色） img1(i,j) = 0; end if D(i,j) == 1 %二值图像当值为1时 （白色） if ~(i &gt; size_center(1) &amp;&amp; j &gt; size_center(1)&amp;&amp; j &lt; size_center(1) + size_center(3)&amp;&amp; i &lt; size_center(1) + size_center(3)) %不在范围内*？ img1(i,j) = 0; %取为黑色* end end end end %% 保存图片 for m = 1:img1_size(1) for n = 1:img1_size(2) if img1(m,n) == 0 %黑色元素点个数* total = total + 1; end end end if total ~= img1_size(1)*img1_size(2) %如果不全是黑* imwrite(img1,train_path); %存入图片* endend 12345678910111213141516171819202122232425262728293031323334clear;clc;%肺实质的图片image_path = 'I:\肺结节\数据\result2\jpg2\';%肺结节的位置信息和良恶性程度xls_path = 'I:\肺结节\数据\result2\result22.xls';[txt,xls_text] = xlsread(xls_path);xls_num = size(xls_text);xls_num(1);for m = 1:xls_num(1) img_name = xls_text(m,1); str = img_name&#123;1&#125;; img_name = [str,'.jpg']; jpg_child_path = [image_path,img_name] if exist(jpg_child_path,'file') col4x = txt(m,4) - txt(m,2); col4y = txt(m,5) - txt(m,3); dir = txt(m,6); times = txt(m,7); size_center = [ ]; if col4x &lt; 32 &amp;&amp; col4y &lt; 32 ma = 0.5 * (32 - max(col4x,col4y)); col4xy = [txt(m,2)-ma,txt(m,3)-ma,32,32]; size_center =[ma,ma,max(col4x,col4y)]; zl_jianqie(jpg_child_path,dir,times,size_center,col4xy); continue; end size_center =[0,0,max(col4x,col4y)]; col4xy = [txt(m,2),txt(m,3),max(col4x,col4y),max(col4x,col4y)]; zl_jianqie(jpg_child_path,dir,times,size_center,col4xy); end % break;end 通过上面的函数，我们可以将肺结节提取出来，并按照良恶性程度分类存储。部分示例如下： 其中1-5表示肺结节的良恶性程度，5表示恶性可能性最大，1表示恶性可能性最小。注意：3表示不确定是否为肺结节，良恶性程度也不确定。 至此，我们的数据预处理结束了，分类训练见下一篇博文。]]></content>
      <categories>
        <category>项目实战</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN网络发展史]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-3-28-CNN%E7%BD%91%E7%BB%9C%E5%8F%91%E5%8F%91%E5%B1%95%E5%8F%B2%2F</url>
    <content type="text"><![CDATA[卷积神经网络可谓是现在深度学习领域中大红大紫的网络框架，尤其在计算机视觉领域更是一枝独秀。 CNN从90年代的LeNet开始，21世纪初沉寂了10年，直到12年AlexNet开始又再焕发第二春，从ZF Net到VGG，GoogLeNet再到ResNet和最近的DenseNet，网络越来越深，架构越来越复杂，解决反向传播时梯度消失的方法也越来越巧妙。本文总结一波CNN的各种经典架构。 转载自：CNN网络架构演进]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[马氏距离和欧式距离详解]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-27-%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB%E5%92%8C%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[一般在机器学习模型中会涉及到衡量两个样本间的距离，如聚类、KNN，K-means等，使用的距离为欧式距离。其实，除了欧氏距离之外，还有很多的距离计算标准，本文主要介绍欧氏距离和马氏距离。 欧氏距离最常见的两点之间或多点之间的距离表示法，又称之为欧几里得度量，它定义于欧几里得空间中，如点 $x = (x_1,…,x_n)$ 和 $y = (y_1,…,y_n)$ 之间的距离为：$$d(x,y) = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2+…+(x_n-y_n)^2} = \sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}$$ 二维平面上两点a(x1,y1)与b(x2,y2)间的欧氏距离：$$d_{12} = \sqrt{(x_1-y_1)^2+(x_2-y_2)^2}$$ 两个n维向量$a(x_{11},x_{12},…,x_{1n})$与 $b(x_{21},x_{22},…,x_{2n})$间的欧氏距离：$$d_{12} = \sqrt{\sum_{k=1}^{n}(x_{1k}-x_{2k})^2}$$ 马氏距离在介绍马氏距离之前，我们先来看如下几个概念： 方差：方差是标准差的平方，而标准差的意义是数据集中各个点到均值点距离的平均值。反应的是数据的离散程度。 协方差：标准差与方差是描述一维数据的，当存在多维数据时，我们通常需要知道每个维数的变量中间是否存在关联。协方差就是衡量多维数据集中，变量之间相关性的统计量。比如说，一个人的身高与他的体重的关系，这就需要用协方差来衡量。如果两个变量之间的协方差为正值，则这两个变量之间存在正相关，若为负值，则为负相关。 协方差矩阵：当变量多了，超过两个变量了。那么，就用协方差矩阵来衡量这么多变量之间的相关性。假设 $X$ 是以 $n$个随机变数（其中的每个随机变数是也是一个向量，当然是一个行向量）组成的列向量： 其中，$μ_i$是第i个元素的期望值，即$μ_i=E(X_i)$。协方差矩阵的第$i,j$项（第$i,j$项是一个协方差）被定义为如下形式：$$\sum_{ij} = cov(X_i,X_j = E[(X_i-\mu_i)(X_j-\mu_j)])$$即： 矩阵中的第 $(i,j)$ 个元素是 $X_i$ 与 $X_j$ 的协方差。 马氏距离的定义：马氏距离（Mahalanobis Distance）是由马哈拉诺比斯（P. C. Mahalanobis）提出的，表示数据的协方差距离。它是一种有效的计算两个未知样本集的相似度的方法。与欧氏距离不同的是它考虑到各种特性之间的联系（例如：一条关于身高的信息会带来一条关于体重的信息，因为两者是有关联的）并且是尺度无关的（scale-invariant），即独立于测量尺度。对于一个均值为$μ=(μ_1,μ_2,μ_3,…,μ_p)^T$，协方差矩阵为$S$的多变量$x=(x_1,x_2,x_3,…,x_p)^T$，其马氏距离为：$$D_M(x) = \sqrt{(x-\mu)^T {S}^{-1}(x-\mu)}$$我们可以发现如果$S^{-1}$是单位阵的时候，马氏距离简化为欧氏距离。 那我们为什么要用马氏距离呢？马氏距离有很多优点： 马氏距离不受量纲的影响，两点之间的马氏距离与原始数据的测量单位无关；由标准化数据和中心化数据(即原始数据与均值之差）计算出的二点之间的马氏距离相同。马氏距离还可以排除变量之间的相关性的干扰。 下面我们来看一个例子：如果我们以厘米为单位来测量人的身高，以克（g）为单位测量人的体重。每个人被表示为一个两维向量，如一个人身高173cm，体重50000g，表示为（173,50000），根据身高体重的信息来判断体型的相似程度。 我们已知小明（160,60000）；小王（160,59000）；小李（170，60000）。根据常识可以知道小明和小王体型相似。但是如果根据欧几里得距离来判断，小明和小王的距离要远远大于小明和小李之间的距离，即小明和小李体型相似。这是因为不同特征的度量标准之间存在差异而导致判断出错。 以克（g）为单位测量人的体重，数据分布比较分散，即方差大，而以厘米为单位来测量人的身高，数据分布就相对集中，方差小。马氏距离的目的就是把方差归一化，使得特征之间的关系更加符合实际情况。 下图（a）展示了三个数据集的初始分布，看起来竖直方向上的那两个集合比较接近。在我们根据数据的协方差归一化空间之后，如图（b），实际上水平方向上的两个集合比较接近。 深入分析：当求距离的时候，由于随机向量的每个分量之间量级不一样，比如说x1可能取值范围只有零点几，而x2有可能时而是2000，时而是3000，因此两个变量的离散度具有很大差异马氏距离除以了一个方差矩阵，这就把各个分量之间的方差都除掉了，消除了量纲性，更加科学合理。 如上图，看左下方的图，比较中间那个绿色的和另外一个绿色的距离，以及中间绿色到蓝色的距离 如果不考虑数据的分布，就是直接计算欧式距离，那就是蓝色距离更近 但实际上需要考虑各分量的分布的，呈椭圆形分布 蓝色的在椭圆外，绿色的在椭圆内，因此绿色的实际上更近 马氏距离除以了协方差矩阵，实际上就是把右上角的图变成了右下角 参考资料：马氏距离通俗理解]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>distance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习中的Bagging和Boosting]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-25-%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84Bagging%E5%92%8CBoosting%2F</url>
    <content type="text"><![CDATA[在机器学习和统计学习中, 集成学习(Ensemble Learning)是一种将多种学习算法组合在一起以取得更好表现的一种方法。它本身不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。也就是我们常说的“博采众长”。集成学习可以用于分类问题集成，回归问题集成，特征选取集成，异常点检测集成等。 集成学习概述什么是集成学习(此处以分类为例) 将多个分类方法聚集在一起，以提高分类的准确率（可以是相同or不同算法） 集成学习法由训练数据构建一组基分类器，然后通过对每个基分类器的预测进行投票来进行分类 严格来说，集成学习并不算是一种分类器，而是一种分类器结合的方法。 如果把单个分类器比作一个决策者的话，集成学习的方法就相当于多个决策者共同进行一项决策。 我们可以对集成学习的思想做一个概括（见下图）。对于训练集数据，我们通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的。 也就是说，集成学习有两个主要的问题需要解决: 如何得到若干个个体学习器 如何选择一种结合策略，将这些个体学习器集合成一个强学习器。 对于第一个问题，如何得到若干个个体学习器，一般有2种选择 所有的个体学习器都是一个种类的,或者说是同质的。比如都是决策树个体学习器，或者都是神经网络个体学习器。（广泛） 所有的个体学习器不全是一个种类的，或者说是异质的。比如我们有一个分类问题，对训练集采用支持向量机个体学习器，逻辑回归个体学习器和朴素贝叶斯个体学习器来学习，再通过某种结合策略来确定最终的分类强学习器。 目前来说，同质个体学习器的应用是最广泛的，一般我们常说的集成学习的方法都是指的同质个体学习器。而同质个体学习器使用最多的模型是CART决策树和神经网络。同质个体学习器按照个体学习器之间是否存在依赖关系可以分为两类： 第一个是个体学习器之间存在强依赖关系，一系列个体学习器基本都需要串行生成，代表算法是Boosting系列算法; 第二个是个体学习器之间不存在强依赖关系，一系列个体学习器可以并行生成，代表算法是Bagging和随机森林（Random Forest）系列算法。 集成学习之BoostingBoosting的算法原理图如下： 从图中可以看出，Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，使得之前弱学习器1学习误差率高的训练样本点的权重变高，让这些误差率高的点在后面的弱学习器2中得到更多的重视。然后基于调整权重后的训练集来训练弱学习器2.，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。Boosting系列算法里最著名算法主要有AdaBoost算法和提升树(boosting tree)系列算法。提升树系列算法里面应用最广泛的是梯度提升树(Gradient Boosting Tree)。 集成学习之BaggingBagging的算法原理和 Boosting不同，它的弱学习器之间没有依赖关系，可以并行生成，原理图如下： 从上图可以看出，bagging的个体弱学习器的训练集是通过随机采样得到的。通过T次的随机采样，我们就可以得到T个采样集，对于这T个采样集，我们可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集合策略来得到最终的强学习器。详细算法过程描述如下： 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的） 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等） 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同） 随机森林（RF）是bagging的一个特化进阶版，所谓的特化是因为随机森林的弱学习器都是决策树。所谓的进阶是随机森林在bagging的样本随机采样基础上，又加上了特征的随机选择。对于Bagging需要注意的是，每次训练集可以取全部的特征进行训练，也可以随机选取部分特征训练，例如随机森林就是每次随机选取部分特征 集成学习之结合策略 平均法对于数值类的回归预测问题，通常使用的结合策略是平均法，也就是说，对于若干个弱学习器的输出进行平均得到最终的预测输出。 最简单的平均是算术平均，也就是说最终预测是$$H(x) = \frac{1}{T}\sum\limits_{1}^{T}h_i(x)$$ 如果每个个体学习器有一个权重w，则最终预测是$$H(x) = \sum\limits_{i=1}^{T}w_ih_i(x)$$其中wi是个体学习器hi的权重，通常有$$w_i \geq 0 ,\;\;\; \sum\limits_{i=1}^{T}w_i = 1$$ 投票法 对于分类问题的预测，我们通常使用的是投票法。假设我们的预测类别是${c_1,c_2,…c_K}​$,对于任意一个预测样本x，我们的T个弱学习器的预测结果分别是$(h_1(x),h_2(x)…h_T(x))$。 最简单的投票法是相对多数投票法，也就是我们常说的少数服从多数，也就是T个弱学习器的对样本x的预测结果中，数量最多的类别ci为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。 稍微复杂的投票法是绝对多数投票法，也就是我们常说的要票过半数。在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数。否则会拒绝预测。 更加复杂的是加权投票法，和加权平均法一样，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的加权票数求和，最大的值对应的类别为最终类别。 学习法 前2种方法都是对弱学习器的结果做平均或者投票，相对比较简单，但是可能学习误差较大，于是就有了学习法这种方法，对于学习法，代表方法是stacking，当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果。 在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。 对比Bagging和Boosting 样本选择上： Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。 Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。 样例权重： Bagging：使用均匀取样，每个样例的权重相等 Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。 预测函数： Bagging：所有预测函数的权重相等。 Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。 并行计算： Bagging：各个预测函数可以并行生成 Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。 偏差和方差 Bagging：主要关注降低偏差（bias），因为他更加关注分类错误的样本 Boosting：更加关注降低方差（variance），因为他不容易受极值点影响详细的解释可参考Bagging和Boosting的区别偏差相当于预测准确性，而方差相当于预测稳定性，下图就能明显的说明偏差和方差。 总结 参考资料Bagging,Boosting,StackingBagging和Boosting的区别]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>ensemble learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数字图像处理的常用方法]]></title>
    <url>%2FComputer-vision%2F2019-3-25-%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[数字图像处理是指将图像信号转换成数字信号并利用计算机对其进行处理的过程。图像处理中，输入的是质量低的图像，输出的是改善质量后的图像，常用的图像处理方法有图像增强、复原、编码、压缩等。 数字图像处理常用方法 图像变换：由于图像阵列很大，直接在空间域中进行处理，涉及计算量很大。因此，往往采用各种图像变换的方法，将空间域的处理转换为变换域处理，这样不仅可减少计算量，而且可获得更有效的处理。常见的有傅立叶变换（在频域中进行数字滤波处理）、沃尔什变换、离散余弦变换等间接处理技术。 图像编码压缩：图像编码压缩技术可减少描述图像的数据量（即比特数），以便节省图像传输、处理时间和减少所占用的存储器容量。压缩可以在不失真的前提下获得，也可以在允许的失真条件下进行。编码是压缩技术中最重要的方法，它在图像处理技术中是发展最早且比较成熟的技术。 图像增强和复原：图像增强和复原的目的是为了提高图像的质量，如去除噪声，提高图像的清晰度等。图像增强不考虑图像降质的原因，突出图像中所感兴趣的部分。如强化图像高频分量，可使图像中物体轮廓清晰，细节明显；如强化低频分量可减少图像中噪声影响。图像复原要求对图像降质的原因有一定的了解，一般讲应根据降质过程建立“降质模型”，再采用某种滤波方法，恢复或重建原来的图像。 图像分割：图像分割是数字图像处理中的关键技术之一。图像分割是将图像中有意义的特征部分提取出来，其有意义的特征有图像中的边缘、区域等，这是进一步进行图像识别、分析和理解的基础。 图像描述：图像描述是图像识别和理解的必要前提。作为最简单的二值图像可采用其几何特性描述物体的特性，一般图像的描述方法采用二维形状描述，它有边界描述和区域描述两类方法。对于特殊的纹理图像可采用二维纹理特征描述。随着图像处理研究的深入发展，已经开始进行三维物体描述的研究，提出了体积描述、表面描述、广义圆柱体描述等方法。 图像分类（识别）：图像分类（识别）属于模式识别的范畴，其主要内容是图像经过某些预处理（增强、复原、压缩）后，进行图像分割和特征提取，从而进行判决分类。图像分类常采用经典的模式识别方法，有统计模式分类和句法（结构）模式分类，近年来新发展起来的模糊模式识别和人工神经网络模式分类在图像识别中也越来越受到重视。]]></content>
      <categories>
        <category>Computer vision</category>
      </categories>
      <tags>
        <tag>Image processing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[准确率，精确率，召回率和F1值]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-22-%E5%87%86%E7%A1%AE%E7%8E%87%EF%BC%8C%E7%B2%BE%E7%A1%AE%E7%8E%87%EF%BC%8C%E5%8F%AC%E5%9B%9E%E7%8E%87%E5%92%8CF1%E5%80%BC%2F</url>
    <content type="text"><![CDATA[机器学习(ML),自然语言处理(NLP),信息检索(IR)等领域,评估(Evaluation)是一个必要的 工作,而其评价指标往往有如下几点:准确率(Accuracy),精确率(Precision),召回率(Recall)和F1-Measure。 (注： 相对来说，IR 的 ground truth 很多时候是一个 Ordered List, 而不是一个 Bool 类型的 Unordered Collection，在都找到的情况下，排在第三名还是第四名损失并不是很大，而排在第一名和第一百名，虽然都是“找到了”，但是意义是不一样的，因此 更多可能适用于 MAP(下面会介绍) 之类评估指标。) 准确率、精确率、召回率在介绍准确率，精确率，召回率和F1值之前，我们先来看这样一个例子： 假设一个班级有100个学生，其中男生70人，女生30人。如下图，蓝色矩形表示男生，橙色矩形表示女生。又假设，我们不知道这些学生的性别，只知道他们的身高和体重。我们有一个程序(分类器)，这个程序可以通过分析每个学生的身高和体重，对这100个学生的性别分别进行预测。最后的预测结果为，60人为男生，40人为女生，(我们假设男生为正例，女生为负例)如下图。 TP：(实际为正例，预测也为正例) 实际为男生，预测为男生； FP：(实际为负例，预测为正例) 实际为女生，预测为男生； FN：(实际为正例，预测为负例) 实际为男生，预测为女生； TN：(实际为负例，预测也为负例) 实际为女生，预测为女生； 准确率(Accuracy) ＝ (TP + TN) / 总样本 ＝(40 + 10)/100 = 50%。 定义是: 对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。 精确率(Precision) ＝ TP / (TP + FP) = 40/60 = 66.67%。它表示：预测为正的样本中有多少是真正的正样本，它是针对我们预测结果而言的。Precision又称为查准率。 召回率(Recall) ＝ TP / (TP + FN) = 40/70 = 57.14% 。它表示：样本中的正例有多少被预测正确了， 它是针对我们原来的样本而言的。Recall又称为查全率。 总结：准确率就是找得对，召回率就是找得全 准确率、召回率、F1信息检索、分类、识别、翻译等领域两个最基本指标是召回率(Recall Rate)和准确率(Precision Rate)，召回率也叫查全率，准确率也叫查准率，下面我们主要看看在信息检索中的情况。概念公式: 召回率(Recall) = 系统检索到的相关文件 / 系统所有相关的文件总数准确率(Precision) = 系统检索到的相关文件 / 系统所有检索到的文件总数 图示如下： A：检索到的，也相关的 （搜到的也想要的） B：检索到的，但是不相关的 （搜到的但没用的） C：未检索到的，但却是相关的 （没搜到，然而实际上想要的） D：未检索到的，也不相关的 （没搜到也没用的） 注意：准确率和召回率是互相影响的，理想情况下肯定是做到两者都高，但是一般情况下准确率高、召回率就低，召回率低、准确率高。一般情况，用不同的阀值，统计出一组不同阀值下的精确率和召回率，如下图： 如果是做搜索，那就是保证召回的情况下提升准确率；如果做疾病监测、反垃圾，则是保准确率的条件下，提升召回率。 所以，在两者都要求高的情况下，可以用F1来衡量，计算公式如下。 MAP(mean Average Precision) 信息检索MAP是为解决P，R，F-measure的单点值局限性的。为了得到 一个能够反映全局性能的指标，可以看考察下图，其中两条曲线(方块点与圆点)分布对应了两个检索系统的准确率-召回率曲线 分析： 可以看出，虽然两个系统的性能曲线有所交叠但是以圆点标示的系统的性能在绝大多数情况下要远好于用方块标示的系统。 我们可以 发现一点，如果一个系统的性能较好，其曲线应当尽可能的向上突出。更加具体的，曲线与坐标轴之间的面积应当越大。 最理想的系统， 其包含的面积应当是1，而所有系统的包含的面积都应当大于0。这就是用以评价信息检索系统的最常用性能指标，平均准确率MAP其规范的定义如下:(其中P，R分别为准确率与召回率) ROC和AUC 分类识别ROC和AUC是评价分类器的指标，ROC的全名叫做Receiver Operating Characteristic。ROC关注两个指标 True Positive Rate ( TPR ) = TP / ( TP + FN) ，TPR代表能将正例分对的概率 False Positive Rate( FPR ) = FP / (FP + TN)，FPR代表将负例错分为正例的概率 在ROC 空间中，每个点的横坐标是FPR，纵坐标是TPR，这也就描绘了分类器在TP（真正的正例）和FP（错误的正例）间的trade-off。ROC的主要分 析工具是一个画在ROC空间的曲线——ROC curve。我们知道，对于二值分类问题，实例的值往往是连续值，我们通过设定一个阈值，将实例分类到正类或者负类（比如大于阈值划分为正类）。因此我们 可以变化阈值，根据不同的阈值进行分类，根据分类结果计算得到ROC空间中相应的点，连接这些点就形成ROC curve。ROC curve经过（0,0）（1,1），实际上(0, 0)和(1, 1)连线形成的ROC curve实际上代表的是一个随机分类器。一般情况下，这个曲线都应该处于(0, 0)和(1, 1)连线的上方。如图所示。 用ROC curve来表示分类器的performance很直观好用。可是，人们总是希望能有一个数值来标志分类器的好坏。于是Area Under roc Curve(AUC)就出现了。顾名思义，AUC的值就是处于ROC curve下方的那部分面积的大小。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>evaluation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛客笔试题之顺丰机器学习真题]]></title>
    <url>%2F%E7%89%9B%E5%AE%A2%E7%AC%94%E8%AF%95%E9%A2%98%2F2019-3-22-%E7%89%9B%E5%AE%A2%E7%AC%94%E8%AF%95%E9%A2%98%E4%B9%8B%E9%A1%BA%E4%B8%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9C%9F%E9%A2%98%2F</url>
    <content type="text"><![CDATA[昨天做了一套顺丰人工智能和机器学习的真题，下面是对其中一些知识点的总结。 Java中的String解析： 链表链表的特性，使其在某些操作上比数组更加高效。 增删不必挪动元素。当进行插入和删除操作时，链表操作的时间复杂度仅为O(1)。 无需实现估计空间。链表在内存中不是连续存储的，所以可以充分利用内存中碎片空间。 UDP与TCP TCP 面向有连接 可靠 面向字节流 数据无边界 速度慢 一对一 UDP 无连接 不可靠会丢包 面向报文 有边界 速度块 一对一或一对多 死锁产生必要条件： 互斥 请求与保持 循环等待 非剥夺 OSI七层模型OSI（Open System Interconnect），即开放式系统互联。 一般都叫OSI参考模型，是ISO（国际标准化组织）组织在1985年研究的网络互连模型。它定义了网络互连的七层框架：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层 TCP/IP五层协议和OSI的七层协议对应关系如下，在每一层都工作着不同的设备： 在每一层实现的协议也各不同，即每一层的服务也不同： 数据库索引 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性 当用户查询索引字段时，索引可以快速地执行检索操作，借助索引，在执行查询的时候不需要扫描整个表就可以快速地找到所需要的数据。 创建索引和维护索引要耗费时间、空间,当对表中的数据进行增加、删除和修改的时候,会降低数据的维护速度 Numpy解析：123456789101112131415161718192021222324import numpy as np'''numpy.repeat(a, repeats, axis=None)将a重复b次&gt;&gt;&gt; x = np.array([[1,2],[3,4]])&gt;&gt;&gt; np.repeat(x, 2)array([1, 1, 2, 2, 3, 3, 4, 4])&gt;&gt;&gt; np.repeat(x, 3, axis=1)array([[1, 1, 1, 2, 2, 2], [3, 3, 3, 4, 4, 4]])&gt;&gt;&gt; np.repeat(x, [1, 2], axis=0)array([[1, 2], [3, 4], [3, 4]])'''a = np.repeat(np.arange(5).reshape([1,-1]),10,axis = 0)+10.0b = np.random.randint(5, size= a.shape) # 生成[0,5)随机矩阵，大小和矩阵a相同c = np.argmin(a*b, axis=1) #矩阵a和b乘积，返回每行最小值位置b = np.zeros(a.shape) #与矩阵a相同大小的全零矩阵print("b变之前：",str(b))print("c的值：",str(c))b[np.arange(b.shape[0]), c] = 1 #将b中每一行的c位置处赋值为1print("b变之后：",str(b))print(b.shape) 输出结果：12345678910111213141516171819202122b变之前： [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]]c的值： [0 3 4 3 1 3 4 2 2 0]b变之后： [[1. 0. 0. 0. 0.] [0. 0. 0. 1. 0.] [0. 0. 0. 0. 1.] [0. 0. 0. 1. 0.] [0. 1. 0. 0. 0.] [0. 0. 0. 1. 0.] [0. 0. 0. 0. 1.] [0. 0. 1. 0. 0.] [0. 0. 1. 0. 0.] [1. 0. 0. 0. 0.]](10, 5) RF GBDT XgBoostRF、GBDT、XgBoost 三者都是集成学习中的方法，其中RF属于Bagging方法，GBDT和XgBoost属于Boosting方法。过段时间会专门写一篇有关Bagging和Boosting介绍的博文。 RF（Random Forest） 随机森林主要运用到的方法是bagging，采用Bootstrap的随机有放回的抽样，抽样出N份数据集，训练出N个决策树。然后根据N个决策树输出的结果决定最终结果（离散型的输出：取最多的类别，连续型的输出：取平均数）。 优势： 容易理解和解释 不需要太多的数据预处理工作 隐含地创造了多个联合特征，并能够解决非线性问题 随机森林模型不容易过拟合 自带out-of-bag (oob)错误评估功能 并行化容易实现 劣势： 不适合小样本，只适合大样本 精度较低 适合决策边界是矩形的，不适合对角线型的 GBDT 通过采用加法模型（即基函数的线性组合），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法。通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。一般选择cart tree且树的深度不会很深。 优势： 精度高 能处理非线性数据 能处理多特征类型 适合低维稠密数据 模型可解释性好 不需要做特征的归一化，可以自动选择特征 能适应多种损失函数 劣势： 不太适合并发执行 计算复杂度高 不适用高维稀疏特征 XgBoost 简单来说XgBoost是GBDT的一种高效实现，主要具有以下几个优势： 显式的把树模型复杂度作为正则项加到优化目标中。 实现了分裂点寻找近似算法。 可以并行执行 RF和GBDT的比较: 相同点： 都是由多棵树组成 最终的结果都是由多棵树一起决定 不同点： 组成RF的树可以是分类树，也可以是回归树；而GBDT只由回归树组成 组成RF的树可以并行生成；而GBDT只能是串行生成 对于最终的输出结果而言，RF采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来 RF对异常值不敏感，GBDT对异常值非常敏感 RF对训练集一视同仁，GBDT是基于权值的弱分类器的集成 RF是通过减少模型方差(variance)提高性能，GBDT是通过减少模型偏差(bias)提高性能 Boosting和Bagging的差别过拟合的模型，通常variance比较大，这时应该用bagging对其进行修正。欠拟合的模型，通常bias比较大，这时应该可以用boosting进行修正。使用boosting时， 每一个模型可以简单一些。 bagging中的模型是强模型，偏差低，方差高。目标是降低方差(variance)。在bagging中，每个模型的bias和variance近似相同，但是互相相关性不太高，因此一般不能降低bias，而一定程度上能降低variance。典型的bagging是random forest (RF)。 boosting中每个模型是弱模型，偏差高，方差低。目标是通过平均降低偏差(bias)。boosting的基本思想就是用贪心法最小化损失函数，显然能降低偏差，但是通常模型的相关性很强，因此不能显著降低variance。典型的Boosting是Adaboost，另外一个常用的并行Boosting算法是GBDT（gradient boosting decision tree）。这一类算法通常不容易出现过拟合。]]></content>
      <categories>
        <category>牛客笔试题</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>computer basis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习正则化之L0、L1与L2范数]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-21-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%AD%A3%E5%88%99%E5%8C%96%E4%B9%8BL0%E3%80%81L1%E4%B8%8EL2%E8%8C%83%E6%95%B0%2F</url>
    <content type="text"><![CDATA[最近刷题时，经常会遇到关于L1和L2范数的知识点，本文就其详细的分析记录一下。 前言我们常见的监督机器学习问题无非就是“minimizeyour error while regularizing your parameters”，也就是在规则化参数的同时要最小化误差。最小化误差是为了让我们的模型拟合我们的训练数据，而规则化参数是防止我们的模型过分拟合我们的训练数据。 如果参数太多，会导致我们的模型复杂度上升，容易过拟合，也就是我们的训练误差会很小。但训练误差小并不是我们的最终目标，我们的目标是希望模型的测试误差小，也就是能准确的预测新的样本。所以，我们需要保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能（也就是测试误差也小），而模型“简单”就是通过规则函数来实现的。另外，规则项的使用还可以约束我们的模型的特性。这样就可以将人对这个模型的先验知识融入到模型的学习当中，强行地让学习到的模型具有人想要的特性，例如稀疏、低秩、平滑等等。 看待规则化的其他角度： 规则化符合奥卡姆剃刀(Occam&#39;s razor)原理,也就是在所有可能选择的模型中，我们应该选择能够很好地解释已知数据并且十分简单的模型。 从贝叶斯估计的角度来看，规则化项对应于模型的先验概率。 规则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项(regularizer)或惩罚项(penalty term)。 一般来说，监督学习可以被看做最小化下面的目标函数：其中，第一项$L(y_{i},f(x_{i};w))$ 衡量我们的模型（分类或者回归）对第$i$个样本的预测值$f(x_{i};w)$和真实的标签$y_{i}$之前的误差。因为模型目的是拟合我们的训练样本，所以我们要求这一项最小，也就是要求我们的模型尽量的拟合我们的训练数据。但正如之前所说，我们不仅要保证训练误差最小，也更希望我们的模型测试误差小，所以我们需要加上第二项，也就是对参数$w$的规则化函数$\Omega(w)​$去约束我们的模型尽量的简单。 机器学习的大部分带参模型都和这个不但形似，而且神似。是的，其实大部分无非就是变换这两项而已。对于第一项Loss函数，如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是exp-Loss，那就是集成Boosting了；如果是log-Loss，那就是 Logistic Regression了；等等。不同的loss函数，具有不同的拟合特性，这个也得就具体问题具体分析的。本文主要关注第二项“规则项Ω(w)”。规则化函数$Ω(w)​$也有很多种选择,本文主要介绍L0、L1和L2范数。 L0范数与L1范数 L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0。换句话说，让参数W是稀疏的。但是在大多数论文中，稀疏都是通过L1范数来实现的。这是因为L0和L1有着某种不寻常的关系。那我们再来看看L1范数是什么？它为什么可以实现稀疏？为什么大家都用L1范数去实现稀疏，而不是L0范数呢？ L1范数是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。 现在我们来分析为什么L1范数会使权值稀疏？有人可能会这样给你回答“它是L0范数的最优凸近似”。实际上，还存在一个更好的回答：任何的规则化算子，如果他$W_{i}=0$的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。W的L1范数是绝对值，$|w|$在$w=0$处是不可微，但这还是不够直观。这里因为我们需要和L2范数进行对比分析。 上面还有一个问题：既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。一句话总结：L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。 参数稀疏的好处： 特征选择(Feature Selection)： 大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，$x_{i}$的大部分元素（也就是特征）都是和最终的输出$y_{i}$没有关系或者不提供任何信息的，在最小化目标函数的时候考虑$x_{i}$这些额外的特征，虽然可以获得更小的训练误差（可能导致模型过拟合），但在预测新的样本时，这些没用的信息反而会被考虑(噪音)，从而干扰了对正确$y_{i}​$的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。 可解释性(Interpretability)： 另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：（一般为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的$w*$就只有很少的非零元素，例如只有5个非零的$wi$，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个$w_{i}$都非0，医生面对这1000种因素，累觉不爱。 L2范数除了L1范数，还有一种更受宠幸的规则化范数是L2范数:$ ||W||_2$。它也不逊于L1范数，它有两个美称，在回归里面，有人把有它的回归叫“岭回归”（Ridge Regression），有人也叫它“权值衰减weight decay”。这用的很多，因为它的强大功效是改善机器学习里面一个非常重要的问题：过拟合。至于过拟合是什么，上面也解释了，就是模型训练时候的误差很小，但在测试的时候误差很大，也就是我们的模型复杂到可以拟合到我们的所有训练样本了，但在实际预测新的样本的时候，准确率很低。通俗的讲就是应试能力很强，实际应用能力很差。擅长背诵知识，却不懂得灵活利用知识。例如下图所示： 上面的图是线性回归，下面的图是Logistic回归，也可以说是分类的情况。从左到右分别是欠拟合（underfitting，也称High-bias）、合适的拟合和过拟合（overfitting，也称High variance）三种情况。可以看到，如果模型复杂（可以拟合任意的复杂函数），它可以让我们的模型拟合所有的数据点，也就是基本上没有误差。对于回归来说，就是我们的函数曲线通过了所有的数据点，如上图右。对分类来说，就是我们的函数曲线要把所有的数据点都分类正确，如下图右。这两种情况很明显过拟合了。 为什么L2范数可以防止过拟合？回答这个问题之前，我们得先看看L2范数的实质。 L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的规则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0，这里是有很大的区别。而越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象。为什么越小的参数说明模型越简单？我也不懂，我的理解是：限制了参数很小，实际上就限制了多项式某些分量的影响很小（看上面线性回归的模型的那个拟合的图），这样就相当于减少参数个数。 一句话总结下：通过L2范数，我们可以实现了对模型空间的限制，从而在一定程度上避免了过拟合。 L2范数的好处： 从学习理论的角度来说，L2范数可以防止过拟合，提升模型的泛化能力。 从优化或者数值计算的角度来说，L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。关于condition number，大家可以看文末的参考资料链接，里面讲的很详细。 L1和L2的差别L1让绝对值最小，L2让平方最小，为什么会有那么大的差别呢？这里给出两种几何上直观的解析： 下降速度： 我们知道，L1和L2都是规则化的方式，我们将权值参数以L1或者L2的方式放到代价函数里面去。然后模型就会尝试去最小化这些权值参数。而这个最小化就像一个下坡的过程，L1和L2的差别就在于这个“坡”不同，如下图：L1就是按绝对值函数的“坡”下降的，而L2是按二次函数的“坡”下降。所以实际上在0附近，L1的下降速度比L2的下降速度要快。所以会非常快得降到0。(此解释待商榷) 模型空间的限制（看的不是太懂）：实际上，对于L1和L2规则化的代价函数来说，我们可以写成以下形式： 也就是说，我们将模型空间限制在w的一个L1-ball 中。为了便于可视化，我们考虑两维的情况，在(w1, w2)平面上可以画出目标函数的等高线，而约束条件则成为平面上半径为C的一个 norm ball 。等高线与 norm ball 首次相交的地方就是最优解： 可以看到，L1-ball 与L2-ball 的不同就在于L1在和每个坐标轴相交的地方都有“角”出现，而目标函数的测地线除非位置摆得非常好，大部分时候都会在角的地方相交。注意到在角的位置就会产生稀疏性，例如图中的相交点就有w1=0，而更高维的时候（想象一下三维的L1-ball 是什么样的？）除了角点以外，还有很多边的轮廓也是既有很大的概率成为第一次相交的地方，又会产生稀疏性。 相比之下，L2-ball 就没有这样的性质，因为没有角，所以第一次相交的地方出现在具有稀疏性的位置的概率就变得非常小了。这就从直观上来解释了为什么L1-regularization 能产生稀疏性，而L2-regularization 不行的原因了。 因此，一句话总结就是：L1会趋向于产生少量的特征，而其他的特征都是0（稀疏），而L2会选择更多的特征，这些特征都会接近于0（平滑）。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。 参考资料：机器学习中的范数规则化之（一）L0、L1与L2范数]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>regularization</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛客笔试题之机器学习]]></title>
    <url>%2F%E7%89%9B%E5%AE%A2%E7%AC%94%E8%AF%95%E9%A2%98%2F2019-3-21-%E7%89%9B%E5%AE%A2%E7%AC%94%E8%AF%95%E9%A2%98%E4%B9%8B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[昨天做完了牛客网上的机器学习试题，下面是对一些错题的分析，并简要总结了一些机器学习中应该注意的知识点，过段时间会对其中的一些方法进行更加详细的分析介绍。题中打问号？代表该题答案存在争议，不一定准确。 过拟合问题解析：造成过拟合的原因主要有： 训练数据不足 训练模型过度导致模型非常复杂，泛化能力差 样本里的噪音数据干扰过大，大到模型过分记住了噪音特征，反而忽略了真实的输入输出间的关系； 权值学习迭代次数足够多(Overtraining)，拟合了训练数据中的噪声和训练样例中没有代表性的特征 选项A增加训练集可以解决训练数据不足的问题，防止过拟合 选项B对应使得模型的复杂度降低，防止过拟合 选项C类似主成分分析，降低数据的特征维度，使得模型复杂度降低，防止过拟合 选项D使得模型更加复杂化，会充分训练数据导致过拟合 条件概率解析：由条件概率公式可知： 先验概率未知解析： 朴素贝叶斯（NB）NB的核心在于它假设向量的所有分量之间是独立的。在贝叶斯理论系统中，都有一个重要的条件独立性假设：假设所有特征之间相互独立，这样才能将联合概率拆分。 分支定界法（branch and bound）解析： 基于核的机器学习算法解析： A EM算法，聚类算法 B 径向基核函数 C 线性判别分析 D 支持向量机核函数的本质就是将一个空间转化为另一个空间的变化，线性判别分析是把高纬空间利用特征值和特征向量转化到一维空间，核化的LDA模型是KFDA。 L1和L2范数解析： L1范数是指向量中各个元素的绝对值之和，也叫”系数规则算子（Lasso regularization）。它可以实现稀疏，通过将无用特征对应的参数W置为零实现。 L2范数是指向量各元素的平方和然后开方，用在回归模型中也称为岭回归（Ridge regression）。L2避免过拟合的原理是：让L2范数的规则项||W||2 尽可能小，可以使得W每个元素都很小，接近于零，但是与L1不同的是，不会等于0；这样得到的模型抗干扰能力强，参数很小时，即使样本数据x发生很大的变化，模型预测值y的变化也会很有限。 准确率、召回率及F1值解析：精准度和召回率是一对矛盾的度量，一般来说，精准度越高，召回率越低；召回率越高，精准度越低。 生成式模型和判别式模型生成式模型(Generative Model)与判别式模型(Discrimitive Model)是分类器常遇到的概念，它们的区别在于： 对于输入x，类别标签y： 生成式模型估计它们的联合概率分布P(x,y) 判别式模型估计条件概率分布P(y|x) 生成式模型可以根据贝叶斯公式得到判别式模型，但反过来不行。 公式上看生成模型： 学习时先得到 P(x,y)，继而得到 P(y|x)。预测时应用最大后验概率法（MAP）得到预测类别 y。判别模型： 直接学习得到P(y|x)，利用MAP得到 y。或者直接学得一个映射函数 y=f(x)。 直观上看生成模型： 关注数据是如何生成的判别模型： 关注类别之间的差别例子：假如你的任务是识别一个语音属于哪种语言。例如对面一个人走过来，和你说了一句话，你需要识别出她说的到底是汉语、英语还是法语等。那么你可以有两种方法达到这个目的： 学习每一种语言，你花了大量精力把汉语、英语和法语等都学会了，我指的学会是你知道什么样的语音对应什么样的语言。然后再有人过来对你说，你就可以知道他说的是什么语音. 不去学习每一种语言，你只学习这些语言之间的差别，然后再判断（分类）。意思是指我学会了汉语和英语等语言的发音是有差别的，我学会这种差别就好了。那么第一种方法就是生成方法，第二种方法是判别方法。 常见的判别式模型： 逻辑回归 Logistic Regression 支持向量机 SVM 神经网络 NN 传统神经网络 Traditional Neural Networks 邻近取样 Nearest Neighbor 条件随机场 CRF 线性判别分析 Linear Discriminant Analysis 提升算法 Boosting 线性回归 Linear Regression 高斯过程 Gaussian Process 分类回归树 Classification and Regression Tree (CART) 区分度训练 常见的生成式模型： 高斯 Gaussians 朴素贝叶斯 Naive Bayes 混合多项式 Mixtures of Multinomials 混合高斯模型 Mixtures of Gaussians 多专家模型 Mixtures of Experts 隐马尔科夫模型 HMM S型信念网络 Sigmoidal Belief Networks 贝叶斯网络 Bayesian Networks 马尔科夫随机场 Markov Random Fields 潜在狄利克雷分配 Latent Dirichlet Allocation(LDA) 判别式分析 K近邻 KNN 深度信念网络 DBN 聚类算法影响因素解析：聚类的目标是使同一类对象的相似度尽可能地大，不同类对象之间的相似度尽可能的小。聚类分析算法主要可以分为： 划分法（Partitioning Methods） 层次法（Hierarchical Me thods） 基于密度的方法（Density-Based Methods） 基于网格的方法（Grid-Based M ethods） 基于模型的方法（Model-Based Methods） 谱聚类（Spectral Clustering） C大约说的是度量方式，例如KMeans 可以用欧式距离啊，也可用其他的距离，这也是分类准则。（C正确） 不过个人觉得C有歧义；特征选取的差异会影响聚类效果（A正 确）。聚类的目标是使同一类对象的相似度尽可能地大，因此不同的相似度测度方法对聚类结 果有着重要影响（B正确）。由于聚类算法是无监督方法，不存在带类别标签的样本，因此， D选项不是聚类算法的输入数据。 隐马尔科夫模型解析： 前向后向算法解决的是一个评估问题，即给定一个模型，求某些特定观测序列的概率，用于评估该序列最匹配的模型。 Baum-Welch算法解决的是一个模型训练问题（学习），即参数估计，是一种无监督的训练方法，主要通过EM迭代实现。 维特比算法解决的是一个预测问题，通信中的解码问题，即给定一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。比如通过海藻变化（输出序列）来观测天气（状态序列）。 AdaBoost及SVM解析： 卷积大小计算解析： 特征选择方法解析： 缺失值处理方法解析：由于调查、编码和录入误差，数据中可能存在一些无效值和缺失值，需要给予适当的处理。常用的处理方法有：估算，整例删除，变量删除和成对删除。 估算(estimation)。最简单的办法就是用某个变量的样本均值、中位数或众数代替无效值和缺失值。这种办法简单，但没有充分考虑数据中已有的信息，误差可能较大。 整例删除(casewise deletion)是剔除含有缺失值的样本。由于很多问卷都可能存在缺失值，这种做法的结果可能导致有效样本量大大减少，无法充分利用已经收集到的数据。因此，只适合关键变量缺失，或者含有无效值或缺失值的样本比重很小的情况。 变量删除(variable deletion)。如果某一变量的无效值和缺失值很多，而且该变量对于所研究的问题不是特别重要，则可以考虑将该变量删除。这种做法减少了供分析用的变量数目，但没有改变样本量。 成对删除(pairwise deletion)是用一个特殊码(通常是9、99、999等)代表无效值和缺失值，同时保留数据集中的全部变量和样本。但是，在具体计算时只采用有完整答案的样本，因而不同的分析因涉及的变量不同，其有效样本量也会有所不同。 信息增益解析：本题主要考察信息增益的计算方式，具体可参考我之前博客决策树：$$Gain(A) = Info(D) - InfoA(D)$$其中Info表示信息熵，计算公式如下：所以可以计算出各特征的信息增益如下所示： 置信度及支持度解析：置信度计算规则为： 同时购买商品A和商品B的交易次数/购买了商品A的次数支持度计算规则为： 同时购买了商品A和商品B的交易次数/总的交易次数 求解线性不可分方法解析：伪逆法： 径向基（RBF）神经网络的训练算法，径向基解决的就是线性不可分的情况。感知器算法： 线性分类模型。H-K算法： 在最小均方误差准则下求得权矢量，二次准则解决非线性问题。势函数法： 势函数非线性。 时间序列模型解析：AR模型：自回归模型，是一种线性模型MA模型：移动平均法模型，其中使用趋势移动平均法建立直线趋势的预测模型ARMA模型：自回归滑动平均模型，拟合较高阶模型GARCH模型：广义回归模型，对误差的方差建模，适用于波动性的分析和预测 PMF PDF CDF解析： CRF（条件随机场）解析： HMM（隐马尔科夫模型）：HMM是一种产生式模型，定义了联合概率分布p(x,y) ，其中x和y分别表示观察序列和相对应的标注序列的随机变量。它包含2个基本假设： 后一个隐藏状态只依赖于前一个隐藏状态。 观测值之间相互独立，观测值只依赖于该时刻的马尔科夫链的隐状态。缺点：1. HMM只依赖于每一个状态和它对应的观察对象：2、目标函数和预测目标函数不匹配： MEMM（最大熵马尔科夫模型）：最大熵马尔科夫模型把HMM模型和Maximum Entropy模型的优点集合成一种生成模型（Generative Model）。克服了观察值之间严格独立产生的问题，但仍存在标注偏置问题（Label bias problem）。 CRF（条件随机场）：CRF模型解决了标注偏置问题，去除了HMM中两个不合理的假设，当然，模型相应得也变复杂了。MEMM是局部归一化，CRF是全局归一化。CRF 的优点：特征灵活，可以容纳较多的上下文信息，能够做到全局最优CRF 的缺点：速度慢总结：三者都是NLP（自然语言处理）中的基础语言模型。 线性回归描述解析： K-L与PCA解析：K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。 数据不均衡解析： 重采样。A可视作重采样的变形。改变数据分布消除不平衡，可能导致过拟合。 欠采样。C的方案 提高少数类的分类性能，可能丢失多数类的重要信息。 权值调整。D方案也是其中一种方式。 HK算法与感知器解析： 时间序列算法模型解析：常见的时间序列算法模型有 移动平均法 (MA) 简单移动平均法 自回归模型(AR) 自回归滑动平均模型(ARMA) GARCH模型 指数平滑法ABD都是一些关于股票涨跌的分析方法。 SVM核函数解析：SVM核函数包括：线性核函数、多项式核函数、径向基核函数（RBF）、高斯核函数、幂指数核函数、拉普拉斯核函数、ANOVA核函数、二次有理核函数、多元二次核函数、逆多元二次核函数以及Sigmoid核函数。 Logit与SVM解析：A. Logit回归本质上是一种根据样本对权值进行极大似然估计的方法，而后验概率正比于先验概率和似然函数的乘积。logit仅仅是最大化似然函数，并没有最大化后验概率，更谈不上最小化后验概率。A错误B. Logit回归的输出就是样本属于正类别的几率，可以计算出概率，正确C. SVM的目标是找到使得训练数据尽可能分开且分类间隔最大的超平面，应该属于结构风险最小化，正确。D. SVM可以通过正则化系数控制模型的复杂度，避免过拟合。（个人觉得但最好是加上正则项吧） LDA与PCALDA（线性判别分析）用于降维，和PCA（主成分分析）有很多相同，也有很多不同的地方，因此值得好好的比较一下两者的降维异同点。相同点： 两者均可以对数据进行降维。 两者在降维时均使用了矩阵特征分解的思想。 两者都假设数据符合高斯分布。 不同点： LDA是有监督的降维方法，而PCA是无监督的降维方法 LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。 LDA除了可以用于降维，还可以用于分类。 LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向 常见的数据降维方法线性 LDA（线性判别分析） PCA（主成分分析） 非线性 核方法（KPCA、KFDA等） 二维化 流行学习（LLE、LPP、ISOMap等） 其他方法： 神经网络（自编码） 聚类3.小波分析 LASSO（参数压缩） SVD奇异值分解 线性分类器准则解析：线性分类器有三大类：感知器准则函数、SVM、Fisher准则，而贝叶斯分类器不是线性分类器。 感知器准则函数：代价函数J=-(W*X+w0)，分类的准则是最小化代价函数。感知器是神经网络（NN）的基础。 SVM：支持向量机也是很经典的算法，优化目标是最大化间隔（margin），又称最大间隔分类器，是一种典型的线性分类器。（使用核函数可解决非线性问题） Fisher准则：更广泛的称呼是线性判别分析（LDA），将所有样本投影到一条远点出发的直线，使得同类样本距离尽可能小，不同类样本距离尽可能大，具体为最大化“广义瑞利商”。贝叶斯分类器：一种基于统计方法的分类器，要求先了解样本的分布特点（高斯、指数等），所以使用起来限制很多。在满足一些特定条件下，其优化目标与线性分类器有相同结构（同方差高斯分布等），其余条件下不是线性分类。]]></content>
      <categories>
        <category>牛客笔试题</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习框架的对比及分析]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-3-19-%E4%B8%89%E5%A4%A7%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E7%9A%84%E5%AF%B9%E6%AF%94%E5%8F%8A%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[本文主要介绍了现在主流的三大深度学习框架Tensorflow、Caffe和Pytorch的组成结构，并对其特点进行了简要分析。 Tensorflow TensorFlow是一个使用数据流图进行数值计算的开源软件库。图中的节点表示数学运算，而图边表示节点之间传递的多维数据阵列（又称张量）。灵活的体系结构允许使用单个API将计算部署到服务器或移动设备中的某个或多个CPU或GPU。它是一个静态图模型，一旦图被构建后，结构不变，每次输入不同的数据。Tensorflow一般先定义各种变量,然后建立一个计算图(数据流图),计算图指定了各个变量之间的计算关系,若此时对计算图进行编译,没有任何输出,计算图还是一个空壳,只有把需要运算的输入放入后,才会在模型中形成数据流，形成输出。Tensorflow中的运算都要放在图中,图中的运算进行发生在回话启动后(session),给节点填充数据,然后进行运算,关闭会话,最后结束运算。 张量 所有的数据都通过张量的形式来表示，它可以被简单的理解为多为数组。张量的使用主要可以分为2大类： 对中间计算结果的引用 当计算图构造完成后，张量可以用来获取计算结果 OP操作 计算图中每一个节点代表着一个操作(operation,Op),一般用来表示施加的的数学运算,也可以表示数据输入的起点以及输出的终点,或者是读取/写入持久变量(persistent veriable)的终点,Op也可用于状态初始化的任务。 类比： 一个神经元有多个输入，一个或者多个输出。这里的OP可以看作神经元，tensor可以看作输入的数据。 计算图中的边 计算图中的边包含这两种关系:数据依赖和控制依赖。(不需要对计算图中的边进行定义,因为在tensorflow中创建节点是已包含了相应的Op完成计算所需的全部输入,tensorflow会自动绘制必要的连接) 。 实线的边代表着数据依赖：标识连接的两个节点之间有tensor的流动(传递) 虚线的边代表着控制依赖：用于控制操作的运行,确保发生前关系, 连接的节点之间没有tensor的传递,但是源节点必须在目的节点执行前完成执行。 TensorFlow程序的阶段 TensorFlow程序通常被组织成一个构建阶段和一个执行阶段。在构建阶段，op 的执行步骤被描述成一个图。在执行阶段，使用会话执行图中的op。 阶段一：如何构建图？ 构建图从创建op开始。有些op的创建是不需要input的，比如Constant。这样的op被成为源op（source op）。 在python中op对象是由op构造器（ops constructors）创建的。op构造器创建一个op对象时可以传递一个源op作为待构造op对象的输入。 op对象被op构造器创建后是作为一个node加入到graph中的。TensorFlow Python 库有一个默认图 (default graph), op 构造器可以为其增加节点。 总结：因为graph是由op对象组成的，所以构建图的过程其实就是创建op对象的过程，以及如果将这些个op对象连接起来（比如某个op对象作为另外某个op对象的输入）的过程。 阶段二：图构建好了，如何执行？ 因为graph需要在session中启动。所以为了启动一个graph，第一步就是创建session对象。 sessoin对象创建的时候如果不制定graph，则使用默认图（default graph）。 Variable及Constant 变量用于维护图执行过程中的状态信息。通常会将一个统计模型中的参数表示为一组变量。 例如, 你可以将一个神经网络的权重作为一个tensor存储在某个变量中。在训练过程中, 通过重复运行训练图，更新这个 tensor。 tensor存储在Constants或者Variables。就像数据可以放在常量和变量中一样。放在变量中的数据是可以修改的，放在常量中的数据是不可以修改的。 搭建神经网络步骤 定义添加神经层的函数； 准备训练的数据； 定义节点准备接收数据； 定义神经层：隐藏层和预测层； 定义loss表达式； 选择optimizer使loss达到最小； 对所有变量进行初始化，通过sess.run optimizer，迭代多次进行学习。 优势 TensorFlow有内置的TF.Learn和TF.Slim等上层组件可以帮助快速地设计新网络，并且兼容Scikit-learn estimator接口，可以方便地实现evaluate、grid search、cross validation等功能。同时TensorFlow不只局限于神经网络，其数据流式图支持非常自由的算法表达，当然也可以轻松实现深度学习以外的机器学习算法。事实上，只要可以将计算表示成计算图的形式，就可以使用TensorFlow。用户可以写内层循环代码控制计算图分支的计算，TensorFlow会自动将相关的分支转为子图并执行迭代运算。TensorFlow也可以将计算图中的各个节点分配到不同的设备执行，充分利用硬件资源。 在可视化方面，Tensorboard非常棒。该工具包含在TensorFlow里，它对于调试和比较不同的训练过程非常有用。例如，在训练模型的时候，你可以在调整某些超参数之后再训练一遍。两次运行过程可以同时显示在Tensorboard上，以显示它们之间存在的差异。Tensorboard能够： 显示模型图 绘制标量变量 使分布和直方图可视化 使图像可视化 使嵌入可视化 播放音频Tensorboard能够显示通过tf.summary模块收集的各种摘要。我们将为前面提到的那个指数例子定义摘要操作，并使用tf.summary.FileWriter将其保存到磁盘上。执行tensorboard --logdir=./tensorboard以启动Tensorboard。这个工具对于云实例来说非常方便，因为它是一个webapp。 Caffe优势及劣势Caffe是一个清晰而高效的深度学习框架，主要优势为： 上手容易，网络结构都是以配置文件形式定义，不需要用代码设计网络 训练速度快，组件模块化，可以方便的拓展到新的模型和学习任务上 拥有大量的训练好的经典模型（AlexNet、VGG、Inception）乃至其他state-of-the-art（ResNet等）的模型，收藏在它的Model Zoo劣势： 但是Caffe最开始设计时的目标只针对于图像，没有考虑文本、语音或者时间序列的数据，因此Caffe对卷积神经网络的支持非常好，但是对于时间序列RNN，LSTM等支持的不是特别充分。 Caffe 的配置文件不能用编程的方式调整超参数，也没有提供像 Scikit-learn 那样好用的 estimator 可以方便地进行交叉验证、超参数的 Grid Search 等操作。 组成Caffe由低到高依次把网络中的数据抽象成Blob, 各层网络抽象成Layer ，整个网络抽象成Net，网络模型的求解方法抽象成Solver。 Blob: 表示网络中的数据，包括训练数据，网络各层自身的参数，网络之间传递的数据都是通过Blob来实现的，同时Blob数据也支持在CPU与GPU上存储，能够在两者之间做同步。 Layer:对神经网络中各种层的抽象，包括卷积层和下采样层，还有全连接层和各种激活函数层等。同时每种Layer都实现了前向传播和反向传播，并通过Blob来传递数据。 Net: 是对整个网络的表示，由各种Layer前后连接组合而成，也是所构建的网络模型。 Solver: 定义了针对Net网络模型的求解方法，记录网络的训练过程，保存网络模型参数，中断并恢复网络的训练过程。自定义Solver能够实现不同的网络求解方式。 核心概念Caffe 的核心概念是Layer，每一个神经网络的模块都是一个Layer。Layer 接收输入数据，同时经过内部计算产生输出数据。设计网络结构时，只需要把各个Layer 拼接在一起构成完整的网络（通过写 protobuf配置文件定义）。比如卷积的Layer，它的输入就是图片的全部像素点，内部进行的操作是各种像素值与Layer 参数的 convolution 操作，最后输出的是所有卷积核filter 的结果。每一个Layer 需要定义两种运算，一种是正向（forward）的运算，即从输入数据计算输出结果，也就是模型的预测过程；另一种是反向（backward）的运算，从输出端的gradient 求解相对于输入的gradient，即反向传播算法，这部分也就是模型的训练过程。实现新Layer 时，需要将正向和反向两种计算过程的函数都实现, 这部分计算需要用户自己写 C++或者 CUDA （当需要运行在 GPU 时）代码，较为困难。 Pytorch新颖处不同于tensorflow的静态图，pytorch是一个动态的框架，这就和python的逻辑是一样的，对变量做任何操作都是灵活的。 组成Pytorch的设计追求最少的封装，不像tensorflow中充斥着session、graph、operation、variable、layer等全新概念。Pytorch的设计遵循tensor—&gt;variable(autograd) —&gt;nn.Model三个由低到高的抽象层次，分别代表高维数组（张量）、自动求导（变量）和神经网络（层/模块），三者紧密联系，可以同时进行修改和操作。 tensor variable nn.Model 优势及劣势优势： 动态图的思想直观明了，更符合人的思考过程。动态图的方式可以使得我们可以任意修改前向传播，还可以随时查看变量的值。如果静态图框架比作C++,每次运行都要编译（sess.run）,那么动态图框架就是python，动态执行，可以交互式查看修改。 动态图调试更容易，pytorch中，代码报错的地方，一般就是你写错代码的地方，而静态图需要先根据你的代码生成Graph对象，然后在session.run()时报错，找到错误很难。 劣势： PyTorch并没有一个类似于Tensorboard的工具，但有一个可以将Tensorboard集成进来的工具。或者，也可以免费使用标准绘图工具：matplotlib和seaborn。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>tensorflow</tag>
        <tag>architecture</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[牛客笔试题之Python]]></title>
    <url>%2F%E7%89%9B%E5%AE%A2%E7%AC%94%E8%AF%95%E9%A2%98%2F2019-3-19-%E7%89%9B%E5%AE%A2%E7%AC%94%E8%AF%95%E9%A2%98%E4%B9%8Bpython%2F</url>
    <content type="text"><![CDATA[这几天做完了牛客网上的Python试题，下面是对一些错题的分析，并总结了一些python中应该注意的知识点。 字符串比较解析：a,b为字符串不可变类型，所以指向相同地址，所以 a is bis：指地址相同==: 内容相同a+b:字符串连接为’123123’ LEGB规则解析：Python一切皆对象，所以在Python中变量名是字符串对象。Python的命名空间是一个字典，字典内保存了变量名称与对象之间的映射关系，因此，查找变量名就是在命名空间字典中查找键-值对。LEGB就是用来规定命名空间查找顺序的规则。LEGB规定了查找一个名称的顺序为：local–&gt;enclosing function locals–&gt;global–&gt;builtinLocal: 即函数内部命名空间；Enclosing function locals: 外部嵌套函数的名字空间module(文件本身)：Global(module): 函数定义所在模块（文件）的名字空间Builtin: Python内置模块的名字空间 Set解析：集合（set）是一个无序的不重复元素序列。可以使用大括号 { } 或者 set() 函数创建集合，注意：创建一个空集合必须用 set() 而不是 { }，因为 { } 是用来创建一个空字典。本题中列表转集合，集合没有重复元素。 转义字符解析：python里面%d表数字，%s表示字符串，%%表示一个%；单引号内嵌套单引号需要转义字符/;单引号内嵌套双引号不需要嵌套；双引号内嵌套双引号需要转义字符/；双引号内引用单引号不需要转义字符； 类实例关系解析：isinstance（object，classinfo）: 用于判断object是否是classinfo的一个实例，或者object是否是classinfo类的子类的一个实例，如果是返回True.issubclass（class，classinfo）: 用于判断class是否是classinfo类的子类，如果是返回True. new和init的区别解析： init是当实例对象创建完成后被调用的，然后设置对象属性的一些初始值。 new是在实例创建之前被调用的，因为它的任务就是创建实例然后返回该实例，是个静态方法。 new在init之前被调用，new的返回值（实例）将传递给init方法的第一个参数，然后init给这个实例设置一些参数。 字典解析：字典是python中唯一的映射类型，阐述了键与键值之间的对应关系。字典中键必须是唯一的。列表中的项目包括在方括号中。列表是可变的数据类型（可以增加或删除项目）。所以，列表中的项目不能用来作为字典的键。 浅拷贝和深拷贝解析见上图中注释 try else finally解析：try的语句出现异常才会执行except后的语句，如果正常，则执行完try后执行else。另外，finally语句不管有无异常都会执行。所以上图中答案为4。 name解析： name定义在一个模块中，当解释器执行这个py文件时，name的值就为main； 当这个模块被引用即被其他模块import时，name的值就是模块名，也就是运行的py文件名。 闭包解析：在函数中可以定义另一个函数时，如果内部的函数引用了外部的函数的变量，则可能产生闭包。所以若a=2,b=3,则程序运行值为8。 装饰器解析见上图 大小比较类似元组、字符串、列表这类格式，在进行两者之间的比较时，先从第一个元素开始比较 ASCII 码值大小，如果相等，则依次向后比较，如果全部相等，则比较数量大小。ASCII 码值大小：数字： 0-9: 48-57字母：A-Z：65-90.a-z： 97-122 布尔值所有标准对象均可以用于布尔测试，下列对象的布尔值是False:• None• False• 所有值为零的数：0（整型），（浮点型），0L（长整型），0.0+0.0j(复数)• “”（空字符串），[ ] (空列表)， （）（空元祖），{} (空字典) 语言类型 语言特性解释性语言的特性是非独立和效率低。Python是解释性语言，在此以Python举例。非独立性： Python代码解释执行结果依赖于IDLE的版本，其中大版本有Python2和Python3之分，Python2最经典的版本为Python2.7，Python3有Python3.4，Python3.6等等。效率低： 由于Python是解释性语言，动态编译，直到代码执行时，才加以解释，相比于编译型语言，可以生成编译代码，执行效率低。 三元运算符$max = x &gt; y ? x : y$ Java和 C中正确，在Python中的三元运算符不是这样的，是$max=x \ if \ x&gt;y \ else\ y​$ 标识符python标识符可以使用下划线 字母 数字组成,但是数字不允许作为标识符的开头出现。 线程协程线程由操作系统控制，协程由程序自身控制。 编译及解码 Python中字符串编译的过程：gbk=&gt;unicode=&gt;utf16=&gt;url 解码 字符串解码顺序为：url解码=&gt;utf16=&gt;unicode=&gt;gbk map函数map() 会根据提供的函数对指定序列做映射。第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表。例如：1234567891011&gt;&gt;&gt;def square(x) : # 计算平方数... return x ** 2... &gt;&gt;&gt; map(square, [1,2,3,4,5]) # 计算列表各个元素的平方[1, 4, 9, 16, 25]&gt;&gt;&gt; map(lambda x: x ** 2, [1, 2, 3, 4, 5]) # 使用 lambda 匿名函数[1, 4, 9, 16, 25] # 提供了两个列表，对相同位置的列表数据进行相加&gt;&gt;&gt; map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])[3, 7, 11, 15, 19] 逻辑运算符 数据类型 结束符C语言中字符串使用‘\0’作为结束符以防止越界，但python中字符串其实是一个固定长度的字符数组，并不需要结束符。 math.floor函数及除法 Python3 中math.floor() 函数的返回值应为整型，而Python2 的 math.floor() 函数返回值是浮点型。 Python2 中除法默认向下取整，为整型；Python3 中的除法为正常除法，会保留小数位，为浮点型。 Python 中万物皆为对象，函数也不例外，函数作为对象可以赋值给一个变量、可以作为元素添加到集合对象中、可作为参数值传递给其它函数，还可以当做函数的返回值。 变量 Python 是弱类型脚本语言，变量就是变量，没有特定类型，因此不需要声明。 但每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。 用 del 语句可以释放已创建的变量（已占用的资源）。 切片slicePython 中 切片（Slice）功能的理解：L[start : stop [ : step]]start 默认值是 0；stop 默认值为 L 的长度；step 默认值是 1。 命名方式]]></content>
      <categories>
        <category>牛客笔试题</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之K-means聚类]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-14-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8BK-means%2F</url>
    <content type="text"><![CDATA[聚类(Clustering)，就是将相似的事物聚集在一 起，而将不相似的事物划分到不同的类别的过程，是数据分析之中十分重要的一种手段。与此前介绍的决策树，支持向量机等监督学习不同，聚类算法是非监督学习(unsupervised learning )，在数据集中，并不清楚每条数据的具体类别。 算法K-means 算法是数据挖掘十大经典算法之一。由于该算法的效率高，所以在对大规模数据进行聚类时被广泛应用。目前，许多算法均围绕着该算法进行扩展和改进。 k-means 算法接受一个参数 k ，表示将数据集中的数据分成 k 个聚类。在同一个聚类中，数据的相似度较高；而不同聚类的数据相似度较低。 算法的步骤：1. 选择任意 k 个数据，作为各个聚类的质心，（质心也可以理解为中心的意思），执行步骤 2；2. 对每个样本进行分类，将样本划分到最近的质心所在的类别（欧氏距离），执行步骤 3；3. 取各个聚类的中心点作为新的质心，执行步骤 2 进行迭代。迭代结束的条件：1. 当新的迭代后的聚类结果没有发生变化；2. 当迭代次数达到预设的值。算法流程图： 实例分析有如下4种药物，我们要根据其2个特征值对其进行分类，事先并不知道它们属于何种类别。聚类后分为2类（1 和 2） 按照之前的算法流程，我们将4种药划分为了2类，聚类过程如下： 代码实现本部分我们将使用和上面实例分析中一致的数据，采用2种方法实现k-means聚类。 自己实现代码主要包含4个小方法，分别是： shouldStop()：聚类迭代的终止条件 updateLabels()：更新迭代后数据的类标签 getLabelFromClosestCenterpoints()：计算各数据到中心点的距离，选取最近距离更新数据类标签 getCenterpoints()：根据聚类结果选取新的中心点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import numpy as npdef kmeans(x,k,maxIt): ''' :param x: 待分类数据 :param k: 最终分为几类 :param maxIt: 最大迭代次数 :return: 分好类的数据 ''' numpoints,numDim = x.shape #数据行数和列数 dataSet = np.zeros((numpoints,numDim+1)) # 创建一个新数组存储分类好的数据 dataSet[:,:-1] = x # 将数据x赋值给dataSet的前n-列 centerpoints = dataSet[np.random.randint(numpoints,size=k),:] # 随机选取k个中心点 centerpoints = dataSet[0:2, :] # 强制选取前2条数据作为中心点，为了对照实例分析 centerpoints[:,-1] = range(1,k+1) # 为选好的中心点数据打上标签 iterations = 0 # 迭代次数 oldCenterpoints = None # 调用函数循环迭代，实现聚类 while not shouldStop(oldCenterpoints,centerpoints,iterations,maxIt): # 输出每次迭代的聚类过程 print("iterations: \n",iterations) print("dataSet: \n",dataSet) print("centerpoints: \n",centerpoints) # 将原始中心点复制存储，方便迭代完后，比较新旧中心点是否发生变化 oldCenterpoints =np.copy(centerpoints) iterations += 1 # 调用方法更新每条数据的类标签 updateLabels(dataSet,centerpoints) # 根据每一次迭代后的聚类结果，重新选取新的中心点 centerpoints = getCenterpoints(dataSet,k) return dataSet# 聚类迭代的终止条件def shouldStop(oldCenterpoints,centerpoints,iterations,maxIt): ''' :param oldCenterpoints: 迭代前的中心点 :param centerpoints: 迭代后的中心点 :param iterations: 当前迭代次数 :param maxIt: 最大迭代次数 :return: True或False ''' if iterations &gt; maxIt: # 超出设定好的最大迭代次数 return True return np.array_equal(oldCenterpoints,centerpoints) # 判断迭代前后中心点是否发生了变化# 更新迭代后数据的类标签def updateLabels(dataSet,centerpoints): ''' :param dataSet: 数据 :param centerpoints: 中心点 ''' numpoints,numDim = dataSet.shape for i in range(0,numpoints): # 调用方法循环更新数据的类标签 dataSet[i,-1] = getLabelFromClosestCenterpoints(dataSet[i,:-1],centerpoints)# 根据计算各数据到中心点的距离，选取最近距离更新数据类标签def getLabelFromClosestCenterpoints(dataSetRow,centerpoints): ''' :param dataSetRow: 待更新类标签的数据 :param centerpoints: 中心点 :return: 数据新的类标签 ''' label = centerpoints[0,-1] # 选取初始类标签 minDist = np.linalg.norm(dataSetRow - centerpoints[0,:-1]) # 计算和当前中心点的距离 # 循环计算数据和每个中心点的距离，选取最近的更新类标签 for i in range(1,centerpoints.shape[0]): dist = np.linalg.norm(dataSetRow - centerpoints[i,:-1]) # 计算距离 if dist &lt; minDist: minDist = dist label = centerpoints[i,-1] print("minDist: ",minDist) return label# 根据聚类结果选取新的中心点def getCenterpoints(dataSet,k): ''' :param dataSet: 数据 :param k: 最终分为几类 :return: 新的中心点 ''' result = np.zeros((k,dataSet.shape[1])) for i in range(1,k+1): oneCluster = dataSet[dataSet[:,-1]==i,:-1] # 同类数据按行求均值，算出新的中心点 result[i-1,:-1] = np.mean(oneCluster,axis=0) result[i-1,-1] = i # 打上标签 return result# 创建测试数据x1 = np.array([1,1])x2 = np.array([2,1])x3 = np.array([4,3])x4 = np.array([5,4])test_x = np.vstack((x1,x2,x3,x4)) #沿着列方向将矩阵堆叠起来result = kmeans(test_x,2,10)print("final result: \n",result) 程序运行结果如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647iterations: 0dataSet: [[1. 1. 1.] [2. 1. 2.] [4. 3. 0.] [5. 4. 0.]]centerpoints: [[1. 1. 1.] [2. 1. 2.]]minDist: 0.0minDist: 0.0minDist: 2.8284271247461903minDist: 4.242640687119285iterations: 1dataSet: [[1. 1. 1.] [2. 1. 2.] [4. 3. 2.] [5. 4. 2.]]centerpoints: [[1. 1. 1. ] [3.66666667 2.66666667 2. ]]minDist: 0.0minDist: 1.0minDist: 0.4714045207910319minDist: 1.885618083164127iterations: 2dataSet: [[1. 1. 1.] [2. 1. 1.] [4. 3. 2.] [5. 4. 2.]]centerpoints: [[1.5 1. 1. ] [4.5 3.5 2. ]]minDist: 0.5minDist: 0.5minDist: 0.7071067811865476minDist: 0.7071067811865476final result: [[1. 1. 1.] [2. 1. 1.] [4. 3. 2.] [5. 4. 2.]] 通过比较，可以发现结果和我们在实例分析中的一致。 Sklearn实现1234567891011121314from sklearn import clusterimport numpy as np# 创建测试数据x1 = np.array([1,1])x2 = np.array([2,1])x3 = np.array([4,3])x4 = np.array([5,4])test_x = np.vstack((x1,x2,x3,x4)) #沿着列方向将矩阵堆叠起来sk = cluster.KMeans(2)sk.fit(test_x)print("中心点：\n",sk.cluster_centers_)print("类别：\n",sk.labels_) 程序运行结果如下：12345中心点： [[4.5 3.5] [1.5 1. ]]类别： [1 1 0 0] 最后的中心点一致，也成功分为了2类 优缺点优点： 速度快，复杂度低，为 O(Nkq)，N是数据总量，k是类别数，q是迭代次数。一般来讲k、q会比N小得多，那么此时复杂度相当于O(N) ，在各种算法中是算很小的； 原理简单，易于理解。 缺点： 对异常点敏感； 局部最优解而不是全局优，(分类结果与初始点选取有关);不能发现非凸形状的聚类。 K-means++算法2007年由D. Arthur等人提出的K-means++算法在k-means的基础上做了进一步的改进。可以直观地将这改进理解成这K个初始聚类中心相互之间应该分得越开越好。整个算法的描述如下图所示： 下面结合一个简单的例子说明K-means++是如何选取初始聚类中心的。数据集中共有8个样本，分布以及对应序号如下图所示： 假设经过图2的步骤一后6号点被选择为第一个初始聚类中心，那在进行步骤二时每个样本的D(x)和被选择为第二个聚类中心的概率如下表所示： 其中的$P(x)$就是每个样本被选为下一个聚类中心的概率。最后一行的$Sum$是概率$P(x)$的累加和，用于轮盘法选择出第二个聚类中心。方法是随机产生出一个0~1之间的随机数，判断它属于哪个区间，那么该区间对应的序号就是被选择出来的第二个聚类中心了。例如1号点的区间为[0,0.2)，2号点的区间为[0.2, 0.525)。 从上表可以直观的看到第二个初始聚类中心是1号，2号，3号，4号中的一个的概率为0.9。而这4个点正好是离第一个初始聚类中心6号点较远的四个点。这也验证了K-means的改进思想：即离当前已有聚类中心较远的点有更大的概率被选为下一个聚类中心。可以看到，该例的K值取2是比较合适的。当K值大于2时，每个样本会有多个距离，需要取最小的那个距离作为$D(x)​$。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>k-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回归中的相关度和决定系数]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-13-%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%2F</url>
    <content type="text"><![CDATA[训练集中可能是有若干维度的特征。但有时并不是所有特征都是有用的，有的特征其实和结果并没有关系。因此需要一个能衡量自变量和因变量之间的相关度。 皮尔逊相关系数皮尔逊相关系数(Pearson correlation coefficient），是用于度量两个变量 X 和 Y 之间的相关（线性相关），其值介于[-1,1] 之间。有三种相关情况： 正向相关: &gt;0 负向相关：&lt;0 无相关性：=0 下图从左到右分别代表了正向相关、无相关性和负向相关： 在介绍皮尔逊相关系数之前，要先理解协方差(Covariance ) ，协方差是一个反映两个随机变量相关程度的指标，如果一个变量跟随着另一个变量同时变大或者变小，那么这两个变量的协方差就是正值，反之相反，公式如下：$$Conv(X,Y) = \frac{\sum_{i=1}^{n}(X_i-\overline{X})(Y_i-\overline{Y})}{n-1}$$皮尔逊相关系数的公式如下：$$r_{xy} = \frac{Conv(X,Y)}{\sqrt{Var(X)Var(Y)}} = \frac{\sum(x-\overline{x})(y-\overline{y})}{\sqrt{\sum{(x-\overline{x})^2}\sum(y-\overline{y})^2}}$$Var表示方差，相关度越高，皮尔逊相关系数其值趋于 1 或 -1 （趋于1表示它们呈正相关， 趋于 -1 表示它们呈负相关）；如果相关系数等于0，表明它们之间不存在线性相关关系。 决定系数决定系数即 R 平方值，反应因变量的全部变异能通过回归关系被自变量解释的比例。如R平方为0.8，则表示回归关系可以解释因变量80%的变异。换句话说，如果我们能控制自变量不变，则因变量的变异程度会减少 80%。 在简单线性回归中，决定系数可以是 R^2 = r * r。而更通用的是： SST 其实是两部分组成的，一部分是模型可预测的SSR，一部分是变异的SSError无法用模型解释的。它们之间的计算公式是: 注意: R平方也有其局限性：R平方随着自变量的增加会变大，R平方和样本量是有关系的。因此，我们要到R平方进行修正。修正的方法：$$\overline{R^2} = 1-(1-R^2)\frac{n-1}{n-p-1}$$其中，n 表示样本大小，p 表示模型中解释变量的总数（不包括常数）。 代码实例代码完全按照上述中的公式计算12345678910111213141516171819202122232425262728293031323334353637383940import numpy as npimport mathfrom sklearn import linear_model#计算皮尔逊相关系数( Pearson correlation coefficient）def computer_conv(x,y): var_x = 0 var_y = 0 SSR = 0 x_bar = np.mean(x) # x的方差 y_bar = np.mean(y) # y的方差 for i in range(len(x)): diff_xbar = x[i] - x_bar diff_ybar = y[i] - y_bar SSR += diff_xbar * diff_ybar var_x += diff_xbar**2 var_y += diff_ybar**2 SST = math.sqrt(var_x*var_y) return SSR/SST#计算决定系数R平方值def computer_r(x,y): SSR = 0 SST = 0 linear = linear_model.LinearRegression() # 创建线性模型 linear.fit(x,y) y_hat = linear.predict(x) y_mean = np.mean(y) for i in range(len(x)): SSR += (y_hat[i] - y_mean)**2 SST += (y[i] - y_mean)**2 return SSR/SSTtest_x = [1,3,8,7,9]test_y = [10,12,24,21,34]test_x2 = [[x] for x in test_x]print("r: ",computer_conv(test_x,test_y))print("r平方: ",computer_conv(test_x,test_y)**2)print("R平方: ",computer_r(test_x2,test_y)) 程序运行结果123r: 0.94031007654487r平方: 0.8841830400518192R平方: 0.8841830400518192 我们发现：在简单线性回归中，决定系数的确满足$ R^2 = r * r$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>Conv&amp;&amp;R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之非线性回归（ Logistic Regression）]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-13-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8B%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[非线性回归是线性回归的延伸，其目标预测函数不是线性的。本文主要介绍逻辑回归（Logistic Regression），它是非线性回归的一种，虽然名字中有“回归”二字，但其本质上是一个分类模型。 含义我们知道，线性回归的模型是求出输出特征向量Y和输入样本矩阵X之间的线性关系系数θ，满足$Y=Xθ$。此时Y是连续的，所以是回归模型。如果Y是离散的话，如何解决？一个可以想到的办法是，我们对于Y再做一次函数转换，变为g(Y)。如果我们令g(Y)的值在某个实数区间的时候是类别A，在另一个实数区间的时候是类别B，以此类推，就得到了一个分类模型。如果结果的类别只有两种，那么就是一个二元分类模型了。逻辑回归的出发点就是从这来的。 看如下实例有这么几组医疗数据，X特征是肿瘤的大小（连续型），Y是其良恶性（离散型），Y只有0（良性）和1（恶性）2种取值。我们选取阈值0.5，h（x）&gt;0.5（恶性），Malignant=1，反之为0，良性。 我们选取阈值0.2，h（x）&gt;0.2（恶性），Malignant=1，反之为0，良性。 比较上述两种情况，新的数值加入时需要不断调整阈值，说明用线性的方法进行回归不太合理。 基本模型我们假设测试数据为$X(x_0,x_1,…,x_n)$需要学习的参数为$\Theta(\theta_0,\theta_1,…,\theta_n)$ 给定函数 $$Z = \theta_0 + \theta_1x_1 + \theta_2x_2+…+\theta_nx_n$$向量化可表示为$$Z = \Theta^TX$$经常需要一个分界线作为区分两类结果。再次需要一个函数进行曲线平滑化，由此引入Sigmoid 函数进行转化： $$g(z) = \frac{1}{1+e^{-z}}$$这样的，可以以 0.5 作为分界线。因此逻辑回归的最终目标函数就是：$$h_\theta(X) = g(\theta_0 + \theta_1x_1 + \theta_2x_2+…+\theta_nx_n) = g(\theta^TX) = \frac{1}{1+e^{-\theta^TX}}$$回归是用来得到样本属于某个分类的概率。因此在分类结果中，假设 y 值是 0 或 1，那么正例 (y = 1): $$h_\theta(X) = P(y=1|X;\theta)$$反例(y = 0):$$1 - h_\theta(X) = P(y=0|X;\theta)$$在线性回归中，我们要找到合适的$\theta^{(i)}$使下面的损失函数值最小：$$Cost(h_\theta(x^{(i)}),y^{(i)}) = \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$$如果在逻辑回归中运用上面这种损失函数，得到的函数 J 是一个非凸函数，存在多个局部最小值，很难进行求解，因此需要换一个 cost 函数。重新定义个 cost 函数如下：$$Cost(h_\theta(x^{(i)}),y^{(i)}) = -\frac{1}{m}[\sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\theta(x^{(i)})))]$$ 求解方法我们采用梯度下降法求解最佳解。梯度下降法的计算过程就是沿梯度下降的方向求解极小值。 先确定向下一步的步幅大小，称之为Learning rate; 任意给定一个初始值：$\theta_0,\theta_1,…,\theta_n$; 确定一个向下的方向，并向下走预先规定的步伐，并更新$\theta_0,\theta_1,…,\theta_n$; 当下降的高度小于某个定义的阈值时，停止下降更新。 这就好比是下山，下一步的方向选的是最陡的方向。梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。θ 的更新方程如下：$$\theta_j = \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)$$其中，偏导是：$$\frac{\partial}{\partial\theta_j}J(\theta) = \frac{1}{m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})x_j^{(i)}$$ 代码实例本部分我们将采用2种方式实现逻辑回归模型，一种自己编写函数方法，一种调用sklearn中的方法库（LogisticRegression）。 自己实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import numpy as npimport randomimport matplotlib.pyplot as plt#sigmod函数，将值量化到0-1def sigmoid(x): return 1.0 / (1 + np.exp(-x))# 梯度下降算法def gradientDescent(x, y, theta, alpha, m, numIteration): ''' :param x: 输入实例 :param y: 分类标签 :param theta: 学习的参数 :param alpha: 学习率 :param m: 实例个数 :param numIteration: 迭代次数 :return: 学习擦参数theta ''' xTrans = x.transpose() # 矩阵的转置 J = [] # 存储损失的列表，方便绘图 for i in range(0, numIteration): hypothsis = sigmoid(np.dot(x, theta)) #量化到0-1 loss = hypothsis - y #计算误差 cost = np.sum(loss ** 2) / (2 * m) #计算损失 J.append(cost) # 将损失存入列表 print("Iteration %d / Cost:%f" % (i, cost)) gradient = np.dot(xTrans, loss) / m theta = theta - alpha * gradient # 更新梯度 plt.plot(J) # 可视化损失变化 plt.show() return theta# 创建数据，用作测试def genData(numPoints, bais, variance): ''' :param numPoints: 输入实例数目 :param bais: 偏向 :param variance: 方差 :return 创建的数据 x,y ''' # 实例（行数）、偏向、方差 x = np.zeros(shape=(numPoints, 2)) # 初始化numPoints行2列(x1,x2)的全零元素矩阵 y = np.zeros(shape=numPoints) # 归类标签 y_list = [0,1] # 标签列表y的取值 for i in range(0, numPoints): x[i][0] = random.uniform(0, 1) * variance # 创建X的特征对 x[i][1] = (i + bais) + random.uniform(0, 1) * variance random.shuffle(y_list) # 给X特征对附上随机的0 1 标签 y[i] =y_list[0] return x, yx, y = genData(100, 2, 5)#print(x)#print(y)#x和y的维度m, n = np.shape(x)n_y = np.shape(y)numIteration = 100000alpha = 0.0005theta = np.ones(n) # 初始化thetatheta = gradientDescent(x, y, theta, alpha, m, numIteration)print(theta) #output [-0.14718538 0.00381781] Sklearn实现123456from sklearn.linear_model import LogisticRegression# 调用sklearn自带的逻辑回归方法sk = LogisticRegression(max_iter=100000)sk.fit(x, y) # 此处的x和y与上面自己实现的一致print(sk.intercept_)print(sk.coef_) #output [-0.11612453 0.00272452] 我们发现自己闪现求解的$\theta$值与sklearn中的相比，还是存在一定误差的，这是因为sklearn中的方法 LogisticRegression有很多参数进行了详细的优化，详情参见https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn-linear-model-logisticregression Cost损失可视化 模型优缺点优点： 适合需要得到一个分类概率的场景 计算代价不高，容易理解实现。LR在时间和内存需求上相当高效 LR对于数据中小噪声的鲁棒性很好 缺点： 容易欠拟合，分类精度不高 对异常值敏感]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>Unlinear regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之线性回归（LR）]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-12-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[线性回归(linear regression)是利用数理统计和归回分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。与之前的分类问题( Classification )不一样的是，分类问题的结果是离散型的；而回归问题中的结果是连续型（数值）的。 数据特征数理统计中，常用的描述数据特征的有： 均值（mean）：又称平均数或平均值，是计算样本中算术平均数：$$\overline{x} = \frac{\sum_{i=1}^{n}x_i}{n}$$ 中位数（median）：将数据中的各个数值按照大小顺序排列，居于中间位置的变量。当n为基数的时候：直接取位置处于中间的变量；当n为偶数的时候，取中间两个量的平均值。 众数（mode）：数据中出现次数最多的数。 方差( variance )：一种描述离散程度的衡量方式： $$s^2 = \frac{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}{n-1}$$ 标准差 （standard deviation） :将方差$s^2​$开方就能得到标准差。 简单线性回归算法简单线性回归(Simple Linear Regression ),，也就是一元线性回归，包含一个自变量x 和一个因变量y 。常被用来描述因变量(y)和自变量(X)以及偏差(error)之间关系的方程叫做回归模型，这个模型是：$$y = \beta_0 + \beta_1x + \epsilon$$其中偏差 $\epsilon$ 满足正态分布的。因此它的期望值是 0 。 $\epsilon$ 的方差(variance)对于所有的自变量 x 是一样的。 等式两边求期望可得： $$E(y) = \beta_0 + \beta_1x$$其中，β0 是回归线的截距，β1 是回归线的斜率，E(y) 是在一个给定 x 值下 y 的期望值（均值）。 假设我们的估值函数（注意，是估计函数）:$$\widehat{y} = b_0 + b_1x$$b0是估计线性方程的纵截距，b1是估计线性方程的斜率，ŷ是在自变量x等于一个给定值的时候，y的估计值。b0和b1是对β0和β1的估计值。 线性回归的分析流程 如何寻找最佳回归线 寻找的回归方程，要求与真实值的离散程度是最小的： $$min\sum_{i=0}^{n}(y_{i}-\widehat {y})^2$$$$b_1 = \frac{\sum(x_{i}-\overline{x})(y_{i}-\overline{y})}{\sum(x_{i}-\overline{x})^2}$$$$b_0 = \overline{y} - b_1\overline{x}$$ 上面方法类似于数值分析中的最小二乘法，详细推导可见https://zh.wikipedia.org/wiki/最小二乘法 代码实例123456789101112131415161718192021222324252627#简单现行回归：只有一个自变量 y=k*x+b 预测使 (y-y*)^2 最小import numpy as npdef SLR(x:list,y:list): n =len(x) fenzi = 0 fenmu = 0 #按照上面推导的公式计算出估计回归方程中的b0，b1 for i in range(n): fenzi += (x[i]-np.mean(x)) * (y[i]-np.mean(y)) fenmu += (x[i]-np.mean(x)) **2 #print("fenzi: " + str(fenzi)) #print("fenmu: " + str(fenmu)) b1 = fenzi/fenmu # 斜率 b0 = np.mean(y) - b1 * np.mean(x) # 截距 return b0,b1#测试def predict(x,b0,b1): return b0 + b1*xif __name__ == "__main__": x = [1,3,2,1,3] y = [14,24,18,17,27] b0,b1 = SLR(x,y) y_predict = predict(6,b0,b1) print("y_predict: " + str(y_predict)) 多元线性回归算法当自变量有多个时，回归模型就变成了: $$E(y) = \beta0 + \beta_1x_1 +…+\beta_nx_n$$估值函数的自变量也变多：$$\widehat {y} = b_0 + b_1x_1+ b_2x_2+…+b_nx_n$$令矩阵：函数可以转换为：$$h(x) = B^TX$$如果有训练数据:$$D = (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…, (x^{(m)},y^{(m)}),$$损失函数( cost function )，寻找的目标函数应该尽可能让损失函数小，这个损失函数为：$$J(B) = \frac{1}{2m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2 = \frac{1}{2m}(XB-y)^T(XB-y)$$目的就是求解出一个使得代价(损失)函数最小的 B。 利用梯度下降求解估值函数梯度下降算法是一种求局部最优解的方法，详见https://www.cnblogs.com/pinard/p/5970503.html ，今后的博客也会详细介绍。B 由多个元素组成，对于损失函数可以求偏导如下： $$\frac{\partial}{\partial{B_j}} = \frac{1}{m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})x_{j}^{(i)}$$更新B：$$B_{j} = B_{j} - \alpha\frac{\partial}{\partial{B_j}}J(B)$$其中α 表示学习率，决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。 最小二乘法求解估值函数多元多项式的情况一样可以利用最小二乘法来进行求解，将m条数据都带入估值函数可以得到线性方程组:向量化后可以表示为：我们要让$$\sum_{i=1}^{m} = (y_i-\widehat{y})^2$$的值最小，则系数矩阵中，各个参数的偏导的值都会0，因此可以得到 n 个等式：得到正规方程组： 写成矩阵表示为:$$X^TXB = X^TY$$解得：$$B = (X^TX)^{-1}X^TY$$ 代码实例本部分我们会用2种方式实现多元线性回归(Multiple Linear Regression )，一种是sklearn库包中自带的方法，一种是我们依据最小二乘法实现的求解方法。应用实例：一家快递公司送货：X1： 运输里程 X2： 运输次数 Y：总运输时间将其数据存储为csv文件如下： sklearn实现123456789101112131415161718192021from numpy import genfromtxtfrom sklearn import linear_modeldatapath = 'G:/PycharmProjects/Machine_Learning/Linear_Regression/data1.csv'data = genfromtxt(datapath,delimiter=',')x = data[:,:-1]y = data[:,-1]# print(x)# print(y)mlr = linear_model.LinearRegression()mlr.fit(x,y)# print (mlr)# print ("coef: " + str(mlr.coef_) )# print ("intercept: " + str(mlr.intercept_))x_predict = [50,3]y1_predict = mlr.predict([x_predict])print("y_predict: " + str(y1_predict))#output [4.95830457] 最小二乘法实现12345678910111213141516171819202122232425262728293031import numpy as npfrom numpy import genfromtxtclass MyLinearRegression(object): def __init__(self): self.b = [] def fit(self, x: list, y: list): # 为每条训练数据前都添加 1 tmpx = [[1] for _ in range(len(x))] for i, v in enumerate(x): tmpx[i] += v x_mat = np.mat(tmpx) y_mat = np.mat(y).T xT = x_mat.T self.b = (xT * x_mat).I * xT * y_mat def predict(self, x): return np.mat([1] + x) * self.bdatapath = 'G:/PycharmProjects/Machine_Learning/Linear_Regression/data1.csv'data = genfromtxt(datapath,delimiter=',')x = data[:,:-1]y = data[:,-1]test_row = [50, 3]linear = MyLinearRegression()linear.fit(x, y)print(linear.predict(test_row))#output[ 4.95830457] 补充：上面实例中，x的特征都是连续数值，如果有离散型的特征（车型）,我们可以采取如下方法，将车型特征进行one-hot编码，代码不需要变化。 参考资料多元线性回归及其优化算法多元线性回归分析理论详解]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>linear regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之神经网络（NN）]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-10-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8BNN%2F</url>
    <content type="text"><![CDATA[神经网络算法( Neural Network )是机器学习中非常非常重要的算法。它 以人脑中的神经网络为启发，是整个深度学习的核心算法。深度学习就是根据神经网络算法进行的一个延伸。 背景神经网络是受神经元启发的，对于神经元的研究由来已久，1904年生物学家就已经知晓了神经元的组成结构。一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。 1949年心理学家Hebb提出了Hebb学习率，认为人脑神经细胞的突触（也就是连接）上的强度上可以变化的。于是计算科学家们开始考虑用调整权值的方法来让机器学习。这为后面的学习算法奠定了基础。 1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字–感知器（ Perceptron ）。 1986年，Rumelhar和Hinton等人提出了反向传播（ Backpropagation ，BP）算法，这是最著名的一个神经网络算法。 组成多层神经网咯主要由三部分组成：输入层(input layer), 隐藏层 (hidden layers), 输入层 (output layers)。 每层由单元(units)组成， 输入层(input layer)是由训练集的实例特征向量传入，经过连接结点的权重(weight)传入下一层，一层的输出是下一层的输入。 隐藏层(hidden layers)的个数可以是任意的，输入层有一层，输出层(output layers)有一层。每个单元(unit)也可以被称作神经结点。上图为2层的神经网络（一般输入层不算）。一层中加权的求和，然后根据非线性方程转化输出。作为多层向前神经网络，理论上，如果有足够多的隐藏层(hidden layers) 和足够大的训练集, 可以模拟出任何方程。神经网络可以用来解决分类(classification）问题，也可以解决回归( regression )问题。 单层到多层的神经网络由两层神经网络构成了单层神经网络，它还有个别名——— 感知机（Perceptron）。 上图中，有3个输入，连接线的权值分别是 w1, w2, w3。将输入与权值进行乘积然后求和，作为 z单元的输入，如果 z 单元是函数 g ，那么就有 z = g(a1 * w1 + a2 * w2 + a3 * w3) 。 单层神经网络的扩展(2个z单元)，也是一样的计算方式： 在多层神经网络中，只不过是将输出作为下一层的输入，一样是乘以权重然后求和： 设计在使用神经网络训练数据之前，必须确定神经网络的层数以及每层单元的个数。特征向量在被传入输入层时通常被先标准化(normalize）到0和1之间 （为了加速学习过程）。离散型变量可以被编码成每一个输入单元对应一个特征值可能赋的值例如：特征值A可能取三个值（$a_0, a_1, a_2$), 可以使用3个输入单元来代表A。 如果$A=a_0$, 那么代表$a_0$的单元值就取1，其他取0，[1, 0, 0] 如果$A=a_1$, 那么代表$a_1$的单元值就取1，其他取0，[0, 1, 0] 如果$A=a_2$, 那么代表$a_2$的单元值就取1，其他取0，[0, 0, 1] 对于分类问题，如果是2类，可以用一个输出单元表示（0和1分别代表2类）。如果多余2类，每一个类别用一个输出单元表示，所以输入层的单元数量通常等于类别的数量。一般没有明确的规则来设计最好有多少个隐藏层，通常是根据实验测试和误差，以及准确度来实验并做出改进。 交叉验证模型训练结束后，如何来衡量我们模型的泛化能力（测试集上准确率）呢？常见的方法是将数据集分为两类，训练集和测试集，利用测试集的数据将模型的预测结果与真实测试标签作对比，得出准确度。下面介绍另一个常用但更科学的方法———交叉验证方法( Cross-Validation )。这个方法不局限于将数据集分成两份。如上图所示，我们可以将原始数据集分成 k 份。第一次用第一份作为训练集，其余作为测试集，得出这一部分的准确度 ( evaluation )。第二次再用第二份作为训练集，其余作为测试集，得出这第二部分的准确度。以此类推，最后取各部分准确度的平均值作为最终的模型衡量指标。 BP算法BP 算法 (BackPropagation )是多层神经网络的训练一个核心的算法。目的是更新每个连接点的权重，从而减小预测值(predicted value )与真实值 ( target value )之间的差距。输入一条训练数据就会更新一次权重，反方向（从输出层=&gt;隐藏层=&gt;输入层）来以最小化误差（error）来更新权重（weitht）。在训练神经网络之前，需要初始化权重(weights )和偏向( bias )，初始化是随机值， -1 到 1 之间或者-0.5到0.5之间，每个单元有一个偏向。 BP算法有2个过程，前向传播和反向传播，后者是关键。我们以下图为例，分析BP算法的整个过程。 前向传播利用上一层的输入，得到本层的输入:$$I_j = \sum_{i}w_{i,j}O_{i} + \theta_{j}$$其中$w_{i,j}$表示权重，$O_{j}$表示当前神经单元的值，$\theta_{j}$为当前神经单元的偏置向。$i=0$时，$O_{j}$即为输入单元的值。得到输入值后，神经元要怎么做呢？我们先将单个神经元进行展开如图： 隐藏层单元得到值后进行累加求和，然后需要进行一个非线性转化，这个转化在神经网络中称为激活函数( Activation function )，这个激活函数是一个 S(下面会有所介绍) 函数，图中以 f 表示，它的函数为：$$O_{j} = \frac{1}{1+e^{-I_j}}$$ 反向传播前向传播结束后，我们要计算误差，然后根据误差(error)反向传送，更新权重和偏置向。对于输出层的误差：$$Err_{j} = O_{j}(1-O_{j})(T_{j}-O_{j})$$其中$O_{j}$表示预测值，$T_{j}$表示真实值。 对于隐藏层的误差：$$Err_{j} = O_{j}(1-O_{j})\sum_{k}Err_{k}w_{j,k}$$其中$Err_{k}$为后层单元误差，$w_{j,k}$为当前单元与后层单元的权重值。 更新权重：$$\Delta w_{i,j} = (l)Err_{j}O_{i}$$ $$w_{i,j} = w_{i,j} + \Delta w_{i,j}$$更新偏置向：$$\Delta\theta{j} = (l)Err_{j}$$ $$\theta{j} = \theta_{j} + \Delta\theta_{j}$$上面$（l）$为学习率 终止条件 权重的更新低于某个阈值，这个阈值是可以人工指定的； 预测的错误率低于某个阈值； 达到预设一定的循环次数。 S型函数（sigmod）神经元在对权重进行求和后，需要进行一个非线性转化，即作为参数传入激活函数去。这个激活函数是一个 S 型函数(Sigmoid)。S 函数不是指某个函数，而是一类函数,详解可参考 https://zh.wikipedia.org/wiki/S函数 。下面有两个常见的S 函数： 双曲函数（tanh ） 逻辑函数（logistic function ）Sigmod函数S 曲线函数可以将一个数值转为值域在 0 到 1 之间，广义上S 函数是满足y值在某个值域范围内渐变的一条曲线。 双曲函数（tanh） 双曲函数是一类与常见的三角函数（也叫圆函数）类似的函数。详见https://zh.wikipedia.org/wiki/双曲函数 。$$tanhx = \frac{sinhx}{coshx}$$导数为$$\frac{d}{dx}tanhx = 1 - tanh^2x =sech^2x = \frac{1}{cosh^2x}$$ 逻辑函数（logistic function ） 逻辑函数或逻辑曲线是一种常见的S函数，详见https://zh.wikipedia.org/wiki/逻辑函数 。一个常见的逻辑函数可以表示如下：$$f(x) = \frac{1}{1 + e^{-x}}$$导数为 $$\frac{d}{dx} = \frac{e^x}{(1+e^x)^2} = f(x)(1 - f(x))$$ BP算法举例假设有一个两层的神经网络，结构，权重和数据集如下： 根据上述公式（3）到（8）计算误差和更新权重： 代码实例（手写数字识别）基础函数根据神经网络原理，代码实现一个完整的神经网络来对手写数字图片分类。（完整代码见附录）根据上面的sigmoid 函数以及其求导得到对应的python代码：123456789101112def tanh(x): return np.tanh(x)def tanh_deriv(x): return 1.0 - np.tanh(x) * np.tanh(x)def logistic(x): return 1.0 / (1 + np.exp(-x))def logistic_deriv(x): fx = logistic(x) return fx * (1 - fx) 网络函数在网络函数中，需要确定神经网络的层数，每层的个数，从而确定单元间的权重规格和单元的偏向。12345678910111213141516171819202122def __init__(self, layers, activation='logistic'): if activation == 'tanh': self.activation = tanh self.activation_deriv = tanh_deriv elif activation == 'logistic': self.activation = logistic self.activation_deriv = logistic_deriv # 初始化随即权重 self.weights = [] # len(layers)layer是一个list[2,2,1]，则len(layer)=3 # 除了输出层,其它层层之间的单元都要赋予一个随机产生的权重 for i in range(len(layers) - 1): tmp_weights = (np.random.random([layers[i], layers[i + 1]]) * 2 - 1) * 0.25 self.weights.append(tmp_weights) #初始化偏置向 #除了输入层，其它层单元都有一个偏置向 self.bias = [] for i in range(1, len(layers)): tmp_bias = (np.random.random(layers[i]) * 2 - 1) * 0.25 self.bias.append(tmp_bias) 其中，layers 参数表示神经网络层数以及各个层单元的数量，例如下图这个模型：上述网络模型就对应了 layers = [4, 3, 2] 。这是一个 2 层，即len(layers) - 1层的神经网络。输入层 4 个单元，其他依次是 3 ，2 。权重是表示层之间单元与单元之间的连接。因此权重也有 2 层。第一层是一个4 x 3的矩阵，即layers[0] x layers[1]。同理，偏向也有 2 层，第一层个数 3 ，即leyers[1] 。利用np.random.random([m, n]) 来创建一个 m x n 的矩阵。在这个神经网络的类中，初始化都随机-0.25 到 0.25之间的数。 训练函数在神经网络的训练中，需要先设定一个训练的终止条件，前面介绍了3种停止条件，这边使用的是 达到预设一定的循环次数 。每次训练是从样本中随机挑选一个实例进行训练，将这个实例的预测结果和真实结果进行对比，再进行反向传播得到各层的误差，然后再更新权重和偏向：12345678910111213141516171819202122232425262728293031323334353637383940def fit(self, X, y, learning_rate=0.2, epochs=10000): #X：数据集,确认是二维，每行是一个实例，每个实例有一些特征值 X = np.atleast_2d(X) #每个实例的标签 y = np.array(y) # 随即梯度 for k in range(epochs): i = np.random.randint(X.shape[0]) a = [X[i]] # 随即取某一条实例 ''' #正向传播计算各单元的值 #np.dot代表两参数的内积，x.dot(y) 等价于 np.dot(x,y) #即a与weights内积加上偏置求和，之后放入非线性转化function求下一层 #a输入层，append不断增长，完成所有正向计算 ''' for j in range(len(self.weights)): a.append(self.activation(np.dot(a[j], self.weights[j]) + self.bias[j] )) # 计算错误率，y[i]真实标记 ，a[-1]预测的classlable errors = y[i] - a[-1] # 计算输出层的误差，根据最后一层当前神经元的值，反向更新 deltas = [errors * self.activation_deriv(a[-1]) ,] # 输出层的误差 # 反向传播，对于隐藏层的误差，从后往前 for j in range(len(a) - 2, 0, -1): tmp_deltas = np.dot(deltas[-1], self.weights[j].T) * self.activation_deriv(a[j]) deltas.append(tmp_deltas) #reverse将deltas的层数跌倒过来 deltas.reverse() # 更新权重 for j in range(len(self.weights)): layer = np.atleast_2d(a[j]) delta = np.atleast_2d(deltas[j]) self.weights[j] += learning_rate * np.dot(layer.T, delta) # 更新偏向 for j in range(len(self.bias)): self.bias[j] += learning_rate * deltas[j] 其中参数 learning_rate 表示学习率， epochs 表示设定的循环次数。 预测函数预测就是将测试实例从输入层传入，通过正向传播，最后返回输出层的值：123456#预测 def predict(self, x): a = np.array(x) # 确保x是 ndarray 对象 for i in range(len(self.weights)): a = self.activation(np.dot(a, self.weights[i]) + self.bias[i]) return a 手写数字图片识别手写数字数据集来自 sklearn ，其中由1797个图像组成，其中每个图像是表示手写数字的 8x8 像素图像： 可以推出，这个神经网络的输入层将有 64 个输入单元，分类结果是 0-9 ，因此输出层有10个单元，构造为： 1nn = NeuralNetwork(layers=[64, 100, 10]) 导入数据集并拆分为训练集和测试集 12345678910digits = datasets.load_digits() #导入数据 X = digits.data y = digits.target # 拆分为训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X, y) # 分类结果离散化 labels_train = LabelBinarizer().fit_transform(y_train) labels_test = LabelBinarizer().fit_transform(y_test) 训练并测试模型1234567891011nn.fit(X_train, labels_train) # 收集测试结果 predictions = [] for i in range(X_test.shape[0]): o = nn.predict(X_test[i] ) predictions.append(np.argmax(o)) # 打印对比结果 print (confusion_matrix(y_test, predictions) ) print (classification_report(y_test, predictions)) 程序运行结果如下：混淆矩阵中，对角线计数越大表示该类别预测越准确，最后预测准确率在95%。 1234567891011121314151617181920212223242526[[53 0 0 0 0 0 0 0 0 0] [ 0 43 0 0 0 0 0 0 0 0] [ 0 0 51 0 0 0 0 0 0 0] [ 0 0 3 32 0 0 0 0 3 1] [ 0 0 0 0 39 0 0 0 0 0] [ 0 0 0 0 0 38 1 0 0 2] [ 0 3 0 0 0 0 51 0 0 0] [ 0 0 0 0 0 1 0 36 0 2] [ 0 4 0 0 0 0 0 0 41 0] [ 0 2 0 0 0 1 0 0 0 43]] precision recall f1-score support 0 1.00 1.00 1.00 53 1 0.83 1.00 0.91 43 2 0.94 1.00 0.97 51 3 1.00 0.82 0.90 39 4 1.00 1.00 1.00 39 5 0.95 0.93 0.94 41 6 0.98 0.94 0.96 54 7 1.00 0.92 0.96 39 8 0.93 0.91 0.92 45 9 0.90 0.93 0.91 46 micro avg 0.95 0.95 0.95 450 macro avg 0.95 0.95 0.95 450weighted avg 0.95 0.95 0.95 450 附录（完整代码）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130import numpy as npfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import LabelBinarizerfrom sklearn.metrics import confusion_matrix, classification_report'''一些激活函数及其导数的计算定义'''def tanh(x): return np.tanh(x)def tanh_deriv(x): return 1.0 - np.tanh(x) * np.tanh(x)def logistic(x): return 1.0 / (1 + np.exp(-x))def logistic_deriv(x): fx = logistic(x) return fx * (1 - fx)#构建神经网络类class NeuralNetwork: ''' 根据类实例化一个函数，_init_代表的是构造函数 self相当于java中的this layers:一个列表，包含了每层神经网络中有几个神经元，至少有两层，输入层不算作 [, , ,]中每个值代表了每层的神经元个数 activation：激活函数可以使用tanh 和 logistics，不指明的情况下就是logistics函数 ''' def __init__(self, layers, activation='logistic'): if activation == 'tanh': self.activation = tanh self.activation_deriv = tanh_deriv elif activation == 'logistic': self.activation = logistic self.activation_deriv = logistic_deriv # 初始化随即权重 self.weights = [] # len(layers)layer是一个list[2,2,1]，则len(layer)=3 # 除了输出层,其它层层之间的单元都要赋予一个随机产生的权重 for i in range(len(layers) - 1): tmp_weights = (np.random.random([layers[i], layers[i + 1]]) * 2 - 1) * 0.25 self.weights.append(tmp_weights) #初始化偏置向 #除了输入层，其它层单元都有一个偏置向 self.bias = [] for i in range(1, len(layers)): tmp_bias = (np.random.random(layers[i]) * 2 - 1) * 0.25 self.bias.append(tmp_bias) def fit(self, X, y, learning_rate=0.2, epochs=10000): #X：数据集,确认是二维，每行是一个实例，每个实例有一些特征值 X = np.atleast_2d(X) #每个实例的标签 y = np.array(y) # 随即梯度 for k in range(epochs): i = np.random.randint(X.shape[0]) a = [X[i]] # 随即取某一条实例 ''' #正向传播计算各单元的值 #np.dot代表两参数的内积，x.dot(y) 等价于 np.dot(x,y) #即a与weights内积加上偏置求和，之后放入非线性转化function求下一层 #a输入层，append不断增长，完成所有正向计算 ''' for j in range(len(self.weights)): a.append(self.activation(np.dot(a[j], self.weights[j]) + self.bias[j] )) # 计算错误率，y[i]真实标记 ，a[-1]预测的classlable errors = y[i] - a[-1] # 计算输出层的误差，根据最后一层当前神经元的值，反向更新 deltas = [errors * self.activation_deriv(a[-1]) ,] # 输出层的误差 # 反向传播，对于隐藏层的误差，从后往前 for j in range(len(a) - 2, 0, -1): tmp_deltas = np.dot(deltas[-1], self.weights[j].T) * self.activation_deriv(a[j]) deltas.append(tmp_deltas) #reverse将deltas的层数跌倒过来 deltas.reverse() # 更新权重 for j in range(len(self.weights)): layer = np.atleast_2d(a[j]) delta = np.atleast_2d(deltas[j]) self.weights[j] += learning_rate * np.dot(layer.T, delta) # 更新偏向 for j in range(len(self.bias)): self.bias[j] += learning_rate * deltas[j] #预测 def predict(self, x): a = np.array(x) # 确保x是 ndarray 对象 for i in range(len(self.weights)): a = self.activation(np.dot(a, self.weights[i]) + self.bias[i]) return aif __name__ == "__main__": ''' 手写体数字图片识别，每张图片像素大小为8*8，共（0-9）10类 调用NeuralNetwork类并设置layer参数： 输入层64个单元，隐藏层100个单元，输出层单元数为类别数10 ''' nn = NeuralNetwork(layers=[64, 100, 10]) digits = datasets.load_digits() #导入数据 X = digits.data y = digits.target # 拆分为训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X, y) # 分类结果离散化 labels_train = LabelBinarizer().fit_transform(y_train) labels_test = LabelBinarizer().fit_transform(y_test) nn.fit(X_train, labels_train) # 收集测试结果 predictions = [] for i in range(X_test.shape[0]): o = nn.predict(X_test[i] ) predictions.append(np.argmax(o)) # 打印对比结果 print (confusion_matrix(y_test, predictions) ) print (classification_report(y_test, predictions)) 参考资料神经网络算法多层神经网络BP算法 原理及推导]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>neural network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之SVM]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-8-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8BSVM%2F</url>
    <content type="text"><![CDATA[支持向量机（support vector machine）,简称SVM，最早在1963年，由 Vladimir N. Vapnik 和 Alexey Ya. Chervonenkis 提出。目前的版本(soft margin)是由Corinna Cortes 和 Vapnik在1993年提出，并在1995年发表。 背景深度学习（2012）出现之前，SVM被认为机器学习中近十几年来最成功，表现最好的算法。SVM本质上是一种二分类器，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。看下图在这个二维的平面中，用一条直线将上面的点分成两类，显然 H1 无法将点区分开，而H2 和 H3 都可以，但这两条哪个更好呢？作为分界线，H3 是更合适的，因为分界线其两边有尽可能大的间隙，这样的好处之一就是增强分类模型的泛化能力，能较为正确的分类预测新的实例。 上面的例子是在二维平面，我们找的是一条直线。假如在三维空间，我们要找的就是一个平面。总的来说就是要寻找区分两类的超平面(hyper plane )，使边界(margin )最大。在一个 n 维空间中，超平面的方程定义为: &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$a_1x_1+a_2x_2+…+a_nx_n = b$ 总共可以有多少个可能的超平面？无数条。我们要寻找的超平面是到边界一测最近点的距离等于到另一侧最近点的距离，在这个边界中，边界两侧的超平面平行。 点到超平面的距离在二维平面中，计算点 (x0, y0) 到线ax + by + c = 0的距离是:在 n 维空间中，点到超平面的距离：将点的坐标和系数都向量化表示，距离公式可以视为:其中 w = {w0, w1, w2,...,wn}我们寻找超平面时，先寻找各分类到超平面的距离最小，再寻找距离之和最大的超平面。共N 个训练点，点的坐标记为xi，结果分类为yi，构成 (xi, yi)：常见的SVM是 二分类模型，因此一般yi 有两种取值为 1 和 -1，取这两个值可以简化求解过程。 超平面推导在 (xi, yi) 中，我们用 xi 表示了点的坐标，yi 表示了分类结果。超平面表示如下：$$w^Tx + b = 0$$在超平面的上方的点满足：$$w^T + b &gt; 0$$在超平面的下方的点满足：$$w^T + b &lt; 0$$因为 yi 只有两种取值 1 和 -1。因此就满足：整合这两个等式（左右都乘以 yi，当yi是负值时，不等号要改方向）得：$$y_i(w^T + b)\geq1,\forall{i}$$ 支持向量所有坐落在边界的边缘上的点被称作是 “支持向量”。分界边缘上的任意一点到超平面的距离为:$\left|\frac{1}{w}\right|$。其中，||w|| 是向量的范数(norm)，或者说是向量的模。它的计算方式为：$$\left|{w}\right| = \sqrt{w * w} = \sqrt{w_1^2 + w_2^2 +…+ w_n^2}$$所以最大边界距离为$\left|\frac{2}{w}\right|$。 找出最大边界的超平面 利用一些数学推导，把 yi(w^T*x + b) &gt;= 1 变为有限制的凸优化问题( convex quadratic optimization )； 利用Karush-Kuhn-Tucker( KKT ) 条件和拉格朗日公式，可以推出 MMH 可以被表示为以下的“决定边界( decision boundary)” $$d(X^T) = \sum_{i=1}^{l}y_ia_iX_iX^T + b_0$$ 上述公式中各个符号含义如下： l：表示 支持向量点 的个数； yi : 第i个支持向量点； Xi ：支持向量的类别标记( class label )； ai 与b0：都是单一数值型参数。 对于测试实例，代入以上公式，按得出结果的正负号来进行分类。 SVM 算法有下面几个特性： 训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以 SVM 不太容易产生过拟合(overfitting )的情况； SVM 训练出来的模型完全依赖于支持向量，即使训练集里所有非支持向量的点都被去除，重新训练，结果仍然会得到完全一样的模型； 一个 SVM 如果训练得出的支持向量个数比较小，那训练出的模型比较容易被泛化 线性不可分在有些情况下，我们无法用一条直线对实例进行划分。其实在很多情况下，数据集在空间中对应的向量不可被一个超平面区分开。这种时候我们要采取以下2个步骤： 利用一个非线性的映射把原数据集中的向量点转化到一个更高维度的空间中； 在这个高维度的空间中找一个线性的超平面来进行可区分处理。如上图表示的，将其从一维转为二维空间，然后在二维空间进行求解。动态演示 高纬空间（核方法）我们如何将原始数据转化到高纬空间呢，举个例子，在三维空间中的向量 X = (x1, x2, x3)转化到六维空间 Z 中去，令:新的决策超平面为:其中，W和Z 都是向量，这个超平面是线性的，解出 W 和b 后，带回原方程：上面转换的求解过程中，需要计算内积，而内积的复杂度非常高，这就需要使用到核方法。 核方法在处理非线性的情况中，SVM 选择一个核函数 ( kernel trick )，通过函数 k(.,.) 将数据映射到高维空间。支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维空间中构造出最优分离超平面。核函数例子： 假设两个向量：x = (a1, a2, a3) y = (b1, b2, b3) ，定义方程：1f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2, x2x3, x3x1, x3x2, x3x3) 假设核函数为:K(x, y) = (&lt;x, y&gt;)^2 , 且设x = (1, 2, 3) y=(4, 5, 6) 则有：123f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)f(y) = (16, 20, 24, 20, 25, 36, 24, 30, 36)&lt;f(x), f(y)&gt; = 16 + 40 + 72 + 40 + 100 + 180 + 72 + 180 + 324 = 1024 核函数计算为: K(x, y) = (4 + 10 + 18 ) ^2 = 32^2 = 1024 ，相同的结果，使用核函数计算会快很多。 核函数能很好的避开直接在高维空间中进行计算，而结果却是等价的。常见的几个核函数 (kernel functions)有 h度多项式核函数(polynomial kernel of degree h) 高斯径向基核函数(Gaussian radial basis function kernel) S型核函数(Sigmoid function kernel)如何选择使用哪个核函数？ 根据先验知识，比如图像分类，使用RBF，尝试不同的核函数，根据结果准确度而定。 多分类SVM 常见的是二分类模型，如果空间中有多个分类，比方有猫，狗，鸟。那么SVM就需要对每个类别做一次模型，是猫还是不是猫？是狗还是不是狗？是鸟还是不是鸟？从中选出可能性最大的。也可以两两做一次SVM：是猫还是狗？是猫还是鸟？是狗还是鸟？最后三个分类器投票决定。因此，多分类情况有两种做法： 对于每个类，有一个当前类和其他类的二类分类器( one-versus-the-rest approach) 两两做一次二类分类器( one-versus-one approace ) 代码实例几个点的分类在上面的二维空间中，有三个点：(1, 1) (2, 0) (2, 3) 。前两个点属于一类，第三个点属于另一类，我们使用这个例子来简单说明 sklearn 中 SVM 的初步用法：123456789101112131415from sklearn import svmx = [[2,0],[1,1],[2,3]]y = [0,0,1]classify = svm.SVC(kernel='linear')classify.fit(x,y)print(classify)#打印支持向量 output:[[1. 1.],[2. 3.]]print(classify.support_vectors_)#打印支持向量在数据实例中的索引 output:[1 2]print(classify.support_)#打印每一类中支持向量的个数 output:[1 1]print(classify.n_support_)#预测新的实例 output:[1]print(classify.predict([[2,3]])) 上面的例子中有两个点是支持向量：(1, 1) (2, 3)，通过clf.support_vectors_可以得到具体的点。这些支持向量点在数据集中的索引可以通过 clf.support_ 得到。在这个例子中，分界线的两侧各有一个支持向量，可以通过clf.n_support_得到，结果为 [1, 1]。 多个点的分类我们增加点的个数，并将超平面画出来，进行可视化展示。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import numpy as npfrom sklearn import svmimport pylab as plnp.random.seed(0) #设置相同的seed,每次产生的随机数相同'''生成2个（20*2）的二维数组，然后按列将它们连接组成（40*2）的二维数组生成的点服从正态分布，-[2,2],+[2,2]会让点靠左下方和右上方'''X = np.r_[np.random.randn(20,2)-[2,2],np.random.randn(20,2)+[2,2]]Y = [0]*20 + [1]*20 #给这40个点分类，标记为0和1#print(X.shape)#print(Y)classify = svm.SVC(kernel='linear') #构造SVM分类模型classify.fit(X,Y)'''利用上面的40个点来画一个二维的分类平面图，假设超平面方程为w0x + w1y + b = 0 转为点斜式就是: y = -(w0/w1)x - (b/w1) ：调用classify，获取w的值'''w = classify.coef_[0]a = -w[0]/w[1]xx = np.linspace(-5,5) #在（-5，5）区间内生成一些连续值，方便作图yy = a*xx - (classify.intercept_[0])/w[1]#print(w)#print(a)'''绘制经过支持向量，并与超平面平行的上下2条直线由于平行，因此斜率相同，只是截距不同'''b = classify.support_vectors_[0] #第一分类中的第一个支持向量点yy_down = a*xx + (b[1]-a*b[0])b = classify.support_vectors_[-1] #第二分类中的最后一个支持向量点yy_up = a*xx + (b[1]-a*b[0])#print(classify.support_vectors_)'''将所有的点、超平面和2条分界线绘制出来k-为实线，即超平面线k--为虚线，2条边界线'''pl.plot(xx, yy, 'k-')pl.plot(xx, yy_down, 'k--')pl.plot(xx, yy_up, 'k--')#单独标记出支持向量点pl.scatter(classify.support_vectors_[:, 0], classify.support_vectors_[:, 1], s=80, facecolors='none')pl.scatter(X[:, 0], X[:, 1], c=Y,cmap=pl.cm.Paired )pl.axis('tight')pl.show() 可视化结果如下： SVM人脸分类我们构建SVM模型分类人脸并进行可视化展示，数据集（lfw）默认存储在~/scikit_learn_data 文件夹中。代码讲解见注释。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129from __future__ import print_functionfrom time import timeimport loggingimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import fetch_lfw_peoplefrom sklearn.model_selection import GridSearchCVfrom sklearn.decomposition import PCAfrom sklearn.svm import SVCfrom sklearn import metrics#输出日志信息logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')'''下载数据集（国外名人）lfw，可以手动下载：https://ndownloader.figshare.com/files/5976015min_faces_per_person表示提取每类人超过这一数目的数据集#resize调整每张人脸图片的比例，默认是0.5'''lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)n_samples, h, w = lfw_people.images.shape # 返回数据集的实例数及图片宽和高X = lfw_people.data #获取每个实例的特征n_features = X.shape[1] #获取每个实例的特征数y = lfw_people.target #标签数据，及每个人的身份target_names = lfw_people.target_names #存储每个人的人名n_classes = target_names.shape[0] # 有几类人print("===== 数据集中信息 =====")print("数据个数(n_samples):", n_samples) # 1288print("特征个数，维度(n_features):", n_features) # 1850print("结果集类别个数(n_classes):", n_classes) # 7'''调用train_test_split方法拆分训练集合测试集test_size=0.25表示随机抽取25%的测试集'''X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)'''原始数据的特征向量维度非常高，意味着训练模型的复杂度非常高,我们要采用PCA降维，保存的组件数目，也即保留下来的特征个数，此处我们选择150'''n_components = 150print("Extracting the top %d eigenfaces from %d faces" % (n_components, X_train.shape[0]))t0 = time()pca = PCA(n_components=n_components,whiten=True).fit(X_train)print("done in %0.3fs" % (time() - t0))#降维后提取每个实例的特征点eigenfaces = pca.components_.reshape((n_components,h,w))print("Projecting the input data on the eigenfaces orthonormal basis")t0 = time()#把训练集和测试集特征向量转化为更低维的矩阵X_train_pca = pca.transform(X_train)X_test_pca = pca.transform(X_test)print("done in %0.3fs" % (time() - t0))#构建SVN分类模型print("Fitting the classifier to the training set")t0 = time()'''C是一个对错误部分的惩罚gamma的参数对不同核函数有不同的表现，gamma表示使用多少比例的特征点使用不同的c和不同值的gamma，进行多个量的尝试此处组成5*6的网格参数点，然后进行搜索，最后选出准确率最高模型'''param_grid= &#123;'C':[1e3, 5e3, 1e4, 5e4, 1e5], 'gamma':[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1] &#125;clf = GridSearchCV(SVC(kernel='rbf',class_weight='balanced'),param_grid)clf = clf.fit(X_train_pca,y_train)print("done in %0.3fs" % (time() - t0))print("Best estimator found by grid search:")print(clf.best_estimator_)#评估测试集print("Predicting people's names on the test set")t0 = time()y_pred = clf.predict(X_test_pca)print("done in %0.3fs" % (time() - t0))'''classification_report查看每一类的各种评价指标confusion_matrix(混淆矩阵) 是建一个 n x n 的方格，横行和纵行分别表示真实的每一组测试集的标记和测试集标记的差别，通常表示这些测试数据哪些对了，哪些错了。这个对角线表示了哪些值对了，对角线数字越多，就表示准确率越高。'''print(metrics.classification_report(y_test,y_pred,target_names=target_names))print(metrics.confusion_matrix(y_test, y_pred, labels=range(n_classes)))#将测试结果可视化展示def plot_gallery(images, titles, h, w, n_row=3, n_col=4): plt.figure(figsize=(1.8 * n_col, 2.4 * n_row)) # 定义画布的大小 plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35) # 调整图片显示位置 for i in range(n_row * n_col): plt.subplot(n_row, n_col, i + 1) #画布划分 plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray) # 图片显示 plt.title(titles[i], size=12) # 标题 #获取或设置x、y轴的当前刻度位置和标签 plt.xticks(()) plt.yticks(())#预测函数归类标签和实际归类标签打印#返回预测人脸和测试人脸姓名的对比titledef title(y_pred, y_test, target_names, i): ''' rsplit（' ',1）从右边开始以右边第一个空格为界，分成两个字符 组成一个list 此处代表把'姓'和'名'分开，然后把后面的姓提出来 末尾加[-1]代表引用分割后的列表最后一个元素 ''' pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1] true_name = target_names[y_test[i]].rsplit(' ', 1)[-1] return 'predicted: %s\ntrue: %s' % (pred_name, true_name)#预测出的人名prediction_titles = [title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])]#测试集的特征向量矩阵和要预测的人名打印plot_gallery(X_test, prediction_titles, h, w)#打印原图和预测的信息eigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]#调用plot_gallery函数打印出实际是谁，预测的谁，以及提取过特征的脸plot_gallery(eigenfaces, eigenface_titles, h, w)plt.show() 程序运行结果如下 1234567891011121314151617181920212223242526272829303132333435363738===== 数据集中信息 =====数据个数(n_samples): 1288特征个数，维度(n_features): 1850结果集类别个数(n_classes): 7Extracting the top 150 eigenfaces from 966 facesdone in 0.309sProjecting the input data on the eigenfaces orthonormal basisdone in 0.025sFitting the classifier to the training setdone in 23.043sBest estimator found by grid search:SVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)Predicting people's names on the test setdone in 0.047s precision recall f1-score support Ariel Sharon 0.81 0.62 0.70 21 Colin Powell 0.72 0.88 0.79 58 Donald Rumsfeld 0.66 0.73 0.69 26 George W Bush 0.88 0.86 0.87 130Gerhard Schroeder 0.81 0.69 0.75 32 Hugo Chavez 1.00 0.69 0.82 13 Tony Blair 0.81 0.83 0.82 42 micro avg 0.81 0.81 0.81 322 macro avg 0.81 0.76 0.78 322 weighted avg 0.82 0.81 0.81 322[[ 13 4 2 2 0 0 0] [ 0 51 1 3 1 0 2] [ 1 2 19 3 0 0 1] [ 1 10 5 112 2 0 0] [ 0 1 1 5 22 0 3] [ 1 1 0 0 0 9 2] [ 0 2 1 2 2 0 35]] 可视化结果展示如下 参考资料机器学习-支持向量机的SVMfetch_lfw_people安装失败]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>svm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之决策树]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树是一个类似于流程图的树结构：其中，每个内部结点表示在一个属性上的测试，每个分支代表一个属性输出，而每个树叶结点代表类或类分布。树的最顶层是根结点。 上图中是否出去玩取决于天气情况（sunny、overcast、rain）和空气湿度（humidity、windy）这2个属性的值。 信息熵决策树算法种类很多，本文主要介绍ID3算法。ID3算法在1970-1980年，由J.Ross. Quinlan提出。在介绍决策树算法前，我们先引入熵（entropy）的概念。 信息和抽象，具体该如何度量呢？1948年，香农提出了信息熵（entropy）的概念。一条信息的信息量大小和它的不确定性有直接的关系，要搞清楚一件非常非常不确定的事情，或者是我们一无所知的事情，需要了解大量信息==&gt;信息量的度量就等于不确定性的多少。我们用比特(bit)来衡量信息的多少，变量的不确定性越大，熵也就越大。计算公式如下，其中$P(x_i)$表示每种事件发生的可能性。 ID3算法 该算法在选择每一个属性判断结点的时候是根据信息增益(Information Gain)。通过下面公式可以计算A来作为节点分类获取了多少信息。$$Gain(A) = Info(D) - InfoA(D)$$ 上图是一个买车的实例，一个人是否买车取决于他的年龄、收入、信用度和他是否是学生。下图（右）是原始数据，下图（左）是简单的决策树划分。那我们是如何确定age作为第一个属性结点的呢？一共有14条数据，其中9人买车，5人没买车，所以根据公式可以算出$Info(D)$的值。接着我们计算选择age作为属性结点的信息熵。age有三个取值，其中youth有5条信息（2人买车，3人没买），middle_aged有4条信息（全部买车），senior有5条信息（3人买车，2人没买），所以根据公式可以算出$Info_{age}(D)$和$Gain(age)$的值，如下所示： 类似，$Gain(income) = 0.029, Gain(student) = 0.151, Gain(credit_rating)=0.048​$所以，选择age作为第一个根节点。重复。。。 最终我们得到了如下所示的决策树。 算法流程 从根节点出发，根节点包括所有的训练样本。 一个节点（包括根节点），若节点内所有样本均属于同一类别，那么将该节点就成为叶节点，并将该节点标记为样本个数最多的类别。 否则利用采用信息增益法来选择用于对样本进行划分的特征，该特征即为测试特征，特征的每一个值都对应着从该节点产生的一个分支及被划分的一个子集。在决策树中，所有的特征均为符号值，即离散值。如果某个特征的值为连续值，那么需要先将其离散化。 递归上述划分子集及产生叶节点的过程，这样每一个子集都会产生一个决策（子）树，直到所有节点变成叶节点。 递归操作的停止条件就是： （1） 一个节点中所有的样本均为同一类别，那么产生叶节点（2） 没有特征可以用来对该节点样本进行划分，这里用attribute_list=null为表示。此时也强制产生叶节点，该节点的类别为样本个数最多的类别（3） 没有样本能满足剩余特征的取值，即test_attribute=a_i 对应的样本为空。此时也强制产生叶节点，该节点的类别为样本个数最多的类别 l流程图如下所示： 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465from sklearn.feature_extraction import DictVectorizerimport csvfrom sklearn import treefrom sklearn import preprocessingimport pydotplus#读入csv文件Extradata = open("G:/PycharmProjects/Machine_Learning/Decision_Tree/AllElectronics.csv","r",encoding="utf-8")reader = csv.reader(Extradata)headers = next(reader) #逐行读取数据 python3中为nex(),python2 中为reader.next()#print(headers)featureList = [] #特征列表labelList = [] #标签列表for row in reader: labelList.append(row[len(row)-1]) #读取每行数据最后一列的标签，存入标签列表 rowDict = &#123;&#125; #创建字典，存储每行特征数据 for i in range(1,len(row)-1):#从第2列开始到倒数第2列读取特征数据 rowDict[headers[i]] = row[i] #对应键值对存储数据 featureList.append(rowDict) #将每行特征数据字典存入特征列表#print(featureList)#向量化特征数据vec = DictVectorizer()dum_X = vec.fit_transform(featureList).toarray()# print("dum_X: " + str(dum_X))# print(vec.get_feature_names())## print("labelList: " + str(labelList))##向量化标签数据label = preprocessing.LabelBinarizer()dum_Y = label.fit_transform(labelList)#print("dum_Y: " + str(dum_Y))#调用sklearn方法构建决策树classfiy = tree.DecisionTreeClassifier(criterion='entropy')classfiy = classfiy.fit(dum_X,dum_Y)#print("classfiy: " + str(classfiy))#将决策树存为dot文件with open("G:/PycharmProjects/Machine_Learning/Decision_Tree/AllElectronics.dot", 'w') as f: f = tree.export_graphviz(classfiy,feature_names=vec.get_feature_names(),out_file=f)#可视化决策树的模型，并存为pdf文件dot_data = tree.export_graphviz(classfiy,out_file=None)graph = pydotplus.graph_from_dot_data(dot_data )graph.write_pdf("aa.pdf")#验证决策树分类模型oneRow_X = dum_X[0,:]#print("oneRow_X: " + str(oneRow_X))newRow_X = oneRow_X#修改特征数据的值newRow_X[0] = 1newRow_X[2] = 0newRow_X[3] = 1newRow_X[4] = 0#print("newRow_X: " + str(newRow_X))finalRow_X = newRow_X.reshape(1,10) #将一位数组转换为二维数组predict_Y = classfiy.predict(finalRow_X) #调用决策树模型预测，注意，finalRow_X要求二维#predict_Y = classfiy.predict([[1., 0., 0., 0., 1., 1., 0., 0., 1., 0.]])print("predict_Y: " + str(predict_Y)) 代码中预测结果见下图： 最后生成的决策树模型见下图： 其他决策树算法 C4.5: QuinlanClassification and Regression Trees (CART): (L. Breiman, J. Friedman, R. Olshen, C. Stone)共同点：都是贪心算法，自上而下(Top-down approach)区别：属性选择度量方法不同： C4.5 （gain ratio), CART(gini index), ID3 (Information Gain) 算法 支持模型 树结构 特征选择 连续值处理 缺失值处理 剪枝 ID3 分类 多叉树 信息增益 不支持 不支持 不支持 C4.5 分类 多叉树 信息增益比 支持 支持 支持 CART 分类、回归 二叉树 基尼系数、均方差 支持 支持 支持 决策树的优缺点优点 简单直观，生成的决策树很直观。 基本不需要预处理，不需要提前归一化，处理缺失值。 既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。 可以处理多维度输出的分类问题。 相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释 可以交叉验证的剪枝来选择模型，从而提高泛化能力。 缺点 决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。 决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。 寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。4.有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。 参考资料 Scikit-learn中的决策树]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>desion tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之KNN]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-2-25-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8BKNN%2F</url>
    <content type="text"><![CDATA[K最近邻(k-Nearest Neighbor，KNN)分类算法，思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。 实例分析 有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形or红色小三角形），下面，我们就要解决这个问题：给这个绿色的圆分类。 俗话说物以类聚，人以群分，判别一个人是一个什么样品质特征的人，常常可以从他/她身边的朋友入手。我们不是要判别上图中那个绿色的圆是属于哪一类数据么，好说，从它的邻居下手。但一次性看多少个邻居呢？从上图中，你还能看到： 若K=3，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。 若K=5，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于蓝色的正方形一类。 我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 时间复杂度 KNN 算法本身简单有效，它是一种 lazy-learning 算法，分类器不需要使用训练集进行训练，训练时间复杂度为0。KNN 分类的计算复杂度和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为 n，那么 KNN 的分类时间复杂度为O(n)。 算法三要素 K 值的选择会对算法的结果产生重大影响。K值较小意味着只有与输入实例较近的训练实例才会对预测结果起作用，但容易发生过拟合；如果 K 值较大，优点是可以减少学习的估计误差，但缺点是学习的近似误差增大，这时与输入实例较远的训练实例也会对预测起作用，使预测发生错误。在实际应用中，K 值一般选择一个较小的数值，通常采用交叉验证的方法来选择最优的 K 值。随着训练实例数目趋向于无穷和 K=1 时，误差率不会超过贝叶斯误差率的2倍，如果K也趋向于无穷，则误差率趋向于贝叶斯误差率。 分类决策规则往往是多数表决，即由输入实例的 K 个最临近的训练实例中的多数类决定输入实例的类别。 距离度量一般采用Lp 距离，当p=2时，即为欧氏距离，在度量之前，应该将每个属性的值规范化，这样有助于防止具有较大初始值域的属性比具有较小初始值域的属性的权重过大。 算法实现 算法步骤 （1）计算已知类别数据集中的点与当前点之间的距离（2）按照距离递增次序排序（3）选取与当前点距离最小的K个点（4）确定前K个点所在类别出现的频率（5）返回前K个点出现频率最高的类别作为当前点的预测分类 代码实例 iris数据集，包含150条用例，每条用例共5列，前4列为特征数据，最后一列为标签数据。 简单实现调用sklearn自带的库1234567891011from sklearn import neighborsfrom sklearn import datasetsknn = neighbors.KNeighborsClassifier() # 申明对象iris = datasets.load_iris() # 导入数据print(iris)knn.fit(iris.data,iris.target) # 生成KNN模型predicit_label = knn.predict([[0.2,0.3,0.3,0.2]]) # 预测print(predicit_label) 复杂实现自己编写函数实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889import csvimport randomimport mathimport operator'''导入数据filename数据存储路径radio，按指定比例将数据划分为训练集和测试集'''def loadDateset(filename,radio,trainSet=[],testSet=[]): with open(filename,'rt') as csvfile: lines = csv.reader(csvfile) # 逐行读取数据 dataset = list(lines) # 转换为列表存储 for x in range(len(dataset)-1): # 循环每行数据，将前4个特征值存入数组 for y in range(4): dataset[x][y] = float(dataset[x][y]) if random.random()&lt;radio: # 取随机值，小于radio就划分到训练集 trainSet.append(dataset[x]) else: testSet.append(dataset[x])'''计算2个样例之间的距离（欧氏距离），length表示数据的维度'''def evaluateDistance(instance1,instance2,length): distance = 0 for x in range(length): # 循环每一维度，数值相减并对其平方，然后进行累加 distance += pow((instance1[x]-instance2[x]),2) return math.sqrt(distance) # 开方求距离'''对于一个实例，找到离他最近的k个实例'''def getNeighbors(trainSet,testInstance,k): distance = [] length = len(testInstance)-1 # 每个测试实例的维度 for x in range(len(trainSet)-1): # 训练集中每一个实例到测试实例的距离 dist = evaluateDistance(testInstance,trainSet[x],length) distance.append((trainSet[x],dist)) # 将每一个训练实例和其对应到测试实例的距离存储到列表 distance.sort(key=operator.itemgetter(1)) # operator模块提供的itemgetter函数用于获取距离维度的数据并排序 neighbors = [] # 存储离一个实例最近的几个实例 for x in range(k): # 取distance中前k个实例存储到neighbors neighbors.append(distance[x][0]) return neighbors'''在最近的K个实例中投票，少数服从多数，把要预测的实例归到多数那一类'''def getResponse(neighbors): classvotes = &#123;&#125; # 定义一个字典，存储每一类别的数目 for x in range(len(neighbors)): response = neighbors[x][-1] if response in classvotes: classvotes[response] += 1 else: classvotes[response] = 1 sortedVotes = sorted(classvotes.items(),key=operator.itemgetter(1),reverse=True) # 排序，输出数目最大的类别 return sortedVotes[0][0]'''计算测试集的准确率'''def getAccuracy(testSet,predictions): correct = 0 for x in range(len(testSet)): if testSet[x][-1] == predictions[x]: # 每行测试用例最后一列的标签与预测标签是否相等 correct += 1 return (correct/float(len(testSet)))*100.0def main(): trainSet = [] # 存储训练集 testSet = [] # 存储测试集 radio = 0.80 # 按4：1划分 loadDateset('G:/PycharmProjects/Machine_Learning/KNN/irisdata.txt',radio,trainSet,testSet) #导入数据并划分 print("trainSetNum: "+ str(len(trainSet))) print("testSetNum: "+ str(len(testSet))) predictions = [] k = 3 # 选取前k个最近的实例 for x in range(len(testSet)): # 循环预测测试集合的每个实例 neighbors = getNeighbors(trainSet,testSet[x],k) result = getResponse(neighbors) predictions.append(result) print('&gt;predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1])) accuracy = getAccuracy(testSet,predictions) print('Accuracy: ' + repr(accuracy) + '%')if __name__ == '__main__': main() 运行结果1234567891011121314151617181920212223242526272829trainSetNum: 124testSetNum: 26&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-versicolor', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'Accuracy: 92.3076923076923% 算法优缺点优点 精度高、对异常值不敏感、无数据输入假定。 KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。 缺点 样本分布不均衡时，如果一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。 适用的数据范围 数值型和标称型。标称型：一般在有限的数据中取，而且只存在‘是’和‘否’两种不同的结果（一般用于分类）数值型：可以在无限的数据中取，而且数值比较具体化，例如4.02,6.23这种值（一般用于回归分析） K-Means简介 如图所示，数据样本用圆点表示，每个簇的中心点用叉叉表示。(a) 刚开始时是原始数据，杂乱无章，没有label，看起来都一样，都是绿色的。(b) 假设数据集可以分为两类，令K=2，随机在坐标上选两个点，作为两个类的中心点。(c-f) 演示了聚类的两种迭代。先划分，把每个数据样本划分到最近的中心点那一簇；划分完后，更新每个簇的中心，即把该簇的所有数据点的坐标加起来去平均值。这样不断进行”划分—更新—划分—更新”，直到每个簇的中心不在移动为止。 KNN和K-Means的区别 KNN K-Means KNN是分类算法 K-Means是聚类算法 监督学习 非监督学习 喂给它的数据集是带label的数据，已经是完全正确的数据 喂给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序 没有明显的前期训练过程，属于memory-based learning 有明显的前期训练过程 K的含义：来了一个样本x，要给它分类，即求出它的y，就从数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c K的含义：K是人工固定好的数字，假设数据集合可以分为K个簇，由于是依靠人工定好，需要一点先验知识 两者相似点：都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法。 参考资料 KNN与K-Means的区别]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>knn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow入门学习2]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-2-24-Tensorflow%E5%85%A5%E9%97%A82%2F</url>
    <content type="text"><![CDATA[这篇博文主要是TensorFlow的一个简单入门，并介绍了如何实现Softmax Regression模型，来对MNIST数据集中的数字手写体进行识别。 然而，由于Softmax Regression模型相对简单，所以最终的识别准确率并不高。下面将针对MNIST数据集构建更加复杂精巧的模型，以进一步提高识别准确率。 深度学习模型TensorFlow很适合用来进行大规模的数值计算，其中也包括实现和训练深度神经网络模型。下面将介绍TensorFlow中模型的基本组成部分，同时将构建一个CNN模型来对MNIST数据集中的数字手写体进行识别。 基本设置在我们构建模型之前，我们首先加载MNIST数据集，然后开启一个TensorFlow会话(session)。 加载MNIST数据集TensorFlow中已经有相关脚本，来自动下载和加载MNIST数据集。（脚本会自动创建MNIST_data文件夹来存储数据集）。下面是脚本程序： 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets('MNIST_data', one_hot=True) 这里mnist是一个轻量级的类文件，存储了NumPy格式的训练集、验证集和测试集，它同样提供了数据中mini-batch迭代的功能。 开启TensorFlow会话TensorFlow后台计算依赖于高效的C++，与后台的连接称为一个会话(session)。TensorFlow中的程序使用，通常都是先创建一个图(graph)，然后在一个会话(session)里运行它。 这里我们使用了一个更为方便的类，InteractiveSession，这能让你在构建代码时更加灵活。InteractiveSession允许你做一些交互操作，通过创建一个计算流图(computation graph)来部分地运行图计算。当你在一些交互环境（例如IPython）中使用时将更加方便。如果你不是使用InteractiveSession，那么你要在启动一个会话和运行图计算前，创建一个整体的计算流图。 下面是如何创建一个InteractiveSession： 12import tensorflow as tfsess = tf.InteractiveSession() 计算流图(Computation Graph)为了在Python中实现高效的数值运算，通常会使用一些Python以外的库函数，如NumPy。但是，这样做会造成转换Python操作的开销，尤其是在GPUs和分布式计算的环境下。TensorFlow在这一方面（指转化操作）做了优化，它让我们能够在Python之外描述一个包含各种交互计算操作的整体流图，而不是每次都独立地在Python之外运行一个单独的计算，避免了许多的转换开销。这样的优化方法同样用在了Theano和Torch上。 所以，以上这样的Python代码的作用是简历一个完整的计算流图，然后指定图中的哪些部分需要运行。关于计算流图的更多具体使用见这里。 Softmax Regression模型见这篇博文。 CNN模型Softmax Regression模型在MNIST数据集上91%的准确率，其实还是比较低的。下面我们将使用一个更加精巧的模型，一个简单的卷积神经网络模型(CNN)。这个模型能够达到99.2%的准确率，尽管这不是最高的，但已经足够接受了。 权值初始化为了建立模型，我们需要先创建一些权值(w)和偏置(b)等参数，这些参数的初始化过程中需要加入一小部分的噪声以破坏参数整体的对称性，同时避免梯度为0.由于我们使用ReLU激活函数（详细介绍)），所以我们通常将这些参数初始化为很小的正值。为了避免重复的初始化操作，我们可以创建下面两个函数： 1234567def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) 卷积(Convolution)和池化(Pooling)TensorFlow同样提供了方便的卷积和池化计算。怎样处理边界元素？怎样设置卷积窗口大小？在这个例子中，卷积操作仅使用了滑动步长为1的窗口，使用0进行填充，所以输出规模和输入的一致；而池化操作是在2 * 2的窗口内采用最大池化技术(max-pooling)。为了使代码简洁，同样将这些操作抽象为函数形式： 123456def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') 其中，padding=&#39;SAME&#39;表示通过填充0，使得输入和输出的形状一致。 第一层：卷积层第一层是卷积层，卷积层将要计算出32个特征映射(feature map)，对每个5 * 5的patch。它的权值tensor的大小为[5, 5, 1, 32]. 前两维是patch的大小，第三维时输入通道的数目，最后一维是输出通道的数目。我们对每个输出通道加上了偏置(bias)。 12W_conv1 = weight_variable([5, 5, 1, 32])b_conv1 = bias_variable([32]) 为了使得图片与计算层匹配，我们首先reshape输入图像x为4维的tensor，第2、3维对应图片的宽和高，最后一维对应颜色通道的数目。（-1就是缺省值，就是先以你们合适，到时总数除以你们几个的乘积，我该是几就是几） 1x_image = tf.reshape(x, [-1,28,28,1]) 然后，使用weight tensor对x_image进行卷积计算，加上bias，再应用到一个ReLU激活函数，最终采用最大池化。 12h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)h_pool1 = max_pool_2x2(h_conv1) 第二层：卷积层为了使得网络有足够深度，我们重复堆积一些相同类型的层。第二层将会有64个特征，对应每个5 * 5的patch。 12345W_conv2 = weight_variable([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2) 全连接层到目前为止，图像的尺寸被缩减为7 * 7，我们最后加入一个神经元数目为1024的全连接层来处理所有的图像上。接着，将最后的pooling层的输出reshape为一个一维向量，与权值相乘，加上偏置，再通过一个ReLu函数。 12345W_fc1 = weight_variable([7 * 7 * 64, 1024])b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) 整个CNN的网络结构如下图： Dropout为了减少过拟合程度，在输出层之前应用dropout技术（即丢弃某些神经元的输出结果）。我们创建一个placeholder来表示一个神经元的输出在dropout时不被丢弃的概率。Dropout能够在训练过程中使用，而在测试过程中不使用。TensorFlow中的tf.nn.dropout操作能够利用mask技术处理各种规模的神经元输出。 12keep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) 输出层最终，我们用一个softmax层，得到类别上的概率分布。（与之前的Softmax Regression模型相同）。 1234W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) 模型训练和测试为了测试模型的性能，需要先对模型进行训练，然后应用在测试集上。和之前Softmax Regression模型中的训练、测试过程类似。区别在于： 用更复杂的ADAM最优化方法代替了之前的梯度下降； 增了额外的参数keep_prob在feed_dict中，以控制dropout的几率； 在训练过程中，增加了log输出功能（每100次迭代输出一次）。 下面是程序： 123456789101112131415cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))sess.run(tf.initialize_all_variables())for i in range(20000): batch = mnist.train.next_batch(50) if i%100 == 0: train_accuracy = accuracy.eval(feed_dict=&#123; x:batch[0], y_: batch[1], keep_prob: 1.0&#125;) print("step %d, training accuracy %g"%(i, train_accuracy)) train_step.run(feed_dict=&#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;)print("test accuracy %g"%accuracy.eval(feed_dict=&#123; x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0&#125;)) 最终，模型在测试集上的准确率大概为99.2%，性能上要优于之前的Softmax Regression模型。 完整代码及运行结果利用CNN模型实现手写体识别的完整代码如下： 123456789101112131415161718__author__ = 'chapter'import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datadef weight_varible(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial)def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') ​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)print("Download Done!")sess = tf.InteractiveSession()# parasW_conv1 = weight_varible([5, 5, 1, 32])b_conv1 = bias_variable([32])# conv layer-1x = tf.placeholder(tf.float32, [None, 784])x_image = tf.reshape(x, [-1, 28, 28, 1])h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)h_pool1 = max_pool_2x2(h_conv1)# conv layer-2W_conv2 = weight_varible([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2)# full connectionW_fc1 = weight_varible([7 * 7 * 64, 1024])b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)# dropoutkeep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)# output layer: softmaxW_fc2 = weight_varible([1024, 10])b_fc2 = bias_variable([10])y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)y_ = tf.placeholder(tf.float32, [None, 10])# model trainingcross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)correct_prediction = tf.equal(tf.arg_max(y_conv, 1), tf.arg_max(y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))sess.run(tf.initialize_all_variables())for i in range(20000): batch = mnist.train.next_batch(50) if i % 100 == 0: train_accuacy = accuracy.eval(feed_dict=&#123;x: batch[0], y_: batch[1], keep_prob: 1.0&#125;) print("step %d, training accuracy %g"%(i, train_accuacy)) train_step.run(feed_dict = &#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;)# accuacy on testprint("test accuracy %g"%(accuracy.eval(feed_dict=&#123;x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0&#125;))) 运行结果如下图： 参考资料 Tensorflow 实战 Google 深度学习框架TensorFlow——Mnist手写数字识别实战教程]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow入门学习]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-2-23-Tensorflow%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[TensorFlow 简介TensorFlow是Google在2015年11月份开源的人工智能系统（Github项目地址），是之前所开发的深度学习基础架构DistBelief的改进版本，该系统可以被用于语音识别、图片识别等多个领域。 官网上对TensorFlow的介绍是，一个使用数据流图(data flow graphs)技术来进行数值计算的开源软件库。数据流图中的节点，代表数值运算；节点节点之间的边，代表多维数据(tensors)之间的某种联系。你可以在多种设备（含有CPU或GPU）上通过简单的API调用来使用该系统的功能。TensorFlow是由Google Brain团队的研发人员负责的项目。 什么是数据流图(Data Flow Graph)数据流图是描述有向图中的数值计算过程。有向图中的节点通常代表数学运算，但也可以表示数据的输入、输出和读写等操作；有向图中的边表示节点之间的某种联系，它负责传输多维数据(Tensors)。图中这些tensors的flow也就是TensorFlow的命名来源。 节点可以被分配到多个计算设备上，可以异步和并行地执行操作。因为是有向图，所以只有等到之前的入度节点们的计算状态完成后，当前节点才能执行操作。 TensorFlow的特性1 灵活性 TensorFlow不是一个严格的神经网络工具包，只要你可以使用数据流图来描述你的计算过程，你可以使用TensorFlow做任何事情。你还可以方便地根据需要来构建数据流图，用简单的Python语言来实现高层次的功能。 2 可移植性 TensorFlow可以在任意具备CPU或者GPU的设备上运行，你可以专注于实现你的想法，而不用去考虑硬件环境问题，你甚至可以利用Docker技术来实现相关的云服务。 3 提高开发效率 TensorFlow可以提升你所研究的东西产品化的效率，并且可以方便与同行们共享代码。 4 支持语言选项 目前TensorFlow支持Python和C++语言。（但是你可以自己编写喜爱语言的SWIG接口） 5 充分利用硬件资源，最大化计算性能 基本使用你需要理解在TensorFlow中，是如何： 将计算流程表示成图； 通过Sessions来执行图计算； 将数据表示为tensors； 使用Variables来保持状态信息； 分别使用feeds和fetches来填充数据和抓取任意的操作结果； 概览TensorFlow是一种将计算表示为图的编程系统。图中的节点称为ops(operation的简称)。一个ops使用0个或以上的Tensors，通过执行某些运算，产生0个或以上的Tensors。一个Tensor是一个多维数组，例如，你可以将一批图像表示为一个四维的数组[batch, height, width, channels]，数组中的值均为浮点数。 TensorFlow中的图描述了计算过程，图通过Session的运行而执行计算。Session将图的节点们(即ops)放置到计算设备(如CPUs和GPUs)上，然后通过方法执行它们；这些方法执行完成后，将返回tensors。在Python中的tensor的形式是numpy ndarray对象，而在C/C++中则是tensorflow::Tensor. 图计算TensorFlow程序中图的创建类似于一个 [施工阶段]，而在 [执行阶段] 则利用一个session来执行图中的节点。很常见的情况是，在 [施工阶段] 创建一个图来表示和训练神经网络，而在 [执行阶段] 在图中重复执行一系列的训练操作。 创建图在TensorFlow中，Constant是一种没有输入的ops，但是你可以将它作为其他ops的输入。Python库中的ops构造器将返回构造器的输出。TensorFlow的Python库中有一个默认的图，将ops构造器作为节点，更多可了解Graph Class文档。 见下面的示例代码： 12345678910111213141516import tensorflow as tf# Create a Constant op that produces a 1x2 matrix. The op is# added as a node to the default graph.## The value returned by the constructor represents the output# of the Constant op.matrix1 = tf.constant([[3., 3.]])# Create another Constant that produces a 2x1 matrix.matrix2 = tf.constant([[2.],[2.]])# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.# The returned value, 'product', represents the result of the matrix# multiplication.product = tf.matmul(matrix1, matrix2) 默认的图(Default Graph)现在有了三个节点：两个 Constant()ops和一个matmul()op。为了得到这两个矩阵的乘积结果，还需要在一个session中启动图计算。 在Session中执行图计算见下面的示例代码，更多可了解Session Class： 1234567891011121314151617181920# Launch the default graph.sess = tf.Session()# To run the matmul op we call the session 'run()' method, passing 'product'# which represents the output of the matmul op. This indicates to the call# that we want to get the output of the matmul op back.## All inputs needed by the op are run automatically by the session. They# typically are run in parallel.## The call 'run(product)' thus causes the execution of threes ops in the# graph: the two constants and matmul.## The output of the op is returned in 'result' as a numpy `ndarray` object.result = sess.run(product)print(result)# ==&gt; [[ 12.]]# Close the Session when we're done.sess.close() Sessions最后需要关闭，以释放相关的资源；你也可以使用with模块，session在with模块中自动会关闭： 123with tf.Session() as sess: result = sess.run([product]) print(result) TensorFlow的这些节点最终将在计算设备(CPUs,GPus)上执行运算。如果是使用GPU，默认会在第一块GPU上执行，如果你想在第二块多余的GPU上执行： 123456with tf.Session() as sess: with tf.device("/gpu:1"): matrix1 = tf.constant([[3., 3.]]) matrix2 = tf.constant([[2.],[2.]]) product = tf.matmul(matrix1, matrix2) ... device中的各个字符串含义如下： &quot;/cpu:0&quot;: 你机器的CPU； &quot;/gpu:0&quot;: 你机器的第一个GPU； &quot;/gpu:1&quot;: 你机器的第二个GPU； 关于TensorFlow中GPU的使用见这里。 交互环境下的使用以上的python示例中，使用了Session和Session.run()来执行图计算。然而，在一些Python的交互环境下(如IPython中)，你可以使用InteractiveSession类，以及Tensor.eval()、Operation.run()等方法。例如，在交互的Python环境下执行以下代码： 1234567891011121314151617# Enter an interactive TensorFlow Session.import tensorflow as tfsess = tf.InteractiveSession()x = tf.Variable([1.0, 2.0])a = tf.constant([3.0, 3.0])# Initialize 'x' using the run() method of its initializer op.x.initializer.run()# Add an op to subtract 'a' from 'x'. Run it and print the resultsub = tf.sub(x, a)print(sub.eval())# ==&gt; [-2. -1.]# Close the Session when we're done.sess.close() TensorsTensorFlow中使用tensor数据结构（实际上就是一个多维数据）表示所有的数据，并在图计算中的节点之间传递数据。一个tensor具有固定的类型、级别和大小，更加深入理解这些概念可参考Rank, Shape, and Type。 变量(Variables)变量在图执行的过程中，保持着自己的状态信息。下面代码中的变量充当了一个简单的计数器角色： 123456789101112131415161718192021222324252627282930# Create a Variable, that will be initialized to the scalar value 0.state = tf.Variable(0, name="counter")# Create an Op to add one to `state`.one = tf.constant(1)new_value = tf.add(state, one)update = tf.assign(state, new_value)# Variables must be initialized by running an `init` Op after having# launched the graph. We first have to add the `init` Op to the graph.init_op = tf.initialize_all_variables()# Launch the graph and run the ops.with tf.Session() as sess: # Run the 'init' op sess.run(init_op) # Print the initial value of 'state' print(sess.run(state)) # Run the op that updates 'state' and print 'state'. for _ in range(3): sess.run(update) print(sess.run(state))# output:# 0# 1# 2# 3 赋值函数assign()和add()函数类似，直到session的run()之后才会执行操作。与之类似的，一般我们会将神经网络模型中的参数表示为一系列的变量，在模型的训练过程中对变量进行更新操作。 抓取(Fetches)为了抓取ops的输出，需要先执行session的run函数。然后，通过print函数打印状态信息。 123456789101112input1 = tf.constant(3.0)input2 = tf.constant(2.0)input3 = tf.constant(5.0)intermed = tf.add(input2, input3)mul = tf.mul(input1, intermed)with tf.Session() as sess: result = sess.run([mul, intermed]) print(result)# output:# [array([ 21.], dtype=float32), array([ 7.], dtype=float32)] 所有tensors的输出都是一次性 [连贯] 执行的。 填充(Feeds)TensorFlow也提供这样的机制：先创建特定数据类型的占位符(placeholder)，之后再进行数据的填充。例如下面的程序： 123456789input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)output = tf.mul(input1, input2)with tf.Session() as sess: print(sess.run([output], feed_dict=&#123;input1:[7.], input2:[2.]&#125;))# output:# [array([ 14.], dtype=float32)] 如果不对placeholder()的变量进行数据填充，将会引发错误，更多的例子可参考MNIST fully-connected feed tutorial (source code)。 示例：曲线拟合下面是一段使用Python写的，曲线拟合计算。官网将此作为刚开始介绍的示例程序。 1234567891011121314151617181920212223242526272829303132# 简化调用库名import tensorflow as tfimport numpy as np# 模拟生成100对数据对, 对应的函数为y = x * 0.1 + 0.3x_data = np.random.rand(100).astype("float32")y_data = x_data * 0.1 + 0.3# 指定w和b变量的取值范围（注意我们要利用TensorFlow来得到w和b的值）W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))b = tf.Variable(tf.zeros([1]))y = W * x_data + b# 最小化均方误差loss = tf.reduce_mean(tf.square(y - y_data))optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss)# 初始化TensorFlow参数init = tf.initialize_all_variables()# 运行数据流图（注意在这一步才开始执行计算过程）sess = tf.Session()sess.run(init)# 观察多次迭代计算时，w和b的拟合值for step in xrange(201): sess.run(train) if step % 20 == 0: print(step, sess.run(W), sess.run(b))# 最好的情况是w和b分别接近甚至等于0.1和0.3 MNIST手写体识别任务下面我们介绍一个神经网络中的经典示例，MNIST手写体识别。这个任务相当于是机器学习中的HelloWorld程序。 MNIST数据集介绍MNIST是一个简单的图片数据集（数据集下载地址），包含了大量的数字手写体图片。下面是一些示例图片： MNIST数据集是含标注信息的，以上图片分别代表5, 0, 4和1。 由于MNIST数据集是TensorFlow的示例数据，所以我们不必下载。只需要下面两行代码，即可实现数据集的读取工作： 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("MNIST_data/", one_hot=True) MNIST数据集一共包含三个部分：训练数据集(55,000份，mnist.train)、测试数据集(10,000份，mnist.test)和验证数据集(5,000份，mnist.validation)。一般来说，训练数据集是用来训练模型，验证数据集可以检验所训练出来的模型的正确性和是否过拟合，测试集是不可见的（相当于一个黑盒），但我们最终的目的是使得所训练出来的模型在测试集上的效果（这里是准确性）达到最佳。 MNIST中的一个数据样本包含两块：手写体图片和对于的label。这里我们用xs和ys分别代表图片和对应的label，训练数据集和测试数据集都有xs和ys，我们使用 mnist.train.images 和 mnist.train.labels 表示训练数据集中图片数据和对于的label数据。 一张图片是一个28*28的像素点矩阵，我们可以用一个同大小的二维整数矩阵来表示。如下： 但是，这里我们可以先简单地使用一个长度为28 * 28 = 784的一维数组来表示图像，因为下面仅仅使用softmax regression来对图片进行识别分类（尽管这样做会损失图片的二维空间信息，所以实际上最好的计算机视觉算法是会利用图片的二维信息的）。 所以MNIST的训练数据集可以是一个形状为55000 * 784位的tensor，也就是一个多维数组，第一维表示图片的索引，第二维表示图片中像素的索引（”tensor”中的像素值在0到1之间）。如下图： MNIST中的数字手写体图片的label值在1到9之间，是图片所表示的真实数字。这里用One-hot vector来表述label值，vector的长度为label值的数目，vector中有且只有一位为1，其他为0.为了方便，我们表示某个数字时在vector中所对应的索引位置设置1，其他位置元素为0. 例如用[0,0,0,1,0,0,0,0,0,0]来表示3。所以，mnist.train.labels是一个55000 * 10的二维数组。如下： 以上是MNIST数据集的描述及TensorFlow中表示。下面介绍Softmax Regression模型。 Softmax Regression模型数字手写体图片的识别，实际上可以转化成一个概率问题，如果我们知道一张图片表示9的概率为80%，而剩下的20%概率分布在8，6和其他数字上，那么从概率的角度上，我们可以大致推断该图片表示的是9. Softmax Regression是一个简单的模型，很适合用来处理得到一个待分类对象在多个类别上的概率分布。所以，这个模型通常是很多高级模型的最后一步。 Softmax Regression大致分为两步（暂时不知道如何合理翻译，转原话）： Step 1: add up the evidence of our input being in certain classes;Step 2: convert that evidence into probabilities. 为了利用图片中各个像素点的信息，我们将图片中的各个像素点的值与一定的权值相乘并累加，权值的正负是有意义的，如果是正的，那么表示对应像素值（不为0的话）对表示该数字类别是积极的；否则，对应像素值(不为0的话)对表示该数字类别是起负面作用的。下面是一个直观的例子，图片中蓝色表示正值，红色表示负值（蓝色区域的形状趋向于数字形状）： 最后，我们在一个图片类别的evidence(不知如何翻译..)中加入偏置(bias)，加入偏置的目的是加入一些与输入独立无关的信息。所以图片类别的evidence可表示为 $$ evidence_{i}=\sum _{j}W_{ij}x_{j}+b_{i} $$ 其中，\( W_i \) 和 \( b_i \) 分别为类别 \( i \) 的权值和偏置，\( j \) 是输入图片 \( x \) 的像素索引。然后，我们将得到的evidence值通过一个”softmax”函数转化为概率值 \( y \) : $$ y = softmax(evidence) $$ 这里softmax函数的作用相当于是一个转换函数，它的作用是将原始的线性函数输出结果以某种方式转换为我们需要的值，这里我们需要0-9十个类别上的概率分布。softmax函数的定义如下： $$ softmax(x) = normalize(exp(x)) ​$$ 具体计算方式如下 $$ softmax(x)_{i} = \dfrac {exp\left( x_{i}\right) } {\Sigma _{j}exp\left( x_{j}\right) } $$ 这里的softmax函数能够得到类别上的概率值分布，并保证所有类别上的概率值之和为1. 下面的图示将有助于你理解softmax函数的计算过程： 如果我们将这个过程公式化，将得到 实际的计算中，我们通常采用矢量计算的方式，如下 也可以简化成 $$ y = softmax( Wx + b ) ​$$ Softmax Regression的程序实现为了在Python中进行科学计算工作，我们常常使用一些独立库函数包，例如NumPy来实现复杂的矩阵计算。但是由于Python的运行效率并不够快，所以常常用一些更加高效的语言来实现。但是，这样做会带来语言转换（例如转换回python操作）的开销。TensorFlow在这方面做了一些优化，可以对你所描述的一系列的交互计算的流程完全独立于Python之外，从而避免了语言切换的开销。 为了使用TensorFlow，我们需要引用该库函数 1import tensorflow as tf 我们利用一些符号变量来描述交互计算的过程，创建如下 1x = tf.placeholder(tf.float32, [None, 784]) 这里的 \( x \) 不是一个特定的值，而是一个占位符，即需要时指定。如前所述，我们用一个1 * 784维的向量来表示一张MNIST中的图片。我们用[None, 784]这样一个二维的tensor来表示整个MNIST数据集，其中None表示可以为任意值。 我们使用Variable(变量)来表示模型中的权值和偏置，这些参数是可变的。如下， 12W = tf.Variable(tf.zeros([784, 10]))b = tf.Variable(tf.zeros([10])) 这里的W和b均被初始化为0值矩阵。W的维数为784 * 10，是因为我们需要将一个784维的像素值经过相应的权值之乘转化为10个类别上的evidence值；b是十个类别上累加的偏置值。 实现softmax regression模型仅需要一行代码，如下 1y = tf.nn.softmax(tf.matmul(x, W) + b) 其中，matmul函数实现了 x 和 W 的乘积，这里 x 为二维矩阵，所以放在前面。可以看出，在TensorFlow中实现softmax regression模型是很简单的。 模型的训练在机器学习中，通常需要选择一个代价函数（或者损失函数），来指示训练模型的好坏。这里，我们使用交叉熵函数（cross-entropy）作为代价函数，交叉熵是一个源于信息论中信息压缩领域的概念，但是现在已经应用在多个领域。它的定义如下： $$ H_{y’}\left( y\right) = -\sum _{i}y_{i}’\log \left( y_{i}\right) $$ 这里 \( y \) 是所预测的概率分布，而 \( y’ \) 是真实的分布(one-hot vector表示的图片label)。直观上，交叉熵函数的输出值表示了预测的概率分布与真实的分布的符合程度。更加深入地理解交叉熵函数，可参考这篇博文。 为了实现交叉熵函数，我们需要先设置一个占位符在存放图片的正确label值， 1y_ = tf.placeholder(tf.float32, [None, 10]) 然后得到交叉熵，即\( -\sum y’\log \left( y\right) \)： 1cross_entropy = -tf.reduce_sum(y_*tf.log(y)) 注意，以上的交叉熵不是局限于一张图片，而是整个可用的数据集。 接下来我们以代价函数最小化为目标，来训练模型以得到相应的参数值(即权值和偏置)。TensorFlow知道你的计算过程，它会自动利用后向传播算法来得到相应的参数变化，对代价函数最小化的影响作用。然后，你可以选择一个优化算法来决定如何最小化代价函数。如下， 1train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) 在这里，我们使用了一个学习率为0.01的梯度下降算法来最小化代价函数。梯度下降是一个简单的计算方式，即使得变量值朝着减小代价函数值的方向变化。TensorFlow也提供了许多其他的优化算法，仅需要一行代码即可实现调用。 TensorFlow提供了以上简单抽象的函数调用功能，你不需要关心其底层实现，可以更加专心于整个计算流程。在模型训练之前，还需要对所有的参数进行初始化： 1init = tf.initialize_all_variables() 我们可以在一个Session里面运行模型，并且进行初始化： 12sess = tf.Session()sess.run(init) 接下来，进行模型的训练 123for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;) 每一次的循环中，我们取训练数据中的100个随机数据，这种操作成为批处理(batch)。然后，每次运行train_step时，将之前所选择的数据，填充至所设置的占位符中，作为模型的输入。 以上过程成为随机梯度下降，在这里使用它是非常合适的。因为它既能保证运行效率，也能一定程度上保证程序运行的正确性。（理论上，我们应该在每一次循环过程中，利用所有的训练数据来得到正确的梯度下降方向，但这样将非常耗时）。 模型的评价怎样评价所训练出来的模型？显然，我们可以用图片预测类别的准确率。 首先，利用tf.argmax()函数来得到预测和实际的图片label值，再用一个tf.equal()函数来判断预测值和真实值是否一致。如下： 1correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) correct_prediction是一个布尔值的列表，例如 [True, False, True, True]。可以使用tf.cast()函数将其转换为[1, 0, 1, 1]，以方便准确率的计算（以上的是准确率为0.75）。 1accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float")) 最后，我们来获取模型在测试集上的准确率， 1print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)) Softmax regression模型由于模型较简单，所以在测试集上的准确率在91%左右，这个结果并不算太好。通过一些简单的优化，准确率可以达到97%，目前最好的模型的准确率为99.7%。（这里有众多模型在MNIST数据集上的运行结果）。 完整代码及运行结果利用Softmax模型实现手写体识别的完整代码如下： 12345678910111213141516171819202122232425262728293031323334353637__author__ = 'chapter'import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("MNIST_data/", one_hot=True)print("Download Done!")x = tf.placeholder(tf.float32, [None, 784])# parasW = tf.Variable(tf.zeros([784, 10]))b = tf.Variable(tf.zeros([10]))y = tf.nn.softmax(tf.matmul(x, W) + b)y_ = tf.placeholder(tf.float32, [None, 10])# loss funccross_entropy = -tf.reduce_sum(y_ * tf.log(y))train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)# initinit = tf.initialize_all_variables()sess = tf.Session()sess.run(init)# trainfor i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)correct_prediction = tf.equal(tf.arg_max(y, 1), tf.arg_max(y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))print("Accuarcy on Test-dataset: ", sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)) 运行结果如下图： 参考资料 TensorFlow官方帮助文档Tensorflow之MNIST解析]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker集群使用文档]]></title>
    <url>%2FDocker%E5%AE%B9%E5%99%A8%2F2019-2-21-Docker%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[用户管理 (管理员权限)添加docker用户组:1sudo groupadd -g 344 docker 添加用户到用户组：1sudo usermod -a -G 用户组 用户 从用户组中删除用户1gpasswd -d 用户 用户组 镜像的基本操作列出本地镜像1docker images 各个选项说明:• REPOSITORY：表示镜像的仓库源（不唯一）• TAG：镜像的标签(不唯一，可以自己设定)• IMAGE ID：镜像ID（唯一）• CREATED：镜像创建时间• SIZE：镜像大小同一个镜像ID可以有多个仓库源和标签，如图中红框所示。 查找镜像 我们可以从Docker Hub网站来搜索镜像，Docker Hub网址为：https://hub.docker.com/我们也可以使用docker search命令来搜索镜像。比如我们需要一个httpd的镜像来作为我们的web服务。我们可以通过docker search命令搜索httpd来寻找适合我们的镜像。 1docker search httpd NAME:镜像仓库源的名称DESCRIPTION:镜像的描述OFFICIAL:是否docker官方发布 下载镜像 当我们在本地主机上使用一个不存在的镜像时 Docker 就会自动下载这个镜像。如果我们想预先下载这个镜像，我们可以使用docker pull命令来下载它。此处以ubuntu:15.10为例,其中15.10为标签，若不写，会默认下载最新的镜像，标签为latest。 1docker pull 镜像名(:标签) 设置镜像标签1docker tag 原始镜像名 新镜像名:标签 发现镜像ID为00a10af6cf18的镜像多了一个新的标签 liufan。 删除镜像 当我们删除某一镜像时，会先尝试删除所有指向该镜像的标签，然后删除该镜像本身。1.若一个镜像有多个标签，我们只想删除已经没用的标签 1docker rmi 仓库源(liufan): 镜像标签(lf) 删除前后我们发现liufan:lf已经被删除 2.彻底删除镜像1docker rmi –f 镜像ID（以8c811b4aec35为例）（不建议-f强制删除） 我们发现8c811b4aec35这个镜像已经被彻底删除（包含所有指向这个镜像的标签） 3.若想删除的镜像有基于它创建的容器存在时，镜像文件是默认无法删除的。（容器会在下面章节有所讲解）1docker run -it --name liufan ubuntu/numpy /bin/bash 我们基于ubuntu/numpy这个镜像创建了一个名为liufan的容器。下面我们退出容器，尝试删除这个镜像，docker会提示有容器在运行，无法删除：若想强制删除，可使用2中的 docker rmi –f 镜像ID，但不建议这样做，因为有容器依赖这个镜像，强制删除会有遗留问题（强制删除的镜像换了新的ID继续存在系统中） 导入导出镜像导出1docker save 镜像(busybox) &gt; 存储位置(/home/lf/aa.tar) 已经在对应目录生成压缩文件 先把本地的busybox镜像删除，然后尝试导入刚刚导出的压缩镜像1docker rmi busybox &amp;&amp; docker images 导入1docker load &lt; (镜像存储位置)/home/lf/aa.tar 我们发现busybox镜像已经成功导入。 注：当已有的镜像不能满足我们的需求时，我们需要自己制作镜像，主要通过下面2中方式：1） 通过Dockerfile文件制作镜像（较难）2） 基于一个原始镜像创建一个容器，在容器里面进行一些操作（安装一些框架或者软件包），然后退出容器，利用commit命令提交生成新的镜像 （简单）### 容器基本操作&gt; 容器是镜像的一个运行实例，它是基于镜像创建的。#### 新建容器12docker create -it --name lf tensorflowdocker ps -a&gt; &gt; 可以看见我们成功创建了一个名为lf，基于tensorflow镜像的容器。使用docker create 命令新建的容器处于停止状态，可以用如下命令来启动并进入它。12docker start lfdocker attach lf#### 启动容器&gt; 启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（stopped）的容器重新启动。1docker run -it --name liufan ubuntu/numpy /bin/bash上述命令等价于先执行docker create,再执行docker start命令。上面我们以交互模式创建了一个基于ubuntu/numpy镜像，名为liufan的容器。&gt; 其中，-i：表示让容器的标准输入保持打开，&gt; -t：让docker分配一个伪终端并绑定到容器的标准输入上，&gt; /bin/bash：不是必要选项，只是在表明创建容器的时候并运行了bash应用，方便我们进入容器内部，不写也可以，不过那就要用其他命令进入容器了。（docker中必须要保持一个进程的运行，要不然整个容器就会退出）我们可以按Ctrl+d或输入exit命令来退出容器。退出后该容器就会处于终止状态（stopped），可通过3.1中的start和attach重新进入容器。#### 查看终止删除容器1docker ps // 查看所有正在运行容器1docker ps -a // 查看所有容器1docker ps -a -q // 查看所有容器ID1docker stop containerId // containerId 是容器的ID或者名字，一个或多个1docker rm containerId // containerId 是容器的ID或者名字，一个或多个1docker rm containerId // containerId 是容器的ID或者名字，一个或多个可以看到lf、wh这两个容器已经被删除1docker stop $(docker ps -a -q) // stop停止所有容器1docker rm $(docker ps -a -q) // remove删除所有容器注：删除容器时必须保证容器是终止态（stopped），若不是先进行docker stop操作再进行docker rm操作，可以-f强制删除但不建议。#### 进入容器&gt; 1.attach命令使用attach命令有时候并不方便。当多个窗口同时attach到同一个容器的时候，所有的窗口都会同步显示。当某个窗口因命令阻塞时，其他窗口就无法执行操作了。&gt; 2.exec命令docker自1.3版本起，提供了一个更加方便的工具exec，可以直接在容器内部运行命令，例如进入到刚创建的容器中，并启动一个bash#### 导入和导出容器1docker run -it --name liufan ubuntu/numpy /bin/bash我们基于ubuntu/numpy镜像创建了一个名为liufan的容器，下面将它&gt; 导出：1docker export 容器名(liufan) &gt; 存储地址(/home/lf/aa.tar)我们将liufan这个容器导出本地并压缩命名为aa.tar文件。&gt; 导入：先将liufan容器删除在尝试导入12docker stop liufan &amp;&amp;docker rm liufan &amp;&amp;docker ps -adocker import /home/lf/aa.tar test/ubuntu:lf我们可以看到刚刚的容器压缩文件已经成功导入，命名为test/ubuntu:lf镜像。前面第一章中的1.6节中，我们介绍过用docker load命令来导入一个镜像文件，其实这边也可以用docker import命令来导入一个容器到本地镜像库。两者的区别是：docker import：丢弃了所有的历史记录和元数据信息，仅保存容器当时的快照状态。在导入的时候可以重新制定标签等元数据信息。docker load：将保存完整记录，体积较大。### 代码实例（以Tensorflow为例）&gt; 上面两章我介绍了镜像和容器的关系和它们的一些基本操作，接下来我将介绍如何在创建的容器里面运行我们的代码。集群上有Tensorflow、Pytorch、Caffe、MXNet等深度学习框架的镜像，此处我已Tensorflow为例，介绍如何在容器里运行我们的代码。#### 创建容器1docker run -it --name liufan bluesliuf/tensorflow /bin/bash我们基于bluesliuf/tensorflow这个镜像创建了一个名为liufan的镜像，进入容器ls查看目录列表，发现此时的容器就类似一个Linux环境，默认的用户权限为root权限。问题：我们的代码和数据集都在本地机器上，如何放到容器内部呢？直接复制困难并且耗时，如果我们的数据集过大。Docker提供了一种方法：挂载。将我们的本地目录挂载到容器内部，实现本机目录文件和容器目录文件共享。#### 挂载本地目录到容器&gt; 注：查询资料发现不能先创建容器，再挂载本地目录，两者必须同时进行，于是我们重新创建容器并挂载本地目录。我的代码和数据集都放在本机/home/lf/lf/catdogs目下，下面将它挂载到容器内。创建容器有2种方式-v:挂载的命令参数红色冒号前：本地目录的绝对路径红色冒号后：容器挂载本地目录的绝对路径蓝色部分表示容器需要使用GPU 时将显卡驱动映射到容器中，默认参数不用修改，如果不使用GPU 可以不加蓝色部分name: 创建的容器名bluesliuf/tensorflow:基于的镜像不调用GPU（本机）：调用GPU（集群）：可以看见我们已经成功将本地目录挂载到了我们指定的容器内部位置。运行代码（本机）：注:本地代码里面通常会有数据集的读取路径，一些生成日志文件的存储路径，我们要对它进行修改，换为容器内读取和存储路径。再去容器内部看，本地的修改已经同步到容器内了。在本地修改文件和容器内修改文件都行，一处修改两者都会同步修改。但建议在本地修改，因为本地修改起来方便，容器内一般用vim编辑器，较为不便。在终端输入命令：python 代码文件名（此处我是tr aining.py） 不调用GPU（本机）： 可以看见代码已经成功运行，并且相应的日志文件也存储到本地目录（容器目录当然也有，两者是同步共享的）此外，docker还提供了类似screen，可以让容器在后台运行的功能，退出时如果想继续运行：按顺序按【ctrl+p】【ctrl+q】，下次再使用docker attach 或者docker exec进入容器，可以看见我们的程序还在继续运行。例如： 调用GPU（本机）：在后台运行和上面一样，也是利用【ctrl+p】【ctrl+q】。 数据卷挂载Docker针对挂载目录还提供了一种高级的用法。叫数据卷。 数据卷：“其实就是一个正常的容器，专门用来提供数据卷供其它容器挂载的”。感觉它就像是由一个容器定义的一个数据挂载信息。其他的容器启动可以直接挂载数据卷容器中定义的挂载信息。示例如下： 1.创建一个普通的容器，名为wuhao，并将本地的文件目录挂载到了容器，接下来把这个容器当做一个数据卷。1docker run -v /home/lf/lf/catdogs:/var/catdogs --name wuhao bluesliuf/tensorflow /bin/bash 2.再创建一个新的容器，来使用这个数据卷。1docker run -it --volumes-from wuhao --name lf bluesliuf/tensorflow /bin/bash –volumes-from用来指定要从哪个数据卷来挂载数据。我们可以发现通过wuhao这个容器（数据卷），我们成功的将本地目录也挂载到了lf这个容器内。 通过数据卷挂载目录更具有优势。1） 我们只需先创建一个容器并挂载本地目录，将其看成数据卷，当我们其他容器也需要挂载同样目录的时候，我们只需要利用–volumes-from就可以实现。2） 当我们需要挂载的本地目录发生改变时，我们只需要修改作为数据卷那个容器挂载的本地目录即可（类似一个全局变量），而无须一个个修改其他容器的本地挂载目录。 挂载成功后。运行代码步骤与上面一样。]]></content>
      <categories>
        <category>Docker容器</category>
      </categories>
      <tags>
        <tag>docker使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的常见模型（GAN）]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-2-20-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B-GAN%2F</url>
    <content type="text"><![CDATA[GAN的来源 14年Goodfellow提出Generative adversarial nets即生成式对抗网络，它要解决的问题是如何从训练样本中学习出新样本，训练样本是图片就生成新图片，训练样本是文章就输出新文章等等。 GANs简单的想法就是用两个模型， 一个生成模型，一个判别模型。判别模型用于判断一个给定的图片是不是真实的图片（从数据集里获取的图片），生成模型的任务是去创造一个看起来像真的图片一样的图片，有点拗口，就是说模型自己去产生一个图片，可以和你想要的图片很像。而在开始的时候这两个模型都是没有经过训练的，这两个模型一起对抗训练，生成模型产生一张图片去欺骗判别模型，然后判别模型去判断这张图片是真是假，最终在这两个模型训练的过程中，两个模型的能力越来越强，最终达到稳态。 GAN的基本组成 GAN 模型中的两位博弈方分别由生成式模型（generative model）和判别式模型（discriminative model）充当。 生成模型： G 捕捉样本数据的分布，用服从某一分布（均匀分布，高斯分布等）的噪声 z 生成一个类似真实训练数据的样本，追求效果是越像真实样本越好； 判别模型: D 是一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率，如果样本来自于真实的训练数据，D 输出大概率，否则，D 输出小概率。 可以做如下类比：生成网络 G 好比假币制造团伙，专门制造假币，判别网络 D 好比警察，专门检测使用的货币是真币还是假币，G 的目标是想方设法生成和真币一样的货币，使得 D 判别不出来，D 的目标是想方设法检测出来 G 生成的假币。 上图是GAN网络的思想导图，我们用1代表真实数据，0来代表生成的假数据。对于判别器D来说，对于真实数据，它要尽可能判别正确输出值1；而对于生成器G，根据随机噪音向量z生成假数据也输入判别器D，对于这些假数据，判别器要尽可能输出值0。 GAN的训练过程可以看成一个博弈的过程，也可以看成2个人在玩一个极大极小值游戏，可以用如下公式表示： $$\min \limits_{G}\max\limits_{D}GAN(D,G)=E_{x\sim P_{data}(x)}[logD(x)]+E_{z\sim p_z(z)}[log(1-D(G(z)))]$$其本质上是两个优化问题，把拆解就如同下面两个公式： 优化D：$$\max\limits_{D}GAN(D,G)=E_{x\sim P_{data}(x)}[logD(x)]+E_{z\sim p_z(z)}[log(1-D(G(z)))]$$优化G：$$\min\limits_{G}GAN(D,G)=E_{z\sim p_z(z)}[log(1-D(G(z)))]$$ 当优化D时，生成器确定,我们要让判别器尽可能输出高的值，所以要最大化公式(2)的值；当优化G的时候，判别器确定，我们要使判别器判断错误，尽可能使D(G(z))的值更大，所以要最小化公式(3)的值。 GAN的训练过程 上图是GAN的训练过程，解析见图中右边文字。 GAN的算法流程和动态求解过程如下图所示： 一开始我们确定G，最大化D，让点沿着D变大的方向移动(红色箭头)，然后我们确定D，最小化G，让点沿着G变小的方向移动(蓝色箭头)。循环上述若干步后，达到期望的鞍点(理想最优解)。 GAN的网络结构判别器(卷积) 卷积层大家应该都很熟悉了,为了方便说明，定义如下： 二维的离散卷积（N=2）方形的特征输入（i1=i2=i）方形的卷积核尺寸（k1=k2=k ）每个维度相同的步长（s1=s2=s）每个维度相同的padding (p1=p2=p)下图(左)表示参数为 (i=5,k=3,s=2,p=1)的卷积计算过程，从计算结果可以看出输出特征的尺寸为 (o1=o2=o=3)；下图(右)表示参为 (i=6,k=3,s=2,p=1)的卷积计算过程，从计算结果可以看出输出特征的尺寸为 (o1=o2=o=3)。 从上述2个例子我们可以总结出卷积层输入特征和输出特征尺寸和卷积核参数的关系为：$$o=\lfloor\frac{i+2p-k}{s}\rfloor+1$$ 生成器(反卷积) 在介绍反卷积之前，我们先来看一下卷积运算和矩阵运算之间的关系。例有如下运算(i=4,k=3,s=1,p=0)，输出为o=2。 通过上述的分析，我们已经知道卷积层的前向操作可以表示为和矩阵C相乘，那么我们很容易得到卷积层的反向传播就是和C的转置相乘。 反卷积和卷积的关系如下： 右上图表示的是参数为( i′=2,k′=3,s′=1,p′=2)的反卷积操作，其对应的卷积操作参数为 (i=4,k=3,s=1,p=0)。我们可以发现对应的卷积和非卷积操作其 (k=k′,s=s′)，但是反卷积却多了p′=2。通过对比我们可以发现卷积层中左上角的输入只对左上角的输出有贡献，所以反卷积层会出现 p′=k−p−1=2。通过示意图，我们可以发现，反卷积层的输入输出在 s=s′=1的情况下关系为：$$o′=i′-k′+2p′+1=i′+(k-1)-2p$$GAN的优点 ●GAN是一种生成式模型，相比较其他生成模型（玻尔兹曼机和GSNs）只用到了反向传播 ●相比其他所有模型, GAN可以产生更加清晰，真实的样本 ●GAN采用的是一种无监督的学习方式训练，可以被广泛用在无监督学习和半监督学习领域GAN的缺点●训练GAN需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到.我们还没有找到很好的达到纳什均衡的方法,所以训练GAN相比VAE或者PixelRNN是不稳定的●GAN不适合处理离散形式的数据，比如文本●GAN存在训练不稳定、梯度消失、模式崩溃的问题实例DCGAN网络网络结构 (判别器) 网络结构 (生成器) 二次元动漫人脸（共50个epoch）数据集：51223张动漫人脸 图左为原始数据集，图右为训练过程训练过程生成效果图如下： 真实人脸（共100个epoch）数据集：CelebA 是香港中文大学的开放数据集，包含10,177个名人身份的202,599张人脸图片。（选取了25600张）,数据集如下：训练过程生成效果图如下：]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
        <tag>GAN model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的常见模型（CNN）]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-1-14-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B-CNN%2F</url>
    <content type="text"><![CDATA[CNN的来源 CNN由纽约大学的Yann LeCun于1998年提出。CNN本质上是一个多层感知机，其成功的原因关键在于它所采用的局部连接和共享权值的方式。 一方面减少了的权值的数量使得网络易于优化，另一方面降低了过拟合的风险。CNN是神经网络中的一种，它的权值共享网络结构使之更类似于生物神经网络，降低了网络模型的复杂度，减少了权值的数量。 权重共享：在卷积神经网络中，卷积层的每一个卷积滤波器重复的作用于整个感受野中，对输入图像进行卷积，卷积结果构成了输入图像的特征图，提取出图像的局部特征。每一个卷积滤波器共享相同的参数，包括相同的权重矩阵和偏置项。共享权重的好处是在对图像进行特征提取时不用考虑局部特征的位置。而且权重共享提供了一种有效的方式，使要学习的卷积神经网络模型参数数量大大降低。 CNN的网络架构 卷积神经网络结构包括：卷积层，降采样层，全链接层。每一层有多个特征图，每个特征图通过一种卷积滤波器提取输入的一种特征，每个特征图有多个神经元。 卷积层（Conv） 再举一个卷积过程的例子如下：我们有下面这个绿色的55输入矩阵，卷积核是一个下面这个黄色的33矩阵。卷积的步幅是一个像素。则卷积的过程如下面的动图。卷积的结果是一个3*3的矩阵。 上面举的例子都是二维的输入，卷积的过程比较简单，那么如果输入是多维的呢？比如在前面一组卷积层+池化层的输出是3个矩阵，这3个矩阵作为输入呢，那么我们怎么去卷积呢？又比如输入的是对应RGB的彩色图像，即是三个分布对应R，G和B的矩阵呢？ 池化层（Pooling） 全连接层（Full Connecting） 总结 一般CNN的结构依次为1、 input2、 ((conv–&gt;relu)N–&gt;pool?)M3、 (fc–&gt;relu)*K4、 fc 卷积神经网络的训练算法 与一般的机器学习算法相比，先定义Loss function,衡量和实际结果之间的差距； 找到最小化损失函数的W（权重）和b（偏置），CNN里面最常见的算法为SGD（随机梯度下降）。 卷积神经网络的优缺点优点 共享卷积核，便于处理高维数据； 不像机器学习人为提取特征，网络训练权重自动提取特征，且分类效果好。 缺点 需要大量训练样本和好的硬件支持（GPU、TPU…）; 物理含义模糊（神经网络是一种难以解释的“黑箱模型”，我们并不知道卷积层到底提取的是什么特征）。 卷积神经网络的典型结构 实战演练猫狗大战，即一个简单的二分类问题，训练出一个自动判别猫狗的模型 训练集（共25000张图片，猫狗各12500张）测试集（共3000张图片，猫狗各1500张） 我们通过Tensorflow这个深度学习框架来构建我们的分类网络。通过其自带的可视化工具Tensorboard我们可以看到网络的详细结构，如下左图所示。模型训练完成后，我们用测试集来测试模型的泛化能力，输入一张测试图片，导入模型，输出分类结果，示例见下右图。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
        <tag>CNN model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的发展]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-1-14-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%91%E5%B1%95%2F</url>
    <content type="text"><![CDATA[深度学习的发展历程 人工智能 机器学习 深度学习 人工智能 远在古希腊时期，发明家就梦想着创造能自主思考的机器。当人类第一次构思可编程计算机时，就已经在思考计算机能否变得智能（尽管这距造出第一台计算机还有一百多年）(Lovelace, 1842)。如今，人工智能（artificialintelligence, AI）已经成为一个具有众多实际应用和活跃研究课题的领域，并且正在蓬勃发展。我们期望通过智能软件自动地处理常规劳动、理解语音或图像、帮助医学诊断和支持基础科学研究。一个人的日常生活需要关于世界的巨量知识。很多这方面的知识是主观的、直观的，因此很难通过形式化的方式表达清楚。计算机需要获取同样的知识才能表现出智能。人工智能的一个关键挑战就是如何将这些非形式化的知识传达给计算机。 机器学习 机器学习(Machine Learning)是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构并不断改善自身性能的学科。简单来说，机器学习就是通过算法，使得机器能从大量的历史数据中学习规律，从而对新的样本做智能识别或预测未来。机器学习在图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等很多方面的发展还存在着没有良好解决的问题。 上图是机器学习解决问题的一般流程，即将原始数据划分为训练数据和测试数据，并提取数据的特征用以训练模型，最终测试数据用来测试模型的好坏（泛化能力）。 深度学习 深度学习的概念源于人工神经网络的研究，含多隐层的多层感知机就是一种深度学习结构。深度学习通过组合低层特征形式更加抽象的高层表示属性类别或特征了，来发现数据的分布式特征表示。其动机在于建立、模拟人脑进行分析学习的神经网络，它模拟人脑的机制来解释数据，例如图像、声音和文本，深度学习是无监督学习的一种。其实，神经网络早在八九十年代就被提出过，真正使得深度学习兴起有2个方面的因素： 大数据，用于训练数据的增加； 计算机的算力大大增加，更快的CPU、通用GPU 的出现 上图是深度学习的简单结构图，主要包含三个部分：输入层（Visible layer）、隐藏层（hidden layer）和输出层（Output layer）。图中解决的是图片分类问题。输入层输入图片，即像素矩阵；对于隐藏层，第一层可以轻易地通过比较相邻像素的亮度来识别边缘。有了第一隐藏层描述的边缘，第二隐藏层可以容易地搜索可识别为角和扩展轮廓的边集合。给定第二隐藏层中关于角和轮廓的图像描述，第三隐藏层可以找到轮廓和角的特定集合来检测特定对象的整个部分；最后根据图像描述中包含的对象部分，输出层输出图片中所包含的对象类别。 深度学习常见的编程框架 观察发现，Google、Microsoft、Facebook等巨头都参与了这场深度学习框架大战，此外，还有毕业于伯克利大学的贾扬清主导开发的Caffe，蒙特利尔大学Lisa Lab团队开发的Theano，以及其他个人或商业组织贡献的框架。 另外，可以看到各大主流框架基本都支持Python，目前Python在科学计算和数据挖掘领域可以说是独领风骚。虽然有来自R、Julia等语言的竞争压力，但是Python的各种库实在是太完善了，Web开发、数据可视化、数据预处理、数据库连接、爬虫等无所不能，有一个完美的生态环境。仅在数据挖据工具链上，Python就有NumPy、SciPy、Pandas、Scikit-learn、XGBoost等组件，做数据采集和预处理都非常方便，并且之后的模型训练阶段可以和TensorFlow等基于Python的深度学习框架完美衔接。 深度学习的应用无人驾驶 深度学习在无人驾驶领域主要用于图像处理， 也就是摄像头上面。 当然也可以用于雷达的数据处理， 但是基于图像极大丰富的信息以及难以手工建模的特性， 深度学习能最大限度的发挥其优势。 在做无人车的公司中，他们都会用到三个传感器激光雷达（lidar），测距雷达（radar）和摄像头（camera），但还是会各有侧重。比如 Waymo（前谷歌无人车）以激光雷达为主，而特斯拉和中国的图森互联以摄像头为主。我们可以从特斯拉近期放出的一段无人驾驶的视频中看到特斯拉有三个摄像头传感器，左中右各一个。 从上图我们可以看出，特斯拉成功识别了道路线（红色的线）前方整个路面（右中图），这个过程也可以用深度学习完成。 AlphaGo阿尔法狗 阿尔法狗（AlphaGo）是第一个击败人类职业围棋选手、第一个战胜围棋世界冠军的人工智能程序。它主要的原理就是深度学习。早在1997年，IBM的国际象棋系统深蓝，击败了世界冠军卡斯帕罗夫时，采用的算法是通过暴力搜索的方式尝试更多的下棋方法从而战胜人类，其所依赖的更多是计算机的计算资源优势。但在围棋上，深蓝的方式完全不适用。为了战胜人类围棋选手，AlphaGo需要更加智能且强大的算法。深度学习为其提供了可能。 AlphaGo主要包括三个组成部分： 蒙特卡洛搜索树（MonteCarlo tree search，MCTS） 估值网络（Value network） 策略网络（Policy notebook） AlphaGo的一个大脑——策略网络，通过深度学习在当前给定棋盘条件下，预测下一步在哪里落子。通过大量对弈棋谱获取训练数据，该网络预测人类棋手下一步落子点的准确率可达57%以上（当年数据）并可以通过自己跟自己对弈的方式提高落子水平。AlphaGo的另一个大脑——估值网络，判断在当前棋盘条件下黑子赢棋的概率。其使用的数据就是策略网络自己和自己对弈时产生的。AlphaGo使用蒙特卡罗树算法，根据策略网络和估值网络对局势的评判结果来寻找最佳落子点。 人脸识别 人脸识别的方法有很多，如face++，DeepFace，FaceNet……常规的人脸识别流程为：人脸检测—&gt;对齐—&gt;表达—&gt;分类。 人脸对齐的方法包括以下几步：1.通过若干个特征点检测人脸；2.剪切；3.建立Delaunay triangulation;4.参考标准3d模型；5.讲3d模型比对到图片上；6.进行仿射变形；7.最终生成正面图像。 学习深度学习所需的基础知识]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[欢迎访问我的博客]]></title>
    <url>%2Funcategorized%2F2019-01-13-FirstBlog%2F</url>
    <content type="text"><![CDATA[Hey 大家好，我是一名计算机领域的在读研究生，现研究方向为Deep Learning、Computer vision,欢迎大家来学习交流。 [查看个人简历][访问主页]]]></content>
      <tags>
        <tag>first</tag>
      </tags>
  </entry>
</search>

<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习方法之K-means聚类]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-14-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8BK-means%2F</url>
    <content type="text"><![CDATA[聚类(Clustering)，就是将相似的事物聚集在一 起，而将不相似的事物划分到不同的类别的过程，是数据分析之中十分重要的一种手段。与此前介绍的决策树，支持向量机等监督学习不同，聚类算法是非监督学习(unsupervised learning )，在数据集中，并不清楚每条数据的具体类别。 算法K-means 算法是数据挖掘十大经典算法之一。由于该算法的效率高，所以在对大规模数据进行聚类时被广泛应用。目前，许多算法均围绕着该算法进行扩展和改进。 k-means 算法接受一个参数 k ，表示将数据集中的数据分成 k 个聚类。在同一个聚类中，数据的相似度较高；而不同聚类的数据相似度较低。 算法的步骤：1. 选择任意 k 个数据，作为各个聚类的质心，（质心也可以理解为中心的意思），执行步骤 2；2. 对每个样本进行分类，将样本划分到最近的质心所在的类别（欧氏距离），执行步骤 3；3. 取各个聚类的中心点作为新的质心，执行步骤 2 进行迭代。迭代结束的条件：1. 当新的迭代后的聚类结果没有发生变化；2. 当迭代次数达到预设的值。算法流程图： 实例分析有如下4种药物，我们要根据其2个特征值对其进行分类，事先并不知道它们属于何种类别。聚类后分为2类（1 和 2） 按照之前的算法流程，我们将4种药划分为了2类，聚类过程如下： 代码实现本部分我们将使用和上面实例分析中一致的数据，采用2种方法实现k-means聚类。 自己实现代码主要包含4个小方法，分别是： shouldStop()：聚类迭代的终止条件 updateLabels()：更新迭代后数据的类标签 getLabelFromClosestCenterpoints()：计算各数据到中心点的距离，选取最近距离更新数据类标签 getCenterpoints()：根据聚类结果选取新的中心点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import numpy as npdef kmeans(x,k,maxIt): ''' :param x: 待分类数据 :param k: 最终分为几类 :param maxIt: 最大迭代次数 :return: 分好类的数据 ''' numpoints,numDim = x.shape #数据行数和列数 dataSet = np.zeros((numpoints,numDim+1)) # 创建一个新数组存储分类好的数据 dataSet[:,:-1] = x # 将数据x赋值给dataSet的前n-列 centerpoints = dataSet[np.random.randint(numpoints,size=k),:] # 随机选取k个中心点 centerpoints = dataSet[0:2, :] # 强制选取前2条数据作为中心点，为了对照实例分析 centerpoints[:,-1] = range(1,k+1) # 为选好的中心点数据打上标签 iterations = 0 # 迭代次数 oldCenterpoints = None # 调用函数循环迭代，实现聚类 while not shouldStop(oldCenterpoints,centerpoints,iterations,maxIt): # 输出每次迭代的聚类过程 print("iterations: \n",iterations) print("dataSet: \n",dataSet) print("centerpoints: \n",centerpoints) # 将原始中心点复制存储，方便迭代完后，比较新旧中心点是否发生变化 oldCenterpoints =np.copy(centerpoints) iterations += 1 # 调用方法更新每条数据的类标签 updateLabels(dataSet,centerpoints) # 根据每一次迭代后的聚类结果，重新选取新的中心点 centerpoints = getCenterpoints(dataSet,k) return dataSet# 聚类迭代的终止条件def shouldStop(oldCenterpoints,centerpoints,iterations,maxIt): ''' :param oldCenterpoints: 迭代前的中心点 :param centerpoints: 迭代后的中心点 :param iterations: 当前迭代次数 :param maxIt: 最大迭代次数 :return: True或False ''' if iterations &gt; maxIt: # 超出设定好的最大迭代次数 return True return np.array_equal(oldCenterpoints,centerpoints) # 判断迭代前后中心点是否发生了变化# 更新迭代后数据的类标签def updateLabels(dataSet,centerpoints): ''' :param dataSet: 数据 :param centerpoints: 中心点 ''' numpoints,numDim = dataSet.shape for i in range(0,numpoints): # 调用方法循环更新数据的类标签 dataSet[i,-1] = getLabelFromClosestCenterpoints(dataSet[i,:-1],centerpoints)# 根据计算各数据到中心点的距离，选取最近距离更新数据类标签def getLabelFromClosestCenterpoints(dataSetRow,centerpoints): ''' :param dataSetRow: 待更新类标签的数据 :param centerpoints: 中心点 :return: 数据新的类标签 ''' label = centerpoints[0,-1] # 选取初始类标签 minDist = np.linalg.norm(dataSetRow - centerpoints[0,:-1]) # 计算和当前中心点的距离 # 循环计算数据和每个中心点的距离，选取最近的更新类标签 for i in range(1,centerpoints.shape[0]): dist = np.linalg.norm(dataSetRow - centerpoints[i,:-1]) # 计算距离 if dist &lt; minDist: minDist = dist label = centerpoints[i,-1] print("minDist: ",minDist) return label# 根据聚类结果选取新的中心点def getCenterpoints(dataSet,k): ''' :param dataSet: 数据 :param k: 最终分为几类 :return: 新的中心点 ''' result = np.zeros((k,dataSet.shape[1])) for i in range(1,k+1): oneCluster = dataSet[dataSet[:,-1]==i,:-1] # 同类数据按行求均值，算出新的中心点 result[i-1,:-1] = np.mean(oneCluster,axis=0) result[i-1,-1] = i # 打上标签 return result# 创建测试数据x1 = np.array([1,1])x2 = np.array([2,1])x3 = np.array([4,3])x4 = np.array([5,4])test_x = np.vstack((x1,x2,x3,x4)) #沿着列方向将矩阵堆叠起来result = kmeans(test_x,2,10)print("final result: \n",result) 程序运行结果如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647iterations: 0dataSet: [[1. 1. 1.] [2. 1. 2.] [4. 3. 0.] [5. 4. 0.]]centerpoints: [[1. 1. 1.] [2. 1. 2.]]minDist: 0.0minDist: 0.0minDist: 2.8284271247461903minDist: 4.242640687119285iterations: 1dataSet: [[1. 1. 1.] [2. 1. 2.] [4. 3. 2.] [5. 4. 2.]]centerpoints: [[1. 1. 1. ] [3.66666667 2.66666667 2. ]]minDist: 0.0minDist: 1.0minDist: 0.4714045207910319minDist: 1.885618083164127iterations: 2dataSet: [[1. 1. 1.] [2. 1. 1.] [4. 3. 2.] [5. 4. 2.]]centerpoints: [[1.5 1. 1. ] [4.5 3.5 2. ]]minDist: 0.5minDist: 0.5minDist: 0.7071067811865476minDist: 0.7071067811865476final result: [[1. 1. 1.] [2. 1. 1.] [4. 3. 2.] [5. 4. 2.]] 通过比较，可以发现结果和我们在实例分析中的一致。 Sklearn实现1234567891011121314from sklearn import clusterimport numpy as np# 创建测试数据x1 = np.array([1,1])x2 = np.array([2,1])x3 = np.array([4,3])x4 = np.array([5,4])test_x = np.vstack((x1,x2,x3,x4)) #沿着列方向将矩阵堆叠起来sk = cluster.KMeans(2)sk.fit(test_x)print("中心点：\n",sk.cluster_centers_)print("类别：\n",sk.labels_) 程序运行结果如下：12345中心点： [[4.5 3.5] [1.5 1. ]]类别： [1 1 0 0] 最后的中心点一致，也成功分为了2类 优缺点优点： 速度快，复杂度低，为 O(Nkq)，N是数据总量，k是类别数，q是迭代次数。一般来讲k、q会比N小得多，那么此时复杂度相当于O(N) ，在各种算法中是算很小的； 原理简单，易于理解。 缺点： 对异常点敏感； 局部最优解而不是全局优，(分类结果与初始点选取有关);不能发现非凸形状的聚类。 K-means++算法2007年由D. Arthur等人提出的K-means++算法在k-means的基础上做了进一步的改进。可以直观地将这改进理解成这K个初始聚类中心相互之间应该分得越开越好。整个算法的描述如下图所示： 下面结合一个简单的例子说明K-means++是如何选取初始聚类中心的。数据集中共有8个样本，分布以及对应序号如下图所示： 假设经过图2的步骤一后6号点被选择为第一个初始聚类中心，那在进行步骤二时每个样本的D(x)和被选择为第二个聚类中心的概率如下表所示： 其中的$P(x)$就是每个样本被选为下一个聚类中心的概率。最后一行的$Sum$是概率$P(x)$的累加和，用于轮盘法选择出第二个聚类中心。方法是随机产生出一个0~1之间的随机数，判断它属于哪个区间，那么该区间对应的序号就是被选择出来的第二个聚类中心了。例如1号点的区间为[0,0.2)，2号点的区间为[0.2, 0.525)。 从上表可以直观的看到第二个初始聚类中心是1号，2号，3号，4号中的一个的概率为0.9。而这4个点正好是离第一个初始聚类中心6号点较远的四个点。这也验证了K-means的改进思想：即离当前已有聚类中心较远的点有更大的概率被选为下一个聚类中心。可以看到，该例的K值取2是比较合适的。当K值大于2时，每个样本会有多个距离，需要取最小的那个距离作为$D(x)​$。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>k-means</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[回归中的相关度和决定系数]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-13-%E5%9B%9E%E5%BD%92%E4%B8%AD%E7%9A%84%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%2F</url>
    <content type="text"><![CDATA[训练集中可能是有若干维度的特征。但有时并不是所有特征都是有用的，有的特征其实和结果并没有关系。因此需要一个能衡量自变量和因变量之间的相关度。 皮尔逊相关系数皮尔逊相关系数(Pearson correlation coefficient），是用于度量两个变量 X 和 Y 之间的相关（线性相关），其值介于[-1,1] 之间。有三种相关情况： 正向相关: &gt;0 负向相关：&lt;0 无相关性：=0 下图从左到右分别代表了正向相关、无相关性和负向相关： 在介绍皮尔逊相关系数之前，要先理解协方差(Covariance ) ，协方差是一个反映两个随机变量相关程度的指标，如果一个变量跟随着另一个变量同时变大或者变小，那么这两个变量的协方差就是正值，反之相反，公式如下：$$Conv(X,Y) = \frac{\sum_{i=1}^{n}(X_i-\overline{X})(Y_i-\overline{Y})}{n-1}$$皮尔逊相关系数的公式如下：$$r_{xy} = \frac{Conv(X,Y)}{\sqrt{Var(X)Var(Y)}} = \frac{\sum(x-\overline{x})(y-\overline{y})}{\sqrt{\sum{(x-\overline{x})^2}\sum(y-\overline{y})^2}}$$Var表示方差，相关度越高，皮尔逊相关系数其值趋于 1 或 -1 （趋于1表示它们呈正相关， 趋于 -1 表示它们呈负相关）；如果相关系数等于0，表明它们之间不存在线性相关关系。 决定系数决定系数即 R 平方值，反应因变量的全部变异能通过回归关系被自变量解释的比例。如R平方为0.8，则表示回归关系可以解释因变量80%的变异。换句话说，如果我们能控制自变量不变，则因变量的变异程度会减少 80%。 在简单线性回归中，决定系数可以是 R^2 = r * r。而更通用的是： SST 其实是两部分组成的，一部分是模型可预测的SSR，一部分是变异的SSError无法用模型解释的。它们之间的计算公式是: 注意: R平方也有其局限性：R平方随着自变量的增加会变大，R平方和样本量是有关系的。因此，我们要到R平方进行修正。修正的方法：$$\overline{R^2} = 1-(1-R^2)\frac{n-1}{n-p-1}$$其中，n 表示样本大小，p 表示模型中解释变量的总数（不包括常数）。 代码实例代码完全按照上述中的公式计算12345678910111213141516171819202122232425262728293031323334353637383940import numpy as npimport mathfrom sklearn import linear_model#计算皮尔逊相关系数( Pearson correlation coefficient）def computer_conv(x,y): var_x = 0 var_y = 0 SSR = 0 x_bar = np.mean(x) # x的方差 y_bar = np.mean(y) # y的方差 for i in range(len(x)): diff_xbar = x[i] - x_bar diff_ybar = y[i] - y_bar SSR += diff_xbar * diff_ybar var_x += diff_xbar**2 var_y += diff_ybar**2 SST = math.sqrt(var_x*var_y) return SSR/SST#计算决定系数R平方值def computer_r(x,y): SSR = 0 SST = 0 linear = linear_model.LinearRegression() # 创建线性模型 linear.fit(x,y) y_hat = linear.predict(x) y_mean = np.mean(y) for i in range(len(x)): SSR += (y_hat[i] - y_mean)**2 SST += (y[i] - y_mean)**2 return SSR/SSTtest_x = [1,3,8,7,9]test_y = [10,12,24,21,34]test_x2 = [[x] for x in test_x]print("r: ",computer_conv(test_x,test_y))print("r平方: ",computer_conv(test_x,test_y)**2)print("R平方: ",computer_r(test_x2,test_y)) 程序运行结果123r: 0.94031007654487r平方: 0.8841830400518192R平方: 0.8841830400518192 我们发现：在简单线性回归中，决定系数的确满足$ R^2 = r * r$]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>Conv&amp;&amp;R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之非线性回归（ Logistic Regression）]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-13-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8B%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[非线性回归是线性回归的延伸，其目标预测函数不是线性的。本文主要介绍逻辑回归（Logistic Regression），它是非线性回归的一种，虽然名字中有“回归”二字，但其本质上是一个分类模型。 含义我们知道，线性回归的模型是求出输出特征向量Y和输入样本矩阵X之间的线性关系系数θ，满足$Y=Xθ$。此时Y是连续的，所以是回归模型。如果Y是离散的话，如何解决？一个可以想到的办法是，我们对于Y再做一次函数转换，变为g(Y)。如果我们令g(Y)的值在某个实数区间的时候是类别A，在另一个实数区间的时候是类别B，以此类推，就得到了一个分类模型。如果结果的类别只有两种，那么就是一个二元分类模型了。逻辑回归的出发点就是从这来的。 看如下实例有这么几组医疗数据，X特征是肿瘤的大小（连续型），Y是其良恶性（离散型），Y只有0（良性）和1（恶性）2种取值。我们选取阈值0.5，h（x）&gt;0.5（恶性），Malignant=1，反之为0，良性。 我们选取阈值0.2，h（x）&gt;0.2（恶性），Malignant=1，反之为0，良性。 比较上述两种情况，新的数值加入时需要不断调整阈值，说明用线性的方法进行回归不太合理。 基本模型我们假设测试数据为$X(x_0,x_1,…,x_n)$需要学习的参数为$\Theta(\theta_0,\theta_1,…,\theta_n)$ 给定函数 $$Z = \theta_0 + \theta_1x_1 + \theta_2x_2+…+\theta_nx_n$$向量化可表示为$$Z = \Theta^TX$$经常需要一个分界线作为区分两类结果。再次需要一个函数进行曲线平滑化，由此引入Sigmoid 函数进行转化： $$g(z) = \frac{1}{1+e^{-z}}$$这样的，可以以 0.5 作为分界线。因此逻辑回归的最终目标函数就是：$$h_\theta(X) = g(\theta_0 + \theta_1x_1 + \theta_2x_2+…+\theta_nx_n) = g(\theta^TX) = \frac{1}{1+e^{-\theta^TX}}$$回归是用来得到样本属于某个分类的概率。因此在分类结果中，假设 y 值是 0 或 1，那么正例 (y = 1): $$h_\theta(X) = P(y=1|X;\theta)$$反例(y = 0):$$1 - h_\theta(X) = P(y=0|X;\theta)$$在线性回归中，我们要找到合适的$\theta^{(i)}$使下面的损失函数值最小：$$Cost(h_\theta(x^{(i)}),y^{(i)}) = \sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2$$如果在逻辑回归中运用上面这种损失函数，得到的函数 J 是一个非凸函数，存在多个局部最小值，很难进行求解，因此需要换一个 cost 函数。重新定义个 cost 函数如下：$$Cost(h_\theta(x^{(i)}),y^{(i)}) = -\frac{1}{m}[\sum_{i=1}^{m}(y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\theta(x^{(i)})))]$$ 求解方法我们采用梯度下降法求解最佳解。梯度下降法的计算过程就是沿梯度下降的方向求解极小值。 先确定向下一步的步幅大小，称之为Learning rate; 任意给定一个初始值：$\theta_0,\theta_1,…,\theta_n$; 确定一个向下的方向，并向下走预先规定的步伐，并更新$\theta_0,\theta_1,…,\theta_n$; 当下降的高度小于某个定义的阈值时，停止下降更新。 这就好比是下山，下一步的方向选的是最陡的方向。梯度下降不一定能够找到全局的最优解，有可能是一个局部最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。θ 的更新方程如下：$$\theta_j = \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta)$$其中，偏导是：$$\frac{\partial}{\partial\theta_j}J(\theta) = \frac{1}{m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})x_j^{(i)}$$ 代码实例本部分我们将采用2种方式实现逻辑回归模型，一种自己编写函数方法，一种调用sklearn中的方法库（LogisticRegression）。 自己实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import numpy as npimport randomimport matplotlib.pyplot as plt#sigmod函数，将值量化到0-1def sigmoid(x): return 1.0 / (1 + np.exp(-x))# 梯度下降算法def gradientDescent(x, y, theta, alpha, m, numIteration): ''' :param x: 输入实例 :param y: 分类标签 :param theta: 学习的参数 :param alpha: 学习率 :param m: 实例个数 :param numIteration: 迭代次数 :return: 学习擦参数theta ''' xTrans = x.transpose() # 矩阵的转置 J = [] # 存储损失的列表，方便绘图 for i in range(0, numIteration): hypothsis = sigmoid(np.dot(x, theta)) #量化到0-1 loss = hypothsis - y #计算误差 cost = np.sum(loss ** 2) / (2 * m) #计算损失 J.append(cost) # 将损失存入列表 print("Iteration %d / Cost:%f" % (i, cost)) gradient = np.dot(xTrans, loss) / m theta = theta - alpha * gradient # 更新梯度 plt.plot(J) # 可视化损失变化 plt.show() return theta# 创建数据，用作测试def genData(numPoints, bais, variance): ''' :param numPoints: 输入实例数目 :param bais: 偏向 :param variance: 方差 :return 创建的数据 x,y ''' # 实例（行数）、偏向、方差 x = np.zeros(shape=(numPoints, 2)) # 初始化numPoints行2列(x1,x2)的全零元素矩阵 y = np.zeros(shape=numPoints) # 归类标签 y_list = [0,1] # 标签列表y的取值 for i in range(0, numPoints): x[i][0] = random.uniform(0, 1) * variance # 创建X的特征对 x[i][1] = (i + bais) + random.uniform(0, 1) * variance random.shuffle(y_list) # 给X特征对附上随机的0 1 标签 y[i] =y_list[0] return x, yx, y = genData(100, 2, 5)#print(x)#print(y)#x和y的维度m, n = np.shape(x)n_y = np.shape(y)numIteration = 100000alpha = 0.0005theta = np.ones(n) # 初始化thetatheta = gradientDescent(x, y, theta, alpha, m, numIteration)print(theta) #output [-0.14718538 0.00381781] Sklearn实现123456from sklearn.linear_model import LogisticRegression# 调用sklearn自带的逻辑回归方法sk = LogisticRegression(max_iter=100000)sk.fit(x, y) # 此处的x和y与上面自己实现的一致print(sk.intercept_)print(sk.coef_) #output [-0.11612453 0.00272452] 我们发现自己闪现求解的$\theta$值与sklearn中的相比，还是存在一定误差的，这是因为sklearn中的方法 LogisticRegression有很多参数进行了详细的优化，详情参见https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn-linear-model-logisticregression Cost损失可视化 模型优缺点优点： 适合需要得到一个分类概率的场景 计算代价不高，容易理解实现。LR在时间和内存需求上相当高效 LR对于数据中小噪声的鲁棒性很好 缺点： 容易欠拟合，分类精度不高 对异常值敏感]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>Unlinear regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之线性回归（LR）]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-12-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[线性回归(linear regression)是利用数理统计和归回分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。与之前的分类问题( Classification )不一样的是，分类问题的结果是离散型的；而回归问题中的结果是连续型（数值）的。 数据特征数理统计中，常用的描述数据特征的有： 均值（mean）：又称平均数或平均值，是计算样本中算术平均数：$$\overline{x} = \frac{\sum_{i=1}^{n}x_i}{n}$$ 中位数（median）：将数据中的各个数值按照大小顺序排列，居于中间位置的变量。当n为基数的时候：直接取位置处于中间的变量；当n为偶数的时候，取中间两个量的平均值。 众数（mode）：数据中出现次数最多的数。 方差( variance )：一种描述离散程度的衡量方式： $$s^2 = \frac{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}{n-1}$$ 标准差 （standard deviation） :将方差$s^2​$开方就能得到标准差。 简单线性回归算法简单线性回归(Simple Linear Regression ),，也就是一元线性回归，包含一个自变量x 和一个因变量y 。常被用来描述因变量(y)和自变量(X)以及偏差(error)之间关系的方程叫做回归模型，这个模型是：$$y = \beta_0 + \beta_1x + \epsilon$$其中偏差 $\epsilon$ 满足正态分布的。因此它的期望值是 0 。 $\epsilon$ 的方差(variance)对于所有的自变量 x 是一样的。 等式两边求期望可得： $$E(y) = \beta_0 + \beta_1x$$其中，β0 是回归线的截距，β1 是回归线的斜率，E(y) 是在一个给定 x 值下 y 的期望值（均值）。 假设我们的估值函数（注意，是估计函数）:$$\widehat{y} = b_0 + b_1x$$b0是估计线性方程的纵截距，b1是估计线性方程的斜率，ŷ是在自变量x等于一个给定值的时候，y的估计值。b0和b1是对β0和β1的估计值。 线性回归的分析流程 如何寻找最佳回归线 寻找的回归方程，要求与真实值的离散程度是最小的： $$min\sum_{i=0}^{n}(y_{i}-\widehat {y})^2$$$$b_1 = \frac{\sum(x_{i}-\overline{x})(y_{i}-\overline{y})}{\sum(x_{i}-\overline{x})^2}$$$$b_0 = \overline{y} - b_1\overline{x}$$ 上面方法类似于数值分析中的最小二乘法，详细推导可见https://zh.wikipedia.org/wiki/最小二乘法 代码实例123456789101112131415161718192021222324252627#简单现行回归：只有一个自变量 y=k*x+b 预测使 (y-y*)^2 最小import numpy as npdef SLR(x:list,y:list): n =len(x) fenzi = 0 fenmu = 0 #按照上面推导的公式计算出估计回归方程中的b0，b1 for i in range(n): fenzi += (x[i]-np.mean(x)) * (y[i]-np.mean(y)) fenmu += (x[i]-np.mean(x)) **2 #print("fenzi: " + str(fenzi)) #print("fenmu: " + str(fenmu)) b1 = fenzi/fenmu # 斜率 b0 = np.mean(y) - b1 * np.mean(x) # 截距 return b0,b1#测试def predict(x,b0,b1): return b0 + b1*xif __name__ == "__main__": x = [1,3,2,1,3] y = [14,24,18,17,27] b0,b1 = SLR(x,y) y_predict = predict(6,b0,b1) print("y_predict: " + str(y_predict)) 多元线性回归算法当自变量有多个时，回归模型就变成了: $$E(y) = \beta0 + \beta_1x_1 +…+\beta_nx_n$$估值函数的自变量也变多：$$\widehat {y} = b_0 + b_1x_1+ b_2x_2+…+b_nx_n$$令矩阵：函数可以转换为：$$h(x) = B^TX$$如果有训练数据:$$D = (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),…, (x^{(m)},y^{(m)}),$$损失函数( cost function )，寻找的目标函数应该尽可能让损失函数小，这个损失函数为：$$J(B) = \frac{1}{2m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2 = \frac{1}{2m}(XB-y)^T(XB-y)$$目的就是求解出一个使得代价(损失)函数最小的 B。 利用梯度下降求解估值函数梯度下降算法是一种求局部最优解的方法，详见https://www.cnblogs.com/pinard/p/5970503.html ，今后的博客也会详细介绍。B 由多个元素组成，对于损失函数可以求偏导如下： $$\frac{\partial}{\partial{B_j}} = \frac{1}{m}\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})x_{j}^{(i)}$$更新B：$$B_{j} = B_{j} - \alpha\frac{\partial}{\partial{B_j}}J(B)$$其中α 表示学习率，决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。 最小二乘法求解估值函数多元多项式的情况一样可以利用最小二乘法来进行求解，将m条数据都带入估值函数可以得到线性方程组:向量化后可以表示为：我们要让$$\sum_{i=1}^{m} = (y_i-\widehat{y})^2$$的值最小，则系数矩阵中，各个参数的偏导的值都会0，因此可以得到 n 个等式：得到正规方程组： 写成矩阵表示为:$$X^TXB = X^TY$$解得：$$B = (X^TX)^{-1}X^TY$$ 代码实例本部分我们会用2种方式实现多元线性回归(Multiple Linear Regression )，一种是sklearn库包中自带的方法，一种是我们依据最小二乘法实现的求解方法。应用实例：一家快递公司送货：X1： 运输里程 X2： 运输次数 Y：总运输时间将其数据存储为csv文件如下： sklearn实现123456789101112131415161718192021from numpy import genfromtxtfrom sklearn import linear_modeldatapath = 'G:/PycharmProjects/Machine_Learning/Linear_Regression/data1.csv'data = genfromtxt(datapath,delimiter=',')x = data[:,:-1]y = data[:,-1]# print(x)# print(y)mlr = linear_model.LinearRegression()mlr.fit(x,y)# print (mlr)# print ("coef: " + str(mlr.coef_) )# print ("intercept: " + str(mlr.intercept_))x_predict = [50,3]y1_predict = mlr.predict([x_predict])print("y_predict: " + str(y1_predict))#output [4.95830457] 最小二乘法实现12345678910111213141516171819202122232425262728293031import numpy as npfrom numpy import genfromtxtclass MyLinearRegression(object): def __init__(self): self.b = [] def fit(self, x: list, y: list): # 为每条训练数据前都添加 1 tmpx = [[1] for _ in range(len(x))] for i, v in enumerate(x): tmpx[i] += v x_mat = np.mat(tmpx) y_mat = np.mat(y).T xT = x_mat.T self.b = (xT * x_mat).I * xT * y_mat def predict(self, x): return np.mat([1] + x) * self.bdatapath = 'G:/PycharmProjects/Machine_Learning/Linear_Regression/data1.csv'data = genfromtxt(datapath,delimiter=',')x = data[:,:-1]y = data[:,-1]test_row = [50, 3]linear = MyLinearRegression()linear.fit(x, y)print(linear.predict(test_row))#output[ 4.95830457] 补充：上面实例中，x的特征都是连续数值，如果有离散型的特征（车型）,我们可以采取如下方法，将车型特征进行one-hot编码，代码不需要变化。 参考资料多元线性回归及其优化算法多元线性回归分析理论详解]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>linear regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之神经网络（NN）]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-10-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8BNN%2F</url>
    <content type="text"><![CDATA[神经网络算法( Neural Network )是机器学习中非常非常重要的算法。它 以人脑中的神经网络为启发，是整个深度学习的核心算法。深度学习就是根据神经网络算法进行的一个延伸。 背景神经网络是受神经元启发的，对于神经元的研究由来已久，1904年生物学家就已经知晓了神经元的组成结构。一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。 1949年心理学家Hebb提出了Hebb学习率，认为人脑神经细胞的突触（也就是连接）上的强度上可以变化的。于是计算科学家们开始考虑用调整权值的方法来让机器学习。这为后面的学习算法奠定了基础。 1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字–感知器（ Perceptron ）。 1986年，Rumelhar和Hinton等人提出了反向传播（ Backpropagation ，BP）算法，这是最著名的一个神经网络算法。 组成多层神经网咯主要由三部分组成：输入层(input layer), 隐藏层 (hidden layers), 输入层 (output layers)。 每层由单元(units)组成， 输入层(input layer)是由训练集的实例特征向量传入，经过连接结点的权重(weight)传入下一层，一层的输出是下一层的输入。 隐藏层(hidden layers)的个数可以是任意的，输入层有一层，输出层(output layers)有一层。每个单元(unit)也可以被称作神经结点。上图为2层的神经网络（一般输入层不算）。一层中加权的求和，然后根据非线性方程转化输出。作为多层向前神经网络，理论上，如果有足够多的隐藏层(hidden layers) 和足够大的训练集, 可以模拟出任何方程。神经网络可以用来解决分类(classification）问题，也可以解决回归( regression )问题。 单层到多层的神经网络由两层神经网络构成了单层神经网络，它还有个别名——— 感知机（Perceptron）。 上图中，有3个输入，连接线的权值分别是 w1, w2, w3。将输入与权值进行乘积然后求和，作为 z单元的输入，如果 z 单元是函数 g ，那么就有 z = g(a1 * w1 + a2 * w2 + a3 * w3) 。 单层神经网络的扩展(2个z单元)，也是一样的计算方式： 在多层神经网络中，只不过是将输出作为下一层的输入，一样是乘以权重然后求和： 设计在使用神经网络训练数据之前，必须确定神经网络的层数以及每层单元的个数。特征向量在被传入输入层时通常被先标准化(normalize）到0和1之间 （为了加速学习过程）。离散型变量可以被编码成每一个输入单元对应一个特征值可能赋的值例如：特征值A可能取三个值（$a_0, a_1, a_2$), 可以使用3个输入单元来代表A。 如果$A=a_0$, 那么代表$a_0$的单元值就取1，其他取0，[1, 0, 0] 如果$A=a_1$, 那么代表$a_1$的单元值就取1，其他取0，[0, 1, 0] 如果$A=a_2$, 那么代表$a_2$的单元值就取1，其他取0，[0, 0, 1] 对于分类问题，如果是2类，可以用一个输出单元表示（0和1分别代表2类）。如果多余2类，每一个类别用一个输出单元表示，所以输入层的单元数量通常等于类别的数量。一般没有明确的规则来设计最好有多少个隐藏层，通常是根据实验测试和误差，以及准确度来实验并做出改进。 交叉验证模型训练结束后，如何来衡量我们模型的泛化能力（测试集上准确率）呢？常见的方法是将数据集分为两类，训练集和测试集，利用测试集的数据将模型的预测结果与真实测试标签作对比，得出准确度。下面介绍另一个常用但更科学的方法———交叉验证方法( Cross-Validation )。这个方法不局限于将数据集分成两份。如上图所示，我们可以将原始数据集分成 k 份。第一次用第一份作为训练集，其余作为测试集，得出这一部分的准确度 ( evaluation )。第二次再用第二份作为训练集，其余作为测试集，得出这第二部分的准确度。以此类推，最后取各部分准确度的平均值作为最终的模型衡量指标。 BP算法BP 算法 (BackPropagation )是多层神经网络的训练一个核心的算法。目的是更新每个连接点的权重，从而减小预测值(predicted value )与真实值 ( target value )之间的差距。输入一条训练数据就会更新一次权重，反方向（从输出层=&gt;隐藏层=&gt;输入层）来以最小化误差（error）来更新权重（weitht）。在训练神经网络之前，需要初始化权重(weights )和偏向( bias )，初始化是随机值， -1 到 1 之间或者-0.5到0.5之间，每个单元有一个偏向。 BP算法有2个过程，前向传播和反向传播，后者是关键。我们以下图为例，分析BP算法的整个过程。 前向传播利用上一层的输入，得到本层的输入:$$I_j = \sum_{i}w_{i,j}O_{i} + \theta_{j}$$其中$w_{i,j}$表示权重，$O_{j}$表示当前神经单元的值，$\theta_{j}$为当前神经单元的偏置向。$i=0$时，$O_{j}$即为输入单元的值。得到输入值后，神经元要怎么做呢？我们先将单个神经元进行展开如图： 隐藏层单元得到值后进行累加求和，然后需要进行一个非线性转化，这个转化在神经网络中称为激活函数( Activation function )，这个激活函数是一个 S(下面会有所介绍) 函数，图中以 f 表示，它的函数为：$$O_{j} = \frac{1}{1+e^{-I_j}}$$ 反向传播前向传播结束后，我们要计算误差，然后根据误差(error)反向传送，更新权重和偏置向。对于输出层的误差：$$Err_{j} = O_{j}(1-O_{j})(T_{j}-O_{j})$$其中$O_{j}$表示预测值，$T_{j}$表示真实值。 对于隐藏层的误差：$$Err_{j} = O_{j}(1-O_{j})\sum_{k}Err_{k}w_{j,k}$$其中$Err_{k}$为后层单元误差，$w_{j,k}$为当前单元与后层单元的权重值。 更新权重：$$\Delta w_{i,j} = (l)Err_{j}O_{i}$$ $$w_{i,j} = w_{i,j} + \Delta w_{i,j}$$更新偏置向：$$\Delta\theta{j} = (l)Err_{j}$$ $$\theta{j} = \theta_{j} + \Delta\theta_{j}$$上面$（l）$为学习率 终止条件 权重的更新低于某个阈值，这个阈值是可以人工指定的； 预测的错误率低于某个阈值； 达到预设一定的循环次数。 S型函数（sigmod）神经元在对权重进行求和后，需要进行一个非线性转化，即作为参数传入激活函数去。这个激活函数是一个 S 型函数(Sigmoid)。S 函数不是指某个函数，而是一类函数,详解可参考 https://zh.wikipedia.org/wiki/S函数 。下面有两个常见的S 函数： 双曲函数（tanh ） 逻辑函数（logistic function ）Sigmod函数S 曲线函数可以将一个数值转为值域在 0 到 1 之间，广义上S 函数是满足y值在某个值域范围内渐变的一条曲线。 双曲函数（tanh） 双曲函数是一类与常见的三角函数（也叫圆函数）类似的函数。详见https://zh.wikipedia.org/wiki/双曲函数 。$$tanhx = \frac{sinhx}{coshx}$$导数为$$\frac{d}{dx}tanhx = 1 - tanh^2x =sech^2x = \frac{1}{cosh^2x}$$ 逻辑函数（logistic function ） 逻辑函数或逻辑曲线是一种常见的S函数，详见https://zh.wikipedia.org/wiki/逻辑函数 。一个常见的逻辑函数可以表示如下：$$f(x) = \frac{1}{1 + e^{-x}}$$导数为 $$\frac{d}{dx} = \frac{e^x}{(1+e^x)^2} = f(x)(1 - f(x))$$ BP算法举例假设有一个两层的神经网络，结构，权重和数据集如下： 根据上述公式（3）到（8）计算误差和更新权重： 代码实例（手写数字识别）基础函数根据神经网络原理，代码实现一个完整的神经网络来对手写数字图片分类。（完整代码见附录）根据上面的sigmoid 函数以及其求导得到对应的python代码：123456789101112def tanh(x): return np.tanh(x)def tanh_deriv(x): return 1.0 - np.tanh(x) * np.tanh(x)def logistic(x): return 1.0 / (1 + np.exp(-x))def logistic_deriv(x): fx = logistic(x) return fx * (1 - fx) 网络函数在网络函数中，需要确定神经网络的层数，每层的个数，从而确定单元间的权重规格和单元的偏向。12345678910111213141516171819202122def __init__(self, layers, activation='logistic'): if activation == 'tanh': self.activation = tanh self.activation_deriv = tanh_deriv elif activation == 'logistic': self.activation = logistic self.activation_deriv = logistic_deriv # 初始化随即权重 self.weights = [] # len(layers)layer是一个list[2,2,1]，则len(layer)=3 # 除了输出层,其它层层之间的单元都要赋予一个随机产生的权重 for i in range(len(layers) - 1): tmp_weights = (np.random.random([layers[i], layers[i + 1]]) * 2 - 1) * 0.25 self.weights.append(tmp_weights) #初始化偏置向 #除了输入层，其它层单元都有一个偏置向 self.bias = [] for i in range(1, len(layers)): tmp_bias = (np.random.random(layers[i]) * 2 - 1) * 0.25 self.bias.append(tmp_bias) 其中，layers 参数表示神经网络层数以及各个层单元的数量，例如下图这个模型：上述网络模型就对应了 layers = [4, 3, 2] 。这是一个 2 层，即len(layers) - 1层的神经网络。输入层 4 个单元，其他依次是 3 ，2 。权重是表示层之间单元与单元之间的连接。因此权重也有 2 层。第一层是一个4 x 3的矩阵，即layers[0] x layers[1]。同理，偏向也有 2 层，第一层个数 3 ，即leyers[1] 。利用np.random.random([m, n]) 来创建一个 m x n 的矩阵。在这个神经网络的类中，初始化都随机-0.25 到 0.25之间的数。 训练函数在神经网络的训练中，需要先设定一个训练的终止条件，前面介绍了3种停止条件，这边使用的是 达到预设一定的循环次数 。每次训练是从样本中随机挑选一个实例进行训练，将这个实例的预测结果和真实结果进行对比，再进行反向传播得到各层的误差，然后再更新权重和偏向：12345678910111213141516171819202122232425262728293031323334353637383940def fit(self, X, y, learning_rate=0.2, epochs=10000): #X：数据集,确认是二维，每行是一个实例，每个实例有一些特征值 X = np.atleast_2d(X) #每个实例的标签 y = np.array(y) # 随即梯度 for k in range(epochs): i = np.random.randint(X.shape[0]) a = [X[i]] # 随即取某一条实例 ''' #正向传播计算各单元的值 #np.dot代表两参数的内积，x.dot(y) 等价于 np.dot(x,y) #即a与weights内积加上偏置求和，之后放入非线性转化function求下一层 #a输入层，append不断增长，完成所有正向计算 ''' for j in range(len(self.weights)): a.append(self.activation(np.dot(a[j], self.weights[j]) + self.bias[j] )) # 计算错误率，y[i]真实标记 ，a[-1]预测的classlable errors = y[i] - a[-1] # 计算输出层的误差，根据最后一层当前神经元的值，反向更新 deltas = [errors * self.activation_deriv(a[-1]) ,] # 输出层的误差 # 反向传播，对于隐藏层的误差，从后往前 for j in range(len(a) - 2, 0, -1): tmp_deltas = np.dot(deltas[-1], self.weights[j].T) * self.activation_deriv(a[j]) deltas.append(tmp_deltas) #reverse将deltas的层数跌倒过来 deltas.reverse() # 更新权重 for j in range(len(self.weights)): layer = np.atleast_2d(a[j]) delta = np.atleast_2d(deltas[j]) self.weights[j] += learning_rate * np.dot(layer.T, delta) # 更新偏向 for j in range(len(self.bias)): self.bias[j] += learning_rate * deltas[j] 其中参数 learning_rate 表示学习率， epochs 表示设定的循环次数。 预测函数预测就是将测试实例从输入层传入，通过正向传播，最后返回输出层的值：123456#预测 def predict(self, x): a = np.array(x) # 确保x是 ndarray 对象 for i in range(len(self.weights)): a = self.activation(np.dot(a, self.weights[i]) + self.bias[i]) return a 手写数字图片识别手写数字数据集来自 sklearn ，其中由1797个图像组成，其中每个图像是表示手写数字的 8x8 像素图像： 可以推出，这个神经网络的输入层将有 64 个输入单元，分类结果是 0-9 ，因此输出层有10个单元，构造为： 1nn = NeuralNetwork(layers=[64, 100, 10]) 导入数据集并拆分为训练集和测试集 12345678910digits = datasets.load_digits() #导入数据 X = digits.data y = digits.target # 拆分为训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X, y) # 分类结果离散化 labels_train = LabelBinarizer().fit_transform(y_train) labels_test = LabelBinarizer().fit_transform(y_test) 训练并测试模型1234567891011nn.fit(X_train, labels_train) # 收集测试结果 predictions = [] for i in range(X_test.shape[0]): o = nn.predict(X_test[i] ) predictions.append(np.argmax(o)) # 打印对比结果 print (confusion_matrix(y_test, predictions) ) print (classification_report(y_test, predictions)) 程序运行结果如下：混淆矩阵中，对角线计数越大表示该类别预测越准确，最后预测准确率在95%。 1234567891011121314151617181920212223242526[[53 0 0 0 0 0 0 0 0 0] [ 0 43 0 0 0 0 0 0 0 0] [ 0 0 51 0 0 0 0 0 0 0] [ 0 0 3 32 0 0 0 0 3 1] [ 0 0 0 0 39 0 0 0 0 0] [ 0 0 0 0 0 38 1 0 0 2] [ 0 3 0 0 0 0 51 0 0 0] [ 0 0 0 0 0 1 0 36 0 2] [ 0 4 0 0 0 0 0 0 41 0] [ 0 2 0 0 0 1 0 0 0 43]] precision recall f1-score support 0 1.00 1.00 1.00 53 1 0.83 1.00 0.91 43 2 0.94 1.00 0.97 51 3 1.00 0.82 0.90 39 4 1.00 1.00 1.00 39 5 0.95 0.93 0.94 41 6 0.98 0.94 0.96 54 7 1.00 0.92 0.96 39 8 0.93 0.91 0.92 45 9 0.90 0.93 0.91 46 micro avg 0.95 0.95 0.95 450 macro avg 0.95 0.95 0.95 450weighted avg 0.95 0.95 0.95 450 附录（完整代码）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130import numpy as npfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import LabelBinarizerfrom sklearn.metrics import confusion_matrix, classification_report'''一些激活函数及其导数的计算定义'''def tanh(x): return np.tanh(x)def tanh_deriv(x): return 1.0 - np.tanh(x) * np.tanh(x)def logistic(x): return 1.0 / (1 + np.exp(-x))def logistic_deriv(x): fx = logistic(x) return fx * (1 - fx)#构建神经网络类class NeuralNetwork: ''' 根据类实例化一个函数，_init_代表的是构造函数 self相当于java中的this layers:一个列表，包含了每层神经网络中有几个神经元，至少有两层，输入层不算作 [, , ,]中每个值代表了每层的神经元个数 activation：激活函数可以使用tanh 和 logistics，不指明的情况下就是logistics函数 ''' def __init__(self, layers, activation='logistic'): if activation == 'tanh': self.activation = tanh self.activation_deriv = tanh_deriv elif activation == 'logistic': self.activation = logistic self.activation_deriv = logistic_deriv # 初始化随即权重 self.weights = [] # len(layers)layer是一个list[2,2,1]，则len(layer)=3 # 除了输出层,其它层层之间的单元都要赋予一个随机产生的权重 for i in range(len(layers) - 1): tmp_weights = (np.random.random([layers[i], layers[i + 1]]) * 2 - 1) * 0.25 self.weights.append(tmp_weights) #初始化偏置向 #除了输入层，其它层单元都有一个偏置向 self.bias = [] for i in range(1, len(layers)): tmp_bias = (np.random.random(layers[i]) * 2 - 1) * 0.25 self.bias.append(tmp_bias) def fit(self, X, y, learning_rate=0.2, epochs=10000): #X：数据集,确认是二维，每行是一个实例，每个实例有一些特征值 X = np.atleast_2d(X) #每个实例的标签 y = np.array(y) # 随即梯度 for k in range(epochs): i = np.random.randint(X.shape[0]) a = [X[i]] # 随即取某一条实例 ''' #正向传播计算各单元的值 #np.dot代表两参数的内积，x.dot(y) 等价于 np.dot(x,y) #即a与weights内积加上偏置求和，之后放入非线性转化function求下一层 #a输入层，append不断增长，完成所有正向计算 ''' for j in range(len(self.weights)): a.append(self.activation(np.dot(a[j], self.weights[j]) + self.bias[j] )) # 计算错误率，y[i]真实标记 ，a[-1]预测的classlable errors = y[i] - a[-1] # 计算输出层的误差，根据最后一层当前神经元的值，反向更新 deltas = [errors * self.activation_deriv(a[-1]) ,] # 输出层的误差 # 反向传播，对于隐藏层的误差，从后往前 for j in range(len(a) - 2, 0, -1): tmp_deltas = np.dot(deltas[-1], self.weights[j].T) * self.activation_deriv(a[j]) deltas.append(tmp_deltas) #reverse将deltas的层数跌倒过来 deltas.reverse() # 更新权重 for j in range(len(self.weights)): layer = np.atleast_2d(a[j]) delta = np.atleast_2d(deltas[j]) self.weights[j] += learning_rate * np.dot(layer.T, delta) # 更新偏向 for j in range(len(self.bias)): self.bias[j] += learning_rate * deltas[j] #预测 def predict(self, x): a = np.array(x) # 确保x是 ndarray 对象 for i in range(len(self.weights)): a = self.activation(np.dot(a, self.weights[i]) + self.bias[i]) return aif __name__ == "__main__": ''' 手写体数字图片识别，每张图片像素大小为8*8，共（0-9）10类 调用NeuralNetwork类并设置layer参数： 输入层64个单元，隐藏层100个单元，输出层单元数为类别数10 ''' nn = NeuralNetwork(layers=[64, 100, 10]) digits = datasets.load_digits() #导入数据 X = digits.data y = digits.target # 拆分为训练集和测试集 X_train, X_test, y_train, y_test = train_test_split(X, y) # 分类结果离散化 labels_train = LabelBinarizer().fit_transform(y_train) labels_test = LabelBinarizer().fit_transform(y_test) nn.fit(X_train, labels_train) # 收集测试结果 predictions = [] for i in range(X_test.shape[0]): o = nn.predict(X_test[i] ) predictions.append(np.argmax(o)) # 打印对比结果 print (confusion_matrix(y_test, predictions) ) print (classification_report(y_test, predictions)) 参考资料神经网络算法多层神经网络BP算法 原理及推导]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>neural network</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之SVM]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-8-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8BSVM%2F</url>
    <content type="text"><![CDATA[支持向量机（support vector machine）,简称SVM，最早在1963年，由 Vladimir N. Vapnik 和 Alexey Ya. Chervonenkis 提出。目前的版本(soft margin)是由Corinna Cortes 和 Vapnik在1993年提出，并在1995年发表。 背景深度学习（2012）出现之前，SVM被认为机器学习中近十几年来最成功，表现最好的算法。SVM本质上是一种二分类器，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。看下图在这个二维的平面中，用一条直线将上面的点分成两类，显然 H1 无法将点区分开，而H2 和 H3 都可以，但这两条哪个更好呢？作为分界线，H3 是更合适的，因为分界线其两边有尽可能大的间隙，这样的好处之一就是增强分类模型的泛化能力，能较为正确的分类预测新的实例。 上面的例子是在二维平面，我们找的是一条直线。假如在三维空间，我们要找的就是一个平面。总的来说就是要寻找区分两类的超平面(hyper plane )，使边界(margin )最大。在一个 n 维空间中，超平面的方程定义为: &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;$a_1x_1+a_2x_2+…+a_nx_n = b$ 总共可以有多少个可能的超平面？无数条。我们要寻找的超平面是到边界一测最近点的距离等于到另一侧最近点的距离，在这个边界中，边界两侧的超平面平行。 点到超平面的距离在二维平面中，计算点 (x0, y0) 到线ax + by + c = 0的距离是:在 n 维空间中，点到超平面的距离：将点的坐标和系数都向量化表示，距离公式可以视为:其中 w = {w0, w1, w2,...,wn}我们寻找超平面时，先寻找各分类到超平面的距离最小，再寻找距离之和最大的超平面。共N 个训练点，点的坐标记为xi，结果分类为yi，构成 (xi, yi)：常见的SVM是 二分类模型，因此一般yi 有两种取值为 1 和 -1，取这两个值可以简化求解过程。 超平面推导在 (xi, yi) 中，我们用 xi 表示了点的坐标，yi 表示了分类结果。超平面表示如下：$$w^Tx + b = 0$$在超平面的上方的点满足：$$w^T + b &gt; 0$$在超平面的下方的点满足：$$w^T + b &lt; 0$$因为 yi 只有两种取值 1 和 -1。因此就满足：整合这两个等式（左右都乘以 yi，当yi是负值时，不等号要改方向）得：$$y_i(w^T + b)\geq1,\forall{i}$$ 支持向量所有坐落在边界的边缘上的点被称作是 “支持向量”。分界边缘上的任意一点到超平面的距离为:$\left|\frac{1}{w}\right|$。其中，||w|| 是向量的范数(norm)，或者说是向量的模。它的计算方式为：$$\left|{w}\right| = \sqrt{w * w} = \sqrt{w_1^2 + w_2^2 +…+ w_n^2}$$所以最大边界距离为$\left|\frac{2}{w}\right|$。 找出最大边界的超平面 利用一些数学推导，把 yi(w^T*x + b) &gt;= 1 变为有限制的凸优化问题( convex quadratic optimization )； 利用Karush-Kuhn-Tucker( KKT ) 条件和拉格朗日公式，可以推出 MMH 可以被表示为以下的“决定边界( decision boundary)” $$d(X^T) = \sum_{i=1}^{l}y_ia_iX_iX^T + b_0$$ 上述公式中各个符号含义如下： l：表示 支持向量点 的个数； yi : 第i个支持向量点； Xi ：支持向量的类别标记( class label )； ai 与b0：都是单一数值型参数。 对于测试实例，代入以上公式，按得出结果的正负号来进行分类。 SVM 算法有下面几个特性： 训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以 SVM 不太容易产生过拟合(overfitting )的情况； SVM 训练出来的模型完全依赖于支持向量，即使训练集里所有非支持向量的点都被去除，重新训练，结果仍然会得到完全一样的模型； 一个 SVM 如果训练得出的支持向量个数比较小，那训练出的模型比较容易被泛化 线性不可分在有些情况下，我们无法用一条直线对实例进行划分。其实在很多情况下，数据集在空间中对应的向量不可被一个超平面区分开。这种时候我们要采取以下2个步骤： 利用一个非线性的映射把原数据集中的向量点转化到一个更高维度的空间中； 在这个高维度的空间中找一个线性的超平面来进行可区分处理。如上图表示的，将其从一维转为二维空间，然后在二维空间进行求解。动态演示 高纬空间（核方法）我们如何将原始数据转化到高纬空间呢，举个例子，在三维空间中的向量 X = (x1, x2, x3)转化到六维空间 Z 中去，令:新的决策超平面为:其中，W和Z 都是向量，这个超平面是线性的，解出 W 和b 后，带回原方程：上面转换的求解过程中，需要计算内积，而内积的复杂度非常高，这就需要使用到核方法。 核方法在处理非线性的情况中，SVM 选择一个核函数 ( kernel trick )，通过函数 k(.,.) 将数据映射到高维空间。支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维空间中构造出最优分离超平面。核函数例子： 假设两个向量：x = (a1, a2, a3) y = (b1, b2, b3) ，定义方程：1f(x) = (x1x1, x1x2, x1x3, x2x1, x2x2, x2x3, x3x1, x3x2, x3x3) 假设核函数为:K(x, y) = (&lt;x, y&gt;)^2 , 且设x = (1, 2, 3) y=(4, 5, 6) 则有：123f(x) = (1, 2, 3, 2, 4, 6, 3, 6, 9)f(y) = (16, 20, 24, 20, 25, 36, 24, 30, 36)&lt;f(x), f(y)&gt; = 16 + 40 + 72 + 40 + 100 + 180 + 72 + 180 + 324 = 1024 核函数计算为: K(x, y) = (4 + 10 + 18 ) ^2 = 32^2 = 1024 ，相同的结果，使用核函数计算会快很多。 核函数能很好的避开直接在高维空间中进行计算，而结果却是等价的。常见的几个核函数 (kernel functions)有 h度多项式核函数(polynomial kernel of degree h) 高斯径向基核函数(Gaussian radial basis function kernel) S型核函数(Sigmoid function kernel)如何选择使用哪个核函数？ 根据先验知识，比如图像分类，使用RBF，尝试不同的核函数，根据结果准确度而定。 多分类SVM 常见的是二分类模型，如果空间中有多个分类，比方有猫，狗，鸟。那么SVM就需要对每个类别做一次模型，是猫还是不是猫？是狗还是不是狗？是鸟还是不是鸟？从中选出可能性最大的。也可以两两做一次SVM：是猫还是狗？是猫还是鸟？是狗还是鸟？最后三个分类器投票决定。因此，多分类情况有两种做法： 对于每个类，有一个当前类和其他类的二类分类器( one-versus-the-rest approach) 两两做一次二类分类器( one-versus-one approace ) 代码实例几个点的分类在上面的二维空间中，有三个点：(1, 1) (2, 0) (2, 3) 。前两个点属于一类，第三个点属于另一类，我们使用这个例子来简单说明 sklearn 中 SVM 的初步用法：123456789101112131415from sklearn import svmx = [[2,0],[1,1],[2,3]]y = [0,0,1]classify = svm.SVC(kernel='linear')classify.fit(x,y)print(classify)#打印支持向量 output:[[1. 1.],[2. 3.]]print(classify.support_vectors_)#打印支持向量在数据实例中的索引 output:[1 2]print(classify.support_)#打印每一类中支持向量的个数 output:[1 1]print(classify.n_support_)#预测新的实例 output:[1]print(classify.predict([[2,3]])) 上面的例子中有两个点是支持向量：(1, 1) (2, 3)，通过clf.support_vectors_可以得到具体的点。这些支持向量点在数据集中的索引可以通过 clf.support_ 得到。在这个例子中，分界线的两侧各有一个支持向量，可以通过clf.n_support_得到，结果为 [1, 1]。 多个点的分类我们增加点的个数，并将超平面画出来，进行可视化展示。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import numpy as npfrom sklearn import svmimport pylab as plnp.random.seed(0) #设置相同的seed,每次产生的随机数相同'''生成2个（20*2）的二维数组，然后按列将它们连接组成（40*2）的二维数组生成的点服从正态分布，-[2,2],+[2,2]会让点靠左下方和右上方'''X = np.r_[np.random.randn(20,2)-[2,2],np.random.randn(20,2)+[2,2]]Y = [0]*20 + [1]*20 #给这40个点分类，标记为0和1#print(X.shape)#print(Y)classify = svm.SVC(kernel='linear') #构造SVM分类模型classify.fit(X,Y)'''利用上面的40个点来画一个二维的分类平面图，假设超平面方程为w0x + w1y + b = 0 转为点斜式就是: y = -(w0/w1)x - (b/w1) ：调用classify，获取w的值'''w = classify.coef_[0]a = -w[0]/w[1]xx = np.linspace(-5,5) #在（-5，5）区间内生成一些连续值，方便作图yy = a*xx - (classify.intercept_[0])/w[1]#print(w)#print(a)'''绘制经过支持向量，并与超平面平行的上下2条直线由于平行，因此斜率相同，只是截距不同'''b = classify.support_vectors_[0] #第一分类中的第一个支持向量点yy_down = a*xx + (b[1]-a*b[0])b = classify.support_vectors_[-1] #第二分类中的最后一个支持向量点yy_up = a*xx + (b[1]-a*b[0])#print(classify.support_vectors_)'''将所有的点、超平面和2条分界线绘制出来k-为实线，即超平面线k--为虚线，2条边界线'''pl.plot(xx, yy, 'k-')pl.plot(xx, yy_down, 'k--')pl.plot(xx, yy_up, 'k--')#单独标记出支持向量点pl.scatter(classify.support_vectors_[:, 0], classify.support_vectors_[:, 1], s=80, facecolors='none')pl.scatter(X[:, 0], X[:, 1], c=Y,cmap=pl.cm.Paired )pl.axis('tight')pl.show() 可视化结果如下： SVM人脸分类我们构建SVM模型分类人脸并进行可视化展示，数据集（lfw）默认存储在~/scikit_learn_data 文件夹中。代码讲解见注释。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129from __future__ import print_functionfrom time import timeimport loggingimport matplotlib.pyplot as pltfrom sklearn.model_selection import train_test_splitfrom sklearn.datasets import fetch_lfw_peoplefrom sklearn.model_selection import GridSearchCVfrom sklearn.decomposition import PCAfrom sklearn.svm import SVCfrom sklearn import metrics#输出日志信息logging.basicConfig(level=logging.INFO, format='%(asctime)s %(message)s')'''下载数据集（国外名人）lfw，可以手动下载：https://ndownloader.figshare.com/files/5976015min_faces_per_person表示提取每类人超过这一数目的数据集#resize调整每张人脸图片的比例，默认是0.5'''lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)n_samples, h, w = lfw_people.images.shape # 返回数据集的实例数及图片宽和高X = lfw_people.data #获取每个实例的特征n_features = X.shape[1] #获取每个实例的特征数y = lfw_people.target #标签数据，及每个人的身份target_names = lfw_people.target_names #存储每个人的人名n_classes = target_names.shape[0] # 有几类人print("===== 数据集中信息 =====")print("数据个数(n_samples):", n_samples) # 1288print("特征个数，维度(n_features):", n_features) # 1850print("结果集类别个数(n_classes):", n_classes) # 7'''调用train_test_split方法拆分训练集合测试集test_size=0.25表示随机抽取25%的测试集'''X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)'''原始数据的特征向量维度非常高，意味着训练模型的复杂度非常高,我们要采用PCA降维，保存的组件数目，也即保留下来的特征个数，此处我们选择150'''n_components = 150print("Extracting the top %d eigenfaces from %d faces" % (n_components, X_train.shape[0]))t0 = time()pca = PCA(n_components=n_components,whiten=True).fit(X_train)print("done in %0.3fs" % (time() - t0))#降维后提取每个实例的特征点eigenfaces = pca.components_.reshape((n_components,h,w))print("Projecting the input data on the eigenfaces orthonormal basis")t0 = time()#把训练集和测试集特征向量转化为更低维的矩阵X_train_pca = pca.transform(X_train)X_test_pca = pca.transform(X_test)print("done in %0.3fs" % (time() - t0))#构建SVN分类模型print("Fitting the classifier to the training set")t0 = time()'''C是一个对错误部分的惩罚gamma的参数对不同核函数有不同的表现，gamma表示使用多少比例的特征点使用不同的c和不同值的gamma，进行多个量的尝试此处组成5*6的网格参数点，然后进行搜索，最后选出准确率最高模型'''param_grid= &#123;'C':[1e3, 5e3, 1e4, 5e4, 1e5], 'gamma':[0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1] &#125;clf = GridSearchCV(SVC(kernel='rbf',class_weight='balanced'),param_grid)clf = clf.fit(X_train_pca,y_train)print("done in %0.3fs" % (time() - t0))print("Best estimator found by grid search:")print(clf.best_estimator_)#评估测试集print("Predicting people's names on the test set")t0 = time()y_pred = clf.predict(X_test_pca)print("done in %0.3fs" % (time() - t0))'''classification_report查看每一类的各种评价指标confusion_matrix(混淆矩阵) 是建一个 n x n 的方格，横行和纵行分别表示真实的每一组测试集的标记和测试集标记的差别，通常表示这些测试数据哪些对了，哪些错了。这个对角线表示了哪些值对了，对角线数字越多，就表示准确率越高。'''print(metrics.classification_report(y_test,y_pred,target_names=target_names))print(metrics.confusion_matrix(y_test, y_pred, labels=range(n_classes)))#将测试结果可视化展示def plot_gallery(images, titles, h, w, n_row=3, n_col=4): plt.figure(figsize=(1.8 * n_col, 2.4 * n_row)) # 定义画布的大小 plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35) # 调整图片显示位置 for i in range(n_row * n_col): plt.subplot(n_row, n_col, i + 1) #画布划分 plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray) # 图片显示 plt.title(titles[i], size=12) # 标题 #获取或设置x、y轴的当前刻度位置和标签 plt.xticks(()) plt.yticks(())#预测函数归类标签和实际归类标签打印#返回预测人脸和测试人脸姓名的对比titledef title(y_pred, y_test, target_names, i): ''' rsplit（' ',1）从右边开始以右边第一个空格为界，分成两个字符 组成一个list 此处代表把'姓'和'名'分开，然后把后面的姓提出来 末尾加[-1]代表引用分割后的列表最后一个元素 ''' pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1] true_name = target_names[y_test[i]].rsplit(' ', 1)[-1] return 'predicted: %s\ntrue: %s' % (pred_name, true_name)#预测出的人名prediction_titles = [title(y_pred, y_test, target_names, i) for i in range(y_pred.shape[0])]#测试集的特征向量矩阵和要预测的人名打印plot_gallery(X_test, prediction_titles, h, w)#打印原图和预测的信息eigenface_titles = ["eigenface %d" % i for i in range(eigenfaces.shape[0])]#调用plot_gallery函数打印出实际是谁，预测的谁，以及提取过特征的脸plot_gallery(eigenfaces, eigenface_titles, h, w)plt.show() 程序运行结果如下 1234567891011121314151617181920212223242526272829303132333435363738===== 数据集中信息 =====数据个数(n_samples): 1288特征个数，维度(n_features): 1850结果集类别个数(n_classes): 7Extracting the top 150 eigenfaces from 966 facesdone in 0.309sProjecting the input data on the eigenfaces orthonormal basisdone in 0.025sFitting the classifier to the training setdone in 23.043sBest estimator found by grid search:SVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0, decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False)Predicting people's names on the test setdone in 0.047s precision recall f1-score support Ariel Sharon 0.81 0.62 0.70 21 Colin Powell 0.72 0.88 0.79 58 Donald Rumsfeld 0.66 0.73 0.69 26 George W Bush 0.88 0.86 0.87 130Gerhard Schroeder 0.81 0.69 0.75 32 Hugo Chavez 1.00 0.69 0.82 13 Tony Blair 0.81 0.83 0.82 42 micro avg 0.81 0.81 0.81 322 macro avg 0.81 0.76 0.78 322 weighted avg 0.82 0.81 0.81 322[[ 13 4 2 2 0 0 0] [ 0 51 1 3 1 0 2] [ 1 2 19 3 0 0 1] [ 1 10 5 112 2 0 0] [ 0 1 1 5 22 0 3] [ 1 1 0 0 0 9 2] [ 0 2 1 2 2 0 35]] 可视化结果展示如下 参考资料机器学习-支持向量机的SVMfetch_lfw_people安装失败]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>svm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之决策树]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-3-6-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树是一个类似于流程图的树结构：其中，每个内部结点表示在一个属性上的测试，每个分支代表一个属性输出，而每个树叶结点代表类或类分布。树的最顶层是根结点。 上图中是否出去玩取决于天气情况（sunny、overcast、rain）和空气湿度（humidity、windy）这2个属性的值。 信息熵决策树算法种类很多，本文主要介绍ID3算法。ID3算法在1970-1980年，由J.Ross. Quinlan提出。在介绍决策树算法前，我们先引入熵（entropy）的概念。 信息和抽象，具体该如何度量呢？1948年，香农提出了信息熵（entropy）的概念。一条信息的信息量大小和它的不确定性有直接的关系，要搞清楚一件非常非常不确定的事情，或者是我们一无所知的事情，需要了解大量信息==&gt;信息量的度量就等于不确定性的多少。我们用比特(bit)来衡量信息的多少，变量的不确定性越大，熵也就越大。计算公式如下，其中$P(x_i)$表示每种事件发生的可能性。 ID3算法 该算法在选择每一个属性判断结点的时候是根据信息增益(Information Gain)。通过下面公式可以计算A来作为节点分类获取了多少信息。$$Gain(A) = Info(D) - InfoA(D)$$ 上图是一个买车的实例，一个人是否买车取决于他的年龄、收入、信用度和他是否是学生。下图（右）是原始数据，下图（左）是简单的决策树划分。那我们是如何确定age作为第一个属性结点的呢？一共有14条数据，其中9人买车，5人没买车，所以根据公式可以算出$Info(D)$的值。接着我们计算选择age作为属性结点的信息熵。age有三个取值，其中youth有5条信息（2人买车，3人没买），middle_aged有4条信息（全部买车），senior有5条信息（3人买车，2人没买），所以根据公式可以算出$Info_{age}(D)$和$Gain(age)$的值，如下所示： 类似，$Gain(income) = 0.029, Gain(student) = 0.151, Gain(credit_rating)=0.048​$所以，选择age作为第一个根节点。重复。。。 最终我们得到了如下所示的决策树。 算法流程 从根节点出发，根节点包括所有的训练样本。 一个节点（包括根节点），若节点内所有样本均属于同一类别，那么将该节点就成为叶节点，并将该节点标记为样本个数最多的类别。 否则利用采用信息增益法来选择用于对样本进行划分的特征，该特征即为测试特征，特征的每一个值都对应着从该节点产生的一个分支及被划分的一个子集。在决策树中，所有的特征均为符号值，即离散值。如果某个特征的值为连续值，那么需要先将其离散化。 递归上述划分子集及产生叶节点的过程，这样每一个子集都会产生一个决策（子）树，直到所有节点变成叶节点。 递归操作的停止条件就是： （1） 一个节点中所有的样本均为同一类别，那么产生叶节点（2） 没有特征可以用来对该节点样本进行划分，这里用attribute_list=null为表示。此时也强制产生叶节点，该节点的类别为样本个数最多的类别（3） 没有样本能满足剩余特征的取值，即test_attribute=a_i 对应的样本为空。此时也强制产生叶节点，该节点的类别为样本个数最多的类别 l流程图如下所示： 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465from sklearn.feature_extraction import DictVectorizerimport csvfrom sklearn import treefrom sklearn import preprocessingimport pydotplus#读入csv文件Extradata = open("G:/PycharmProjects/Machine_Learning/Decision_Tree/AllElectronics.csv","r",encoding="utf-8")reader = csv.reader(Extradata)headers = next(reader) #逐行读取数据 python3中为nex(),python2 中为reader.next()#print(headers)featureList = [] #特征列表labelList = [] #标签列表for row in reader: labelList.append(row[len(row)-1]) #读取每行数据最后一列的标签，存入标签列表 rowDict = &#123;&#125; #创建字典，存储每行特征数据 for i in range(1,len(row)-1):#从第2列开始到倒数第2列读取特征数据 rowDict[headers[i]] = row[i] #对应键值对存储数据 featureList.append(rowDict) #将每行特征数据字典存入特征列表#print(featureList)#向量化特征数据vec = DictVectorizer()dum_X = vec.fit_transform(featureList).toarray()# print("dum_X: " + str(dum_X))# print(vec.get_feature_names())## print("labelList: " + str(labelList))##向量化标签数据label = preprocessing.LabelBinarizer()dum_Y = label.fit_transform(labelList)#print("dum_Y: " + str(dum_Y))#调用sklearn方法构建决策树classfiy = tree.DecisionTreeClassifier(criterion='entropy')classfiy = classfiy.fit(dum_X,dum_Y)#print("classfiy: " + str(classfiy))#将决策树存为dot文件with open("G:/PycharmProjects/Machine_Learning/Decision_Tree/AllElectronics.dot", 'w') as f: f = tree.export_graphviz(classfiy,feature_names=vec.get_feature_names(),out_file=f)#可视化决策树的模型，并存为pdf文件dot_data = tree.export_graphviz(classfiy,out_file=None)graph = pydotplus.graph_from_dot_data(dot_data )graph.write_pdf("aa.pdf")#验证决策树分类模型oneRow_X = dum_X[0,:]#print("oneRow_X: " + str(oneRow_X))newRow_X = oneRow_X#修改特征数据的值newRow_X[0] = 1newRow_X[2] = 0newRow_X[3] = 1newRow_X[4] = 0#print("newRow_X: " + str(newRow_X))finalRow_X = newRow_X.reshape(1,10) #将一位数组转换为二维数组predict_Y = classfiy.predict(finalRow_X) #调用决策树模型预测，注意，finalRow_X要求二维#predict_Y = classfiy.predict([[1., 0., 0., 0., 1., 1., 0., 0., 1., 0.]])print("predict_Y: " + str(predict_Y)) 代码中预测结果见下图： 最后生成的决策树模型见下图： 其他决策树算法 C4.5: QuinlanClassification and Regression Trees (CART): (L. Breiman, J. Friedman, R. Olshen, C. Stone)共同点：都是贪心算法，自上而下(Top-down approach)区别：属性选择度量方法不同： C4.5 （gain ratio), CART(gini index), ID3 (Information Gain) 算法 支持模型 树结构 特征选择 连续值处理 缺失值处理 剪枝 ID3 分类 多叉树 信息增益 不支持 不支持 不支持 C4.5 分类 多叉树 信息增益比 支持 支持 支持 CART 分类、回归 二叉树 基尼系数、均方差 支持 支持 支持 决策树的优缺点优点 简单直观，生成的决策树很直观。 基本不需要预处理，不需要提前归一化，处理缺失值。 既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。 可以处理多维度输出的分类问题。 相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释 可以交叉验证的剪枝来选择模型，从而提高泛化能力。 缺点 决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。 决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。 寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。4.有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。 如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。 参考资料 Scikit-learn中的决策树]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>desion tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习方法之KNN]]></title>
    <url>%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F2019-2-25-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E4%B9%8BKNN%2F</url>
    <content type="text"><![CDATA[K最近邻(k-Nearest Neighbor，KNN)分类算法，思路是：如果一个样本在特征空间中的k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别。 实例分析 有两类不同的样本数据，分别用蓝色的小正方形和红色的小三角形表示，而图正中间的那个绿色的圆所标示的数据则是待分类的数据。也就是说，现在，我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形or红色小三角形），下面，我们就要解决这个问题：给这个绿色的圆分类。 俗话说物以类聚，人以群分，判别一个人是一个什么样品质特征的人，常常可以从他/她身边的朋友入手。我们不是要判别上图中那个绿色的圆是属于哪一类数据么，好说，从它的邻居下手。但一次性看多少个邻居呢？从上图中，你还能看到： 若K=3，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。 若K=5，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于蓝色的正方形一类。 我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。 时间复杂度 KNN 算法本身简单有效，它是一种 lazy-learning 算法，分类器不需要使用训练集进行训练，训练时间复杂度为0。KNN 分类的计算复杂度和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为 n，那么 KNN 的分类时间复杂度为O(n)。 算法三要素 K 值的选择会对算法的结果产生重大影响。K值较小意味着只有与输入实例较近的训练实例才会对预测结果起作用，但容易发生过拟合；如果 K 值较大，优点是可以减少学习的估计误差，但缺点是学习的近似误差增大，这时与输入实例较远的训练实例也会对预测起作用，使预测发生错误。在实际应用中，K 值一般选择一个较小的数值，通常采用交叉验证的方法来选择最优的 K 值。随着训练实例数目趋向于无穷和 K=1 时，误差率不会超过贝叶斯误差率的2倍，如果K也趋向于无穷，则误差率趋向于贝叶斯误差率。 分类决策规则往往是多数表决，即由输入实例的 K 个最临近的训练实例中的多数类决定输入实例的类别。 距离度量一般采用Lp 距离，当p=2时，即为欧氏距离，在度量之前，应该将每个属性的值规范化，这样有助于防止具有较大初始值域的属性比具有较小初始值域的属性的权重过大。 算法实现 算法步骤 （1）计算已知类别数据集中的点与当前点之间的距离（2）按照距离递增次序排序（3）选取与当前点距离最小的K个点（4）确定前K个点所在类别出现的频率（5）返回前K个点出现频率最高的类别作为当前点的预测分类 代码实例 iris数据集，包含150条用例，每条用例共5列，前4列为特征数据，最后一列为标签数据。 简单实现调用sklearn自带的库1234567891011from sklearn import neighborsfrom sklearn import datasetsknn = neighbors.KNeighborsClassifier() # 申明对象iris = datasets.load_iris() # 导入数据print(iris)knn.fit(iris.data,iris.target) # 生成KNN模型predicit_label = knn.predict([[0.2,0.3,0.3,0.2]]) # 预测print(predicit_label) 复杂实现自己编写函数实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889import csvimport randomimport mathimport operator'''导入数据filename数据存储路径radio，按指定比例将数据划分为训练集和测试集'''def loadDateset(filename,radio,trainSet=[],testSet=[]): with open(filename,'rt') as csvfile: lines = csv.reader(csvfile) # 逐行读取数据 dataset = list(lines) # 转换为列表存储 for x in range(len(dataset)-1): # 循环每行数据，将前4个特征值存入数组 for y in range(4): dataset[x][y] = float(dataset[x][y]) if random.random()&lt;radio: # 取随机值，小于radio就划分到训练集 trainSet.append(dataset[x]) else: testSet.append(dataset[x])'''计算2个样例之间的距离（欧氏距离），length表示数据的维度'''def evaluateDistance(instance1,instance2,length): distance = 0 for x in range(length): # 循环每一维度，数值相减并对其平方，然后进行累加 distance += pow((instance1[x]-instance2[x]),2) return math.sqrt(distance) # 开方求距离'''对于一个实例，找到离他最近的k个实例'''def getNeighbors(trainSet,testInstance,k): distance = [] length = len(testInstance)-1 # 每个测试实例的维度 for x in range(len(trainSet)-1): # 训练集中每一个实例到测试实例的距离 dist = evaluateDistance(testInstance,trainSet[x],length) distance.append((trainSet[x],dist)) # 将每一个训练实例和其对应到测试实例的距离存储到列表 distance.sort(key=operator.itemgetter(1)) # operator模块提供的itemgetter函数用于获取距离维度的数据并排序 neighbors = [] # 存储离一个实例最近的几个实例 for x in range(k): # 取distance中前k个实例存储到neighbors neighbors.append(distance[x][0]) return neighbors'''在最近的K个实例中投票，少数服从多数，把要预测的实例归到多数那一类'''def getResponse(neighbors): classvotes = &#123;&#125; # 定义一个字典，存储每一类别的数目 for x in range(len(neighbors)): response = neighbors[x][-1] if response in classvotes: classvotes[response] += 1 else: classvotes[response] = 1 sortedVotes = sorted(classvotes.items(),key=operator.itemgetter(1),reverse=True) # 排序，输出数目最大的类别 return sortedVotes[0][0]'''计算测试集的准确率'''def getAccuracy(testSet,predictions): correct = 0 for x in range(len(testSet)): if testSet[x][-1] == predictions[x]: # 每行测试用例最后一列的标签与预测标签是否相等 correct += 1 return (correct/float(len(testSet)))*100.0def main(): trainSet = [] # 存储训练集 testSet = [] # 存储测试集 radio = 0.80 # 按4：1划分 loadDateset('G:/PycharmProjects/Machine_Learning/KNN/irisdata.txt',radio,trainSet,testSet) #导入数据并划分 print("trainSetNum: "+ str(len(trainSet))) print("testSetNum: "+ str(len(testSet))) predictions = [] k = 3 # 选取前k个最近的实例 for x in range(len(testSet)): # 循环预测测试集合的每个实例 neighbors = getNeighbors(trainSet,testSet[x],k) result = getResponse(neighbors) predictions.append(result) print('&gt;predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1])) accuracy = getAccuracy(testSet,predictions) print('Accuracy: ' + repr(accuracy) + '%')if __name__ == '__main__': main() 运行结果1234567891011121314151617181920212223242526272829trainSetNum: 124testSetNum: 26&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-setosa', actual='Iris-setosa'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-versicolor'&gt;predicted='Iris-versicolor', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-versicolor', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'&gt;predicted='Iris-virginica', actual='Iris-virginica'Accuracy: 92.3076923076923% 算法优缺点优点 精度高、对异常值不敏感、无数据输入假定。 KNN算法不仅可以用于分类，还可以用于回归。通过找出一个样本的k个最近邻居，将这些邻居的属性的平均值赋给该样本，就可以得到该样本的属性。 缺点 样本分布不均衡时，如果一个类的样本容量很大，而其他类样本容量很小时，有可能导致当输入一个新样本时，该样本的K个邻居中大容量类的样本占多数。 计算量较大，因为对每一个待分类的文本都要计算它到全体已知样本的距离，才能求得它的K个最近邻点。 适用的数据范围 数值型和标称型。标称型：一般在有限的数据中取，而且只存在‘是’和‘否’两种不同的结果（一般用于分类）数值型：可以在无限的数据中取，而且数值比较具体化，例如4.02,6.23这种值（一般用于回归分析） K-Means简介 如图所示，数据样本用圆点表示，每个簇的中心点用叉叉表示。(a) 刚开始时是原始数据，杂乱无章，没有label，看起来都一样，都是绿色的。(b) 假设数据集可以分为两类，令K=2，随机在坐标上选两个点，作为两个类的中心点。(c-f) 演示了聚类的两种迭代。先划分，把每个数据样本划分到最近的中心点那一簇；划分完后，更新每个簇的中心，即把该簇的所有数据点的坐标加起来去平均值。这样不断进行”划分—更新—划分—更新”，直到每个簇的中心不在移动为止。 KNN和K-Means的区别 KNN K-Means KNN是分类算法 K-Means是聚类算法 监督学习 非监督学习 喂给它的数据集是带label的数据，已经是完全正确的数据 喂给它的数据集是无label的数据，是杂乱无章的，经过聚类后才变得有点顺序，先无序，后有序 没有明显的前期训练过程，属于memory-based learning 有明显的前期训练过程 K的含义：来了一个样本x，要给它分类，即求出它的y，就从数据集中，在x附近找离它最近的K个数据点，这K个数据点，类别c占的个数最多，就把x的label设为c K的含义：K是人工固定好的数字，假设数据集合可以分为K个簇，由于是依靠人工定好，需要一点先验知识 两者相似点：都包含这样的过程，给定一个点，在数据集中找离它最近的点。即二者都用到了NN(Nears Neighbor)算法。 参考资料 KNN与K-Means的区别]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>knn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow入门学习2]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-2-24-Tensorflow%E5%85%A5%E9%97%A82%2F</url>
    <content type="text"><![CDATA[这篇博文主要是TensorFlow的一个简单入门，并介绍了如何实现Softmax Regression模型，来对MNIST数据集中的数字手写体进行识别。 然而，由于Softmax Regression模型相对简单，所以最终的识别准确率并不高。下面将针对MNIST数据集构建更加复杂精巧的模型，以进一步提高识别准确率。 深度学习模型TensorFlow很适合用来进行大规模的数值计算，其中也包括实现和训练深度神经网络模型。下面将介绍TensorFlow中模型的基本组成部分，同时将构建一个CNN模型来对MNIST数据集中的数字手写体进行识别。 基本设置在我们构建模型之前，我们首先加载MNIST数据集，然后开启一个TensorFlow会话(session)。 加载MNIST数据集TensorFlow中已经有相关脚本，来自动下载和加载MNIST数据集。（脚本会自动创建MNIST_data文件夹来存储数据集）。下面是脚本程序： 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets('MNIST_data', one_hot=True) 这里mnist是一个轻量级的类文件，存储了NumPy格式的训练集、验证集和测试集，它同样提供了数据中mini-batch迭代的功能。 开启TensorFlow会话TensorFlow后台计算依赖于高效的C++，与后台的连接称为一个会话(session)。TensorFlow中的程序使用，通常都是先创建一个图(graph)，然后在一个会话(session)里运行它。 这里我们使用了一个更为方便的类，InteractiveSession，这能让你在构建代码时更加灵活。InteractiveSession允许你做一些交互操作，通过创建一个计算流图(computation graph)来部分地运行图计算。当你在一些交互环境（例如IPython）中使用时将更加方便。如果你不是使用InteractiveSession，那么你要在启动一个会话和运行图计算前，创建一个整体的计算流图。 下面是如何创建一个InteractiveSession： 12import tensorflow as tfsess = tf.InteractiveSession() 计算流图(Computation Graph)为了在Python中实现高效的数值运算，通常会使用一些Python以外的库函数，如NumPy。但是，这样做会造成转换Python操作的开销，尤其是在GPUs和分布式计算的环境下。TensorFlow在这一方面（指转化操作）做了优化，它让我们能够在Python之外描述一个包含各种交互计算操作的整体流图，而不是每次都独立地在Python之外运行一个单独的计算，避免了许多的转换开销。这样的优化方法同样用在了Theano和Torch上。 所以，以上这样的Python代码的作用是简历一个完整的计算流图，然后指定图中的哪些部分需要运行。关于计算流图的更多具体使用见这里。 Softmax Regression模型见这篇博文。 CNN模型Softmax Regression模型在MNIST数据集上91%的准确率，其实还是比较低的。下面我们将使用一个更加精巧的模型，一个简单的卷积神经网络模型(CNN)。这个模型能够达到99.2%的准确率，尽管这不是最高的，但已经足够接受了。 权值初始化为了建立模型，我们需要先创建一些权值(w)和偏置(b)等参数，这些参数的初始化过程中需要加入一小部分的噪声以破坏参数整体的对称性，同时避免梯度为0.由于我们使用ReLU激活函数（详细介绍)），所以我们通常将这些参数初始化为很小的正值。为了避免重复的初始化操作，我们可以创建下面两个函数： 1234567def weight_variable(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial) 卷积(Convolution)和池化(Pooling)TensorFlow同样提供了方便的卷积和池化计算。怎样处理边界元素？怎样设置卷积窗口大小？在这个例子中，卷积操作仅使用了滑动步长为1的窗口，使用0进行填充，所以输出规模和输入的一致；而池化操作是在2 * 2的窗口内采用最大池化技术(max-pooling)。为了使代码简洁，同样将这些操作抽象为函数形式： 123456def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') 其中，padding=&#39;SAME&#39;表示通过填充0，使得输入和输出的形状一致。 第一层：卷积层第一层是卷积层，卷积层将要计算出32个特征映射(feature map)，对每个5 * 5的patch。它的权值tensor的大小为[5, 5, 1, 32]. 前两维是patch的大小，第三维时输入通道的数目，最后一维是输出通道的数目。我们对每个输出通道加上了偏置(bias)。 12W_conv1 = weight_variable([5, 5, 1, 32])b_conv1 = bias_variable([32]) 为了使得图片与计算层匹配，我们首先reshape输入图像x为4维的tensor，第2、3维对应图片的宽和高，最后一维对应颜色通道的数目。（-1就是缺省值，就是先以你们合适，到时总数除以你们几个的乘积，我该是几就是几） 1x_image = tf.reshape(x, [-1,28,28,1]) 然后，使用weight tensor对x_image进行卷积计算，加上bias，再应用到一个ReLU激活函数，最终采用最大池化。 12h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)h_pool1 = max_pool_2x2(h_conv1) 第二层：卷积层为了使得网络有足够深度，我们重复堆积一些相同类型的层。第二层将会有64个特征，对应每个5 * 5的patch。 12345W_conv2 = weight_variable([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2) 全连接层到目前为止，图像的尺寸被缩减为7 * 7，我们最后加入一个神经元数目为1024的全连接层来处理所有的图像上。接着，将最后的pooling层的输出reshape为一个一维向量，与权值相乘，加上偏置，再通过一个ReLu函数。 12345W_fc1 = weight_variable([7 * 7 * 64, 1024])b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1) 整个CNN的网络结构如下图： Dropout为了减少过拟合程度，在输出层之前应用dropout技术（即丢弃某些神经元的输出结果）。我们创建一个placeholder来表示一个神经元的输出在dropout时不被丢弃的概率。Dropout能够在训练过程中使用，而在测试过程中不使用。TensorFlow中的tf.nn.dropout操作能够利用mask技术处理各种规模的神经元输出。 12keep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob) 输出层最终，我们用一个softmax层，得到类别上的概率分布。（与之前的Softmax Regression模型相同）。 1234W_fc2 = weight_variable([1024, 10])b_fc2 = bias_variable([10])y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2) 模型训练和测试为了测试模型的性能，需要先对模型进行训练，然后应用在测试集上。和之前Softmax Regression模型中的训练、测试过程类似。区别在于： 用更复杂的ADAM最优化方法代替了之前的梯度下降； 增了额外的参数keep_prob在feed_dict中，以控制dropout的几率； 在训练过程中，增加了log输出功能（每100次迭代输出一次）。 下面是程序： 123456789101112131415cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))sess.run(tf.initialize_all_variables())for i in range(20000): batch = mnist.train.next_batch(50) if i%100 == 0: train_accuracy = accuracy.eval(feed_dict=&#123; x:batch[0], y_: batch[1], keep_prob: 1.0&#125;) print("step %d, training accuracy %g"%(i, train_accuracy)) train_step.run(feed_dict=&#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;)print("test accuracy %g"%accuracy.eval(feed_dict=&#123; x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0&#125;)) 最终，模型在测试集上的准确率大概为99.2%，性能上要优于之前的Softmax Regression模型。 完整代码及运行结果利用CNN模型实现手写体识别的完整代码如下： 123456789101112131415161718__author__ = 'chapter'import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datadef weight_varible(shape): initial = tf.truncated_normal(shape, stddev=0.1) return tf.Variable(initial)def bias_variable(shape): initial = tf.constant(0.1, shape=shape) return tf.Variable(initial)def conv2d(x, W): return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')def max_pool_2x2(x): return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') ​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)print("Download Done!")sess = tf.InteractiveSession()# parasW_conv1 = weight_varible([5, 5, 1, 32])b_conv1 = bias_variable([32])# conv layer-1x = tf.placeholder(tf.float32, [None, 784])x_image = tf.reshape(x, [-1, 28, 28, 1])h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)h_pool1 = max_pool_2x2(h_conv1)# conv layer-2W_conv2 = weight_varible([5, 5, 32, 64])b_conv2 = bias_variable([64])h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)h_pool2 = max_pool_2x2(h_conv2)# full connectionW_fc1 = weight_varible([7 * 7 * 64, 1024])b_fc1 = bias_variable([1024])h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)# dropoutkeep_prob = tf.placeholder(tf.float32)h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)# output layer: softmaxW_fc2 = weight_varible([1024, 10])b_fc2 = bias_variable([10])y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)y_ = tf.placeholder(tf.float32, [None, 10])# model trainingcross_entropy = -tf.reduce_sum(y_ * tf.log(y_conv))train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)correct_prediction = tf.equal(tf.arg_max(y_conv, 1), tf.arg_max(y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))sess.run(tf.initialize_all_variables())for i in range(20000): batch = mnist.train.next_batch(50) if i % 100 == 0: train_accuacy = accuracy.eval(feed_dict=&#123;x: batch[0], y_: batch[1], keep_prob: 1.0&#125;) print("step %d, training accuracy %g"%(i, train_accuacy)) train_step.run(feed_dict = &#123;x: batch[0], y_: batch[1], keep_prob: 0.5&#125;)# accuacy on testprint("test accuracy %g"%(accuracy.eval(feed_dict=&#123;x: mnist.test.images, y: mnist.test.labels, keep_prob: 1.0&#125;))) 运行结果如下图： 参考资料 Tensorflow 实战 Google 深度学习框架TensorFlow——Mnist手写数字识别实战教程]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow入门学习]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-2-23-Tensorflow%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[TensorFlow 简介TensorFlow是Google在2015年11月份开源的人工智能系统（Github项目地址），是之前所开发的深度学习基础架构DistBelief的改进版本，该系统可以被用于语音识别、图片识别等多个领域。 官网上对TensorFlow的介绍是，一个使用数据流图(data flow graphs)技术来进行数值计算的开源软件库。数据流图中的节点，代表数值运算；节点节点之间的边，代表多维数据(tensors)之间的某种联系。你可以在多种设备（含有CPU或GPU）上通过简单的API调用来使用该系统的功能。TensorFlow是由Google Brain团队的研发人员负责的项目。 什么是数据流图(Data Flow Graph)数据流图是描述有向图中的数值计算过程。有向图中的节点通常代表数学运算，但也可以表示数据的输入、输出和读写等操作；有向图中的边表示节点之间的某种联系，它负责传输多维数据(Tensors)。图中这些tensors的flow也就是TensorFlow的命名来源。 节点可以被分配到多个计算设备上，可以异步和并行地执行操作。因为是有向图，所以只有等到之前的入度节点们的计算状态完成后，当前节点才能执行操作。 TensorFlow的特性1 灵活性 TensorFlow不是一个严格的神经网络工具包，只要你可以使用数据流图来描述你的计算过程，你可以使用TensorFlow做任何事情。你还可以方便地根据需要来构建数据流图，用简单的Python语言来实现高层次的功能。 2 可移植性 TensorFlow可以在任意具备CPU或者GPU的设备上运行，你可以专注于实现你的想法，而不用去考虑硬件环境问题，你甚至可以利用Docker技术来实现相关的云服务。 3 提高开发效率 TensorFlow可以提升你所研究的东西产品化的效率，并且可以方便与同行们共享代码。 4 支持语言选项 目前TensorFlow支持Python和C++语言。（但是你可以自己编写喜爱语言的SWIG接口） 5 充分利用硬件资源，最大化计算性能 基本使用你需要理解在TensorFlow中，是如何： 将计算流程表示成图； 通过Sessions来执行图计算； 将数据表示为tensors； 使用Variables来保持状态信息； 分别使用feeds和fetches来填充数据和抓取任意的操作结果； 概览TensorFlow是一种将计算表示为图的编程系统。图中的节点称为ops(operation的简称)。一个ops使用0个或以上的Tensors，通过执行某些运算，产生0个或以上的Tensors。一个Tensor是一个多维数组，例如，你可以将一批图像表示为一个四维的数组[batch, height, width, channels]，数组中的值均为浮点数。 TensorFlow中的图描述了计算过程，图通过Session的运行而执行计算。Session将图的节点们(即ops)放置到计算设备(如CPUs和GPUs)上，然后通过方法执行它们；这些方法执行完成后，将返回tensors。在Python中的tensor的形式是numpy ndarray对象，而在C/C++中则是tensorflow::Tensor. 图计算TensorFlow程序中图的创建类似于一个 [施工阶段]，而在 [执行阶段] 则利用一个session来执行图中的节点。很常见的情况是，在 [施工阶段] 创建一个图来表示和训练神经网络，而在 [执行阶段] 在图中重复执行一系列的训练操作。 创建图在TensorFlow中，Constant是一种没有输入的ops，但是你可以将它作为其他ops的输入。Python库中的ops构造器将返回构造器的输出。TensorFlow的Python库中有一个默认的图，将ops构造器作为节点，更多可了解Graph Class文档。 见下面的示例代码： 12345678910111213141516import tensorflow as tf# Create a Constant op that produces a 1x2 matrix. The op is# added as a node to the default graph.## The value returned by the constructor represents the output# of the Constant op.matrix1 = tf.constant([[3., 3.]])# Create another Constant that produces a 2x1 matrix.matrix2 = tf.constant([[2.],[2.]])# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.# The returned value, 'product', represents the result of the matrix# multiplication.product = tf.matmul(matrix1, matrix2) 默认的图(Default Graph)现在有了三个节点：两个 Constant()ops和一个matmul()op。为了得到这两个矩阵的乘积结果，还需要在一个session中启动图计算。 在Session中执行图计算见下面的示例代码，更多可了解Session Class： 1234567891011121314151617181920# Launch the default graph.sess = tf.Session()# To run the matmul op we call the session 'run()' method, passing 'product'# which represents the output of the matmul op. This indicates to the call# that we want to get the output of the matmul op back.## All inputs needed by the op are run automatically by the session. They# typically are run in parallel.## The call 'run(product)' thus causes the execution of threes ops in the# graph: the two constants and matmul.## The output of the op is returned in 'result' as a numpy `ndarray` object.result = sess.run(product)print(result)# ==&gt; [[ 12.]]# Close the Session when we're done.sess.close() Sessions最后需要关闭，以释放相关的资源；你也可以使用with模块，session在with模块中自动会关闭： 123with tf.Session() as sess: result = sess.run([product]) print(result) TensorFlow的这些节点最终将在计算设备(CPUs,GPus)上执行运算。如果是使用GPU，默认会在第一块GPU上执行，如果你想在第二块多余的GPU上执行： 123456with tf.Session() as sess: with tf.device("/gpu:1"): matrix1 = tf.constant([[3., 3.]]) matrix2 = tf.constant([[2.],[2.]]) product = tf.matmul(matrix1, matrix2) ... device中的各个字符串含义如下： &quot;/cpu:0&quot;: 你机器的CPU； &quot;/gpu:0&quot;: 你机器的第一个GPU； &quot;/gpu:1&quot;: 你机器的第二个GPU； 关于TensorFlow中GPU的使用见这里。 交互环境下的使用以上的python示例中，使用了Session和Session.run()来执行图计算。然而，在一些Python的交互环境下(如IPython中)，你可以使用InteractiveSession类，以及Tensor.eval()、Operation.run()等方法。例如，在交互的Python环境下执行以下代码： 1234567891011121314151617# Enter an interactive TensorFlow Session.import tensorflow as tfsess = tf.InteractiveSession()x = tf.Variable([1.0, 2.0])a = tf.constant([3.0, 3.0])# Initialize 'x' using the run() method of its initializer op.x.initializer.run()# Add an op to subtract 'a' from 'x'. Run it and print the resultsub = tf.sub(x, a)print(sub.eval())# ==&gt; [-2. -1.]# Close the Session when we're done.sess.close() TensorsTensorFlow中使用tensor数据结构（实际上就是一个多维数据）表示所有的数据，并在图计算中的节点之间传递数据。一个tensor具有固定的类型、级别和大小，更加深入理解这些概念可参考Rank, Shape, and Type。 变量(Variables)变量在图执行的过程中，保持着自己的状态信息。下面代码中的变量充当了一个简单的计数器角色： 123456789101112131415161718192021222324252627282930# Create a Variable, that will be initialized to the scalar value 0.state = tf.Variable(0, name="counter")# Create an Op to add one to `state`.one = tf.constant(1)new_value = tf.add(state, one)update = tf.assign(state, new_value)# Variables must be initialized by running an `init` Op after having# launched the graph. We first have to add the `init` Op to the graph.init_op = tf.initialize_all_variables()# Launch the graph and run the ops.with tf.Session() as sess: # Run the 'init' op sess.run(init_op) # Print the initial value of 'state' print(sess.run(state)) # Run the op that updates 'state' and print 'state'. for _ in range(3): sess.run(update) print(sess.run(state))# output:# 0# 1# 2# 3 赋值函数assign()和add()函数类似，直到session的run()之后才会执行操作。与之类似的，一般我们会将神经网络模型中的参数表示为一系列的变量，在模型的训练过程中对变量进行更新操作。 抓取(Fetches)为了抓取ops的输出，需要先执行session的run函数。然后，通过print函数打印状态信息。 123456789101112input1 = tf.constant(3.0)input2 = tf.constant(2.0)input3 = tf.constant(5.0)intermed = tf.add(input2, input3)mul = tf.mul(input1, intermed)with tf.Session() as sess: result = sess.run([mul, intermed]) print(result)# output:# [array([ 21.], dtype=float32), array([ 7.], dtype=float32)] 所有tensors的输出都是一次性 [连贯] 执行的。 填充(Feeds)TensorFlow也提供这样的机制：先创建特定数据类型的占位符(placeholder)，之后再进行数据的填充。例如下面的程序： 123456789input1 = tf.placeholder(tf.float32)input2 = tf.placeholder(tf.float32)output = tf.mul(input1, input2)with tf.Session() as sess: print(sess.run([output], feed_dict=&#123;input1:[7.], input2:[2.]&#125;))# output:# [array([ 14.], dtype=float32)] 如果不对placeholder()的变量进行数据填充，将会引发错误，更多的例子可参考MNIST fully-connected feed tutorial (source code)。 示例：曲线拟合下面是一段使用Python写的，曲线拟合计算。官网将此作为刚开始介绍的示例程序。 1234567891011121314151617181920212223242526272829303132# 简化调用库名import tensorflow as tfimport numpy as np# 模拟生成100对数据对, 对应的函数为y = x * 0.1 + 0.3x_data = np.random.rand(100).astype("float32")y_data = x_data * 0.1 + 0.3# 指定w和b变量的取值范围（注意我们要利用TensorFlow来得到w和b的值）W = tf.Variable(tf.random_uniform([1], -1.0, 1.0))b = tf.Variable(tf.zeros([1]))y = W * x_data + b# 最小化均方误差loss = tf.reduce_mean(tf.square(y - y_data))optimizer = tf.train.GradientDescentOptimizer(0.5)train = optimizer.minimize(loss)# 初始化TensorFlow参数init = tf.initialize_all_variables()# 运行数据流图（注意在这一步才开始执行计算过程）sess = tf.Session()sess.run(init)# 观察多次迭代计算时，w和b的拟合值for step in xrange(201): sess.run(train) if step % 20 == 0: print(step, sess.run(W), sess.run(b))# 最好的情况是w和b分别接近甚至等于0.1和0.3 MNIST手写体识别任务下面我们介绍一个神经网络中的经典示例，MNIST手写体识别。这个任务相当于是机器学习中的HelloWorld程序。 MNIST数据集介绍MNIST是一个简单的图片数据集（数据集下载地址），包含了大量的数字手写体图片。下面是一些示例图片： MNIST数据集是含标注信息的，以上图片分别代表5, 0, 4和1。 由于MNIST数据集是TensorFlow的示例数据，所以我们不必下载。只需要下面两行代码，即可实现数据集的读取工作： 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("MNIST_data/", one_hot=True) MNIST数据集一共包含三个部分：训练数据集(55,000份，mnist.train)、测试数据集(10,000份，mnist.test)和验证数据集(5,000份，mnist.validation)。一般来说，训练数据集是用来训练模型，验证数据集可以检验所训练出来的模型的正确性和是否过拟合，测试集是不可见的（相当于一个黑盒），但我们最终的目的是使得所训练出来的模型在测试集上的效果（这里是准确性）达到最佳。 MNIST中的一个数据样本包含两块：手写体图片和对于的label。这里我们用xs和ys分别代表图片和对应的label，训练数据集和测试数据集都有xs和ys，我们使用 mnist.train.images 和 mnist.train.labels 表示训练数据集中图片数据和对于的label数据。 一张图片是一个28*28的像素点矩阵，我们可以用一个同大小的二维整数矩阵来表示。如下： 但是，这里我们可以先简单地使用一个长度为28 * 28 = 784的一维数组来表示图像，因为下面仅仅使用softmax regression来对图片进行识别分类（尽管这样做会损失图片的二维空间信息，所以实际上最好的计算机视觉算法是会利用图片的二维信息的）。 所以MNIST的训练数据集可以是一个形状为55000 * 784位的tensor，也就是一个多维数组，第一维表示图片的索引，第二维表示图片中像素的索引（”tensor”中的像素值在0到1之间）。如下图： MNIST中的数字手写体图片的label值在1到9之间，是图片所表示的真实数字。这里用One-hot vector来表述label值，vector的长度为label值的数目，vector中有且只有一位为1，其他为0.为了方便，我们表示某个数字时在vector中所对应的索引位置设置1，其他位置元素为0. 例如用[0,0,0,1,0,0,0,0,0,0]来表示3。所以，mnist.train.labels是一个55000 * 10的二维数组。如下： 以上是MNIST数据集的描述及TensorFlow中表示。下面介绍Softmax Regression模型。 Softmax Regression模型数字手写体图片的识别，实际上可以转化成一个概率问题，如果我们知道一张图片表示9的概率为80%，而剩下的20%概率分布在8，6和其他数字上，那么从概率的角度上，我们可以大致推断该图片表示的是9. Softmax Regression是一个简单的模型，很适合用来处理得到一个待分类对象在多个类别上的概率分布。所以，这个模型通常是很多高级模型的最后一步。 Softmax Regression大致分为两步（暂时不知道如何合理翻译，转原话）： Step 1: add up the evidence of our input being in certain classes;Step 2: convert that evidence into probabilities. 为了利用图片中各个像素点的信息，我们将图片中的各个像素点的值与一定的权值相乘并累加，权值的正负是有意义的，如果是正的，那么表示对应像素值（不为0的话）对表示该数字类别是积极的；否则，对应像素值(不为0的话)对表示该数字类别是起负面作用的。下面是一个直观的例子，图片中蓝色表示正值，红色表示负值（蓝色区域的形状趋向于数字形状）： 最后，我们在一个图片类别的evidence(不知如何翻译..)中加入偏置(bias)，加入偏置的目的是加入一些与输入独立无关的信息。所以图片类别的evidence可表示为 $$ evidence_{i}=\sum _{j}W_{ij}x_{j}+b_{i} $$ 其中，\( W_i \) 和 \( b_i \) 分别为类别 \( i \) 的权值和偏置，\( j \) 是输入图片 \( x \) 的像素索引。然后，我们将得到的evidence值通过一个”softmax”函数转化为概率值 \( y \) : $$ y = softmax(evidence) $$ 这里softmax函数的作用相当于是一个转换函数，它的作用是将原始的线性函数输出结果以某种方式转换为我们需要的值，这里我们需要0-9十个类别上的概率分布。softmax函数的定义如下： $$ softmax(x) = normalize(exp(x)) ​$$ 具体计算方式如下 $$ softmax(x)_{i} = \dfrac {exp\left( x_{i}\right) } {\Sigma _{j}exp\left( x_{j}\right) } $$ 这里的softmax函数能够得到类别上的概率值分布，并保证所有类别上的概率值之和为1. 下面的图示将有助于你理解softmax函数的计算过程： 如果我们将这个过程公式化，将得到 实际的计算中，我们通常采用矢量计算的方式，如下 也可以简化成 $$ y = softmax( Wx + b ) ​$$ Softmax Regression的程序实现为了在Python中进行科学计算工作，我们常常使用一些独立库函数包，例如NumPy来实现复杂的矩阵计算。但是由于Python的运行效率并不够快，所以常常用一些更加高效的语言来实现。但是，这样做会带来语言转换（例如转换回python操作）的开销。TensorFlow在这方面做了一些优化，可以对你所描述的一系列的交互计算的流程完全独立于Python之外，从而避免了语言切换的开销。 为了使用TensorFlow，我们需要引用该库函数 1import tensorflow as tf 我们利用一些符号变量来描述交互计算的过程，创建如下 1x = tf.placeholder(tf.float32, [None, 784]) 这里的 \( x \) 不是一个特定的值，而是一个占位符，即需要时指定。如前所述，我们用一个1 * 784维的向量来表示一张MNIST中的图片。我们用[None, 784]这样一个二维的tensor来表示整个MNIST数据集，其中None表示可以为任意值。 我们使用Variable(变量)来表示模型中的权值和偏置，这些参数是可变的。如下， 12W = tf.Variable(tf.zeros([784, 10]))b = tf.Variable(tf.zeros([10])) 这里的W和b均被初始化为0值矩阵。W的维数为784 * 10，是因为我们需要将一个784维的像素值经过相应的权值之乘转化为10个类别上的evidence值；b是十个类别上累加的偏置值。 实现softmax regression模型仅需要一行代码，如下 1y = tf.nn.softmax(tf.matmul(x, W) + b) 其中，matmul函数实现了 x 和 W 的乘积，这里 x 为二维矩阵，所以放在前面。可以看出，在TensorFlow中实现softmax regression模型是很简单的。 模型的训练在机器学习中，通常需要选择一个代价函数（或者损失函数），来指示训练模型的好坏。这里，我们使用交叉熵函数（cross-entropy）作为代价函数，交叉熵是一个源于信息论中信息压缩领域的概念，但是现在已经应用在多个领域。它的定义如下： $$ H_{y’}\left( y\right) = -\sum _{i}y_{i}’\log \left( y_{i}\right) $$ 这里 \( y \) 是所预测的概率分布，而 \( y’ \) 是真实的分布(one-hot vector表示的图片label)。直观上，交叉熵函数的输出值表示了预测的概率分布与真实的分布的符合程度。更加深入地理解交叉熵函数，可参考这篇博文。 为了实现交叉熵函数，我们需要先设置一个占位符在存放图片的正确label值， 1y_ = tf.placeholder(tf.float32, [None, 10]) 然后得到交叉熵，即\( -\sum y’\log \left( y\right) \)： 1cross_entropy = -tf.reduce_sum(y_*tf.log(y)) 注意，以上的交叉熵不是局限于一张图片，而是整个可用的数据集。 接下来我们以代价函数最小化为目标，来训练模型以得到相应的参数值(即权值和偏置)。TensorFlow知道你的计算过程，它会自动利用后向传播算法来得到相应的参数变化，对代价函数最小化的影响作用。然后，你可以选择一个优化算法来决定如何最小化代价函数。如下， 1train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) 在这里，我们使用了一个学习率为0.01的梯度下降算法来最小化代价函数。梯度下降是一个简单的计算方式，即使得变量值朝着减小代价函数值的方向变化。TensorFlow也提供了许多其他的优化算法，仅需要一行代码即可实现调用。 TensorFlow提供了以上简单抽象的函数调用功能，你不需要关心其底层实现，可以更加专心于整个计算流程。在模型训练之前，还需要对所有的参数进行初始化： 1init = tf.initialize_all_variables() 我们可以在一个Session里面运行模型，并且进行初始化： 12sess = tf.Session()sess.run(init) 接下来，进行模型的训练 123for i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;) 每一次的循环中，我们取训练数据中的100个随机数据，这种操作成为批处理(batch)。然后，每次运行train_step时，将之前所选择的数据，填充至所设置的占位符中，作为模型的输入。 以上过程成为随机梯度下降，在这里使用它是非常合适的。因为它既能保证运行效率，也能一定程度上保证程序运行的正确性。（理论上，我们应该在每一次循环过程中，利用所有的训练数据来得到正确的梯度下降方向，但这样将非常耗时）。 模型的评价怎样评价所训练出来的模型？显然，我们可以用图片预测类别的准确率。 首先，利用tf.argmax()函数来得到预测和实际的图片label值，再用一个tf.equal()函数来判断预测值和真实值是否一致。如下： 1correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) correct_prediction是一个布尔值的列表，例如 [True, False, True, True]。可以使用tf.cast()函数将其转换为[1, 0, 1, 1]，以方便准确率的计算（以上的是准确率为0.75）。 1accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float")) 最后，我们来获取模型在测试集上的准确率， 1print(sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)) Softmax regression模型由于模型较简单，所以在测试集上的准确率在91%左右，这个结果并不算太好。通过一些简单的优化，准确率可以达到97%，目前最好的模型的准确率为99.7%。（这里有众多模型在MNIST数据集上的运行结果）。 完整代码及运行结果利用Softmax模型实现手写体识别的完整代码如下： 12345678910111213141516171819202122232425262728293031323334353637__author__ = 'chapter'import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("MNIST_data/", one_hot=True)print("Download Done!")x = tf.placeholder(tf.float32, [None, 784])# parasW = tf.Variable(tf.zeros([784, 10]))b = tf.Variable(tf.zeros([10]))y = tf.nn.softmax(tf.matmul(x, W) + b)y_ = tf.placeholder(tf.float32, [None, 10])# loss funccross_entropy = -tf.reduce_sum(y_ * tf.log(y))train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)# initinit = tf.initialize_all_variables()sess = tf.Session()sess.run(init)# trainfor i in range(1000): batch_xs, batch_ys = mnist.train.next_batch(100) sess.run(train_step, feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)correct_prediction = tf.equal(tf.arg_max(y, 1), tf.arg_max(y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))print("Accuarcy on Test-dataset: ", sess.run(accuracy, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)) 运行结果如下图： 参考资料 TensorFlow官方帮助文档Tensorflow之MNIST解析]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker集群使用文档]]></title>
    <url>%2FDocker%E5%AE%B9%E5%99%A8%2F2019-2-21-Docker%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[用户管理 (管理员权限)添加docker用户组:1sudo groupadd -g 344 docker 添加用户到用户组：1sudo usermod -a -G 用户组 用户 从用户组中删除用户1gpasswd -d 用户 用户组 镜像的基本操作列出本地镜像1docker images 各个选项说明:• REPOSITORY：表示镜像的仓库源（不唯一）• TAG：镜像的标签(不唯一，可以自己设定)• IMAGE ID：镜像ID（唯一）• CREATED：镜像创建时间• SIZE：镜像大小同一个镜像ID可以有多个仓库源和标签，如图中红框所示。 查找镜像 我们可以从Docker Hub网站来搜索镜像，Docker Hub网址为：https://hub.docker.com/我们也可以使用docker search命令来搜索镜像。比如我们需要一个httpd的镜像来作为我们的web服务。我们可以通过docker search命令搜索httpd来寻找适合我们的镜像。 1docker search httpd NAME:镜像仓库源的名称DESCRIPTION:镜像的描述OFFICIAL:是否docker官方发布 下载镜像 当我们在本地主机上使用一个不存在的镜像时 Docker 就会自动下载这个镜像。如果我们想预先下载这个镜像，我们可以使用docker pull命令来下载它。此处以ubuntu:15.10为例,其中15.10为标签，若不写，会默认下载最新的镜像，标签为latest。 1docker pull 镜像名(:标签) 设置镜像标签1docker tag 原始镜像名 新镜像名:标签 发现镜像ID为00a10af6cf18的镜像多了一个新的标签 liufan。 删除镜像 当我们删除某一镜像时，会先尝试删除所有指向该镜像的标签，然后删除该镜像本身。1.若一个镜像有多个标签，我们只想删除已经没用的标签 1docker rmi 仓库源(liufan): 镜像标签(lf) 删除前后我们发现liufan:lf已经被删除 2.彻底删除镜像1docker rmi –f 镜像ID（以8c811b4aec35为例）（不建议-f强制删除） 我们发现8c811b4aec35这个镜像已经被彻底删除（包含所有指向这个镜像的标签） 3.若想删除的镜像有基于它创建的容器存在时，镜像文件是默认无法删除的。（容器会在下面章节有所讲解）1docker run -it --name liufan ubuntu/numpy /bin/bash 我们基于ubuntu/numpy这个镜像创建了一个名为liufan的容器。下面我们退出容器，尝试删除这个镜像，docker会提示有容器在运行，无法删除：若想强制删除，可使用2中的 docker rmi –f 镜像ID，但不建议这样做，因为有容器依赖这个镜像，强制删除会有遗留问题（强制删除的镜像换了新的ID继续存在系统中） 导入导出镜像导出1docker save 镜像(busybox) &gt; 存储位置(/home/lf/aa.tar) 已经在对应目录生成压缩文件 先把本地的busybox镜像删除，然后尝试导入刚刚导出的压缩镜像1docker rmi busybox &amp;&amp; docker images 导入1docker load &lt; (镜像存储位置)/home/lf/aa.tar 我们发现busybox镜像已经成功导入。 注：当已有的镜像不能满足我们的需求时，我们需要自己制作镜像，主要通过下面2中方式：1） 通过Dockerfile文件制作镜像（较难）2） 基于一个原始镜像创建一个容器，在容器里面进行一些操作（安装一些框架或者软件包），然后退出容器，利用commit命令提交生成新的镜像 （简单）### 容器基本操作&gt; 容器是镜像的一个运行实例，它是基于镜像创建的。#### 新建容器12docker create -it --name lf tensorflowdocker ps -a&gt; &gt; 可以看见我们成功创建了一个名为lf，基于tensorflow镜像的容器。使用docker create 命令新建的容器处于停止状态，可以用如下命令来启动并进入它。12docker start lfdocker attach lf#### 启动容器&gt; 启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（stopped）的容器重新启动。1docker run -it --name liufan ubuntu/numpy /bin/bash上述命令等价于先执行docker create,再执行docker start命令。上面我们以交互模式创建了一个基于ubuntu/numpy镜像，名为liufan的容器。&gt; 其中，-i：表示让容器的标准输入保持打开，&gt; -t：让docker分配一个伪终端并绑定到容器的标准输入上，&gt; /bin/bash：不是必要选项，只是在表明创建容器的时候并运行了bash应用，方便我们进入容器内部，不写也可以，不过那就要用其他命令进入容器了。（docker中必须要保持一个进程的运行，要不然整个容器就会退出）我们可以按Ctrl+d或输入exit命令来退出容器。退出后该容器就会处于终止状态（stopped），可通过3.1中的start和attach重新进入容器。#### 查看终止删除容器1docker ps // 查看所有正在运行容器1docker ps -a // 查看所有容器1docker ps -a -q // 查看所有容器ID1docker stop containerId // containerId 是容器的ID或者名字，一个或多个1docker rm containerId // containerId 是容器的ID或者名字，一个或多个1docker rm containerId // containerId 是容器的ID或者名字，一个或多个可以看到lf、wh这两个容器已经被删除1docker stop $(docker ps -a -q) // stop停止所有容器1docker rm $(docker ps -a -q) // remove删除所有容器注：删除容器时必须保证容器是终止态（stopped），若不是先进行docker stop操作再进行docker rm操作，可以-f强制删除但不建议。#### 进入容器&gt; 1.attach命令使用attach命令有时候并不方便。当多个窗口同时attach到同一个容器的时候，所有的窗口都会同步显示。当某个窗口因命令阻塞时，其他窗口就无法执行操作了。&gt; 2.exec命令docker自1.3版本起，提供了一个更加方便的工具exec，可以直接在容器内部运行命令，例如进入到刚创建的容器中，并启动一个bash#### 导入和导出容器1docker run -it --name liufan ubuntu/numpy /bin/bash我们基于ubuntu/numpy镜像创建了一个名为liufan的容器，下面将它&gt; 导出：1docker export 容器名(liufan) &gt; 存储地址(/home/lf/aa.tar)我们将liufan这个容器导出本地并压缩命名为aa.tar文件。&gt; 导入：先将liufan容器删除在尝试导入12docker stop liufan &amp;&amp;docker rm liufan &amp;&amp;docker ps -adocker import /home/lf/aa.tar test/ubuntu:lf我们可以看到刚刚的容器压缩文件已经成功导入，命名为test/ubuntu:lf镜像。前面第一章中的1.6节中，我们介绍过用docker load命令来导入一个镜像文件，其实这边也可以用docker import命令来导入一个容器到本地镜像库。两者的区别是：docker import：丢弃了所有的历史记录和元数据信息，仅保存容器当时的快照状态。在导入的时候可以重新制定标签等元数据信息。docker load：将保存完整记录，体积较大。### 代码实例（以Tensorflow为例）&gt; 上面两章我介绍了镜像和容器的关系和它们的一些基本操作，接下来我将介绍如何在创建的容器里面运行我们的代码。集群上有Tensorflow、Pytorch、Caffe、MXNet等深度学习框架的镜像，此处我已Tensorflow为例，介绍如何在容器里运行我们的代码。#### 创建容器1docker run -it --name liufan bluesliuf/tensorflow /bin/bash我们基于bluesliuf/tensorflow这个镜像创建了一个名为liufan的镜像，进入容器ls查看目录列表，发现此时的容器就类似一个Linux环境，默认的用户权限为root权限。问题：我们的代码和数据集都在本地机器上，如何放到容器内部呢？直接复制困难并且耗时，如果我们的数据集过大。Docker提供了一种方法：挂载。将我们的本地目录挂载到容器内部，实现本机目录文件和容器目录文件共享。#### 挂载本地目录到容器&gt; 注：查询资料发现不能先创建容器，再挂载本地目录，两者必须同时进行，于是我们重新创建容器并挂载本地目录。我的代码和数据集都放在本机/home/lf/lf/catdogs目下，下面将它挂载到容器内。创建容器有2种方式-v:挂载的命令参数红色冒号前：本地目录的绝对路径红色冒号后：容器挂载本地目录的绝对路径蓝色部分表示容器需要使用GPU 时将显卡驱动映射到容器中，默认参数不用修改，如果不使用GPU 可以不加蓝色部分name: 创建的容器名bluesliuf/tensorflow:基于的镜像不调用GPU（本机）：调用GPU（集群）：可以看见我们已经成功将本地目录挂载到了我们指定的容器内部位置。运行代码（本机）：注:本地代码里面通常会有数据集的读取路径，一些生成日志文件的存储路径，我们要对它进行修改，换为容器内读取和存储路径。再去容器内部看，本地的修改已经同步到容器内了。在本地修改文件和容器内修改文件都行，一处修改两者都会同步修改。但建议在本地修改，因为本地修改起来方便，容器内一般用vim编辑器，较为不便。在终端输入命令：python 代码文件名（此处我是tr aining.py） 不调用GPU（本机）： 可以看见代码已经成功运行，并且相应的日志文件也存储到本地目录（容器目录当然也有，两者是同步共享的）此外，docker还提供了类似screen，可以让容器在后台运行的功能，退出时如果想继续运行：按顺序按【ctrl+p】【ctrl+q】，下次再使用docker attach 或者docker exec进入容器，可以看见我们的程序还在继续运行。例如： 调用GPU（本机）：在后台运行和上面一样，也是利用【ctrl+p】【ctrl+q】。 数据卷挂载Docker针对挂载目录还提供了一种高级的用法。叫数据卷。 数据卷：“其实就是一个正常的容器，专门用来提供数据卷供其它容器挂载的”。感觉它就像是由一个容器定义的一个数据挂载信息。其他的容器启动可以直接挂载数据卷容器中定义的挂载信息。示例如下： 1.创建一个普通的容器，名为wuhao，并将本地的文件目录挂载到了容器，接下来把这个容器当做一个数据卷。1docker run -v /home/lf/lf/catdogs:/var/catdogs --name wuhao bluesliuf/tensorflow /bin/bash 2.再创建一个新的容器，来使用这个数据卷。1docker run -it --volumes-from wuhao --name lf bluesliuf/tensorflow /bin/bash –volumes-from用来指定要从哪个数据卷来挂载数据。我们可以发现通过wuhao这个容器（数据卷），我们成功的将本地目录也挂载到了lf这个容器内。 通过数据卷挂载目录更具有优势。1） 我们只需先创建一个容器并挂载本地目录，将其看成数据卷，当我们其他容器也需要挂载同样目录的时候，我们只需要利用–volumes-from就可以实现。2） 当我们需要挂载的本地目录发生改变时，我们只需要修改作为数据卷那个容器挂载的本地目录即可（类似一个全局变量），而无须一个个修改其他容器的本地挂载目录。 挂载成功后。运行代码步骤与上面一样。]]></content>
      <categories>
        <category>Docker容器</category>
      </categories>
      <tags>
        <tag>docker使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的常见模型（GAN）]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-2-20-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B-GAN%2F</url>
    <content type="text"><![CDATA[GAN的来源 14年Goodfellow提出Generative adversarial nets即生成式对抗网络，它要解决的问题是如何从训练样本中学习出新样本，训练样本是图片就生成新图片，训练样本是文章就输出新文章等等。 GANs简单的想法就是用两个模型， 一个生成模型，一个判别模型。判别模型用于判断一个给定的图片是不是真实的图片（从数据集里获取的图片），生成模型的任务是去创造一个看起来像真的图片一样的图片，有点拗口，就是说模型自己去产生一个图片，可以和你想要的图片很像。而在开始的时候这两个模型都是没有经过训练的，这两个模型一起对抗训练，生成模型产生一张图片去欺骗判别模型，然后判别模型去判断这张图片是真是假，最终在这两个模型训练的过程中，两个模型的能力越来越强，最终达到稳态。 GAN的基本组成 GAN 模型中的两位博弈方分别由生成式模型（generative model）和判别式模型（discriminative model）充当。 生成模型： G 捕捉样本数据的分布，用服从某一分布（均匀分布，高斯分布等）的噪声 z 生成一个类似真实训练数据的样本，追求效果是越像真实样本越好； 判别模型: D 是一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率，如果样本来自于真实的训练数据，D 输出大概率，否则，D 输出小概率。 可以做如下类比：生成网络 G 好比假币制造团伙，专门制造假币，判别网络 D 好比警察，专门检测使用的货币是真币还是假币，G 的目标是想方设法生成和真币一样的货币，使得 D 判别不出来，D 的目标是想方设法检测出来 G 生成的假币。 上图是GAN网络的思想导图，我们用1代表真实数据，0来代表生成的假数据。对于判别器D来说，对于真实数据，它要尽可能判别正确输出值1；而对于生成器G，根据随机噪音向量z生成假数据也输入判别器D，对于这些假数据，判别器要尽可能输出值0。 GAN的训练过程可以看成一个博弈的过程，也可以看成2个人在玩一个极大极小值游戏，可以用如下公式表示： $$\min \limits_{G}\max\limits_{D}GAN(D,G)=E_{x\sim P_{data}(x)}[logD(x)]+E_{z\sim p_z(z)}[log(1-D(G(z)))]$$其本质上是两个优化问题，把拆解就如同下面两个公式： 优化D：$$\max\limits_{D}GAN(D,G)=E_{x\sim P_{data}(x)}[logD(x)]+E_{z\sim p_z(z)}[log(1-D(G(z)))]$$优化G：$$\min\limits_{G}GAN(D,G)=E_{z\sim p_z(z)}[log(1-D(G(z)))]$$ 当优化D时，生成器确定,我们要让判别器尽可能输出高的值，所以要最大化公式(2)的值；当优化G的时候，判别器确定，我们要使判别器判断错误，尽可能使D(G(z))的值更大，所以要最小化公式(3)的值。 GAN的训练过程 上图是GAN的训练过程，解析见图中右边文字。 GAN的算法流程和动态求解过程如下图所示： 一开始我们确定G，最大化D，让点沿着D变大的方向移动(红色箭头)，然后我们确定D，最小化G，让点沿着G变小的方向移动(蓝色箭头)。循环上述若干步后，达到期望的鞍点(理想最优解)。 GAN的网络结构判别器(卷积) 卷积层大家应该都很熟悉了,为了方便说明，定义如下： 二维的离散卷积（N=2）方形的特征输入（i1=i2=i）方形的卷积核尺寸（k1=k2=k ）每个维度相同的步长（s1=s2=s）每个维度相同的padding (p1=p2=p)下图(左)表示参数为 (i=5,k=3,s=2,p=1)的卷积计算过程，从计算结果可以看出输出特征的尺寸为 (o1=o2=o=3)；下图(右)表示参为 (i=6,k=3,s=2,p=1)的卷积计算过程，从计算结果可以看出输出特征的尺寸为 (o1=o2=o=3)。 从上述2个例子我们可以总结出卷积层输入特征和输出特征尺寸和卷积核参数的关系为：$$o=\lfloor\frac{i+2p-k}{s}\rfloor+1$$ 生成器(反卷积) 在介绍反卷积之前，我们先来看一下卷积运算和矩阵运算之间的关系。例有如下运算(i=4,k=3,s=1,p=0)，输出为o=2。 通过上述的分析，我们已经知道卷积层的前向操作可以表示为和矩阵C相乘，那么我们很容易得到卷积层的反向传播就是和C的转置相乘。 反卷积和卷积的关系如下： 右上图表示的是参数为( i′=2,k′=3,s′=1,p′=2)的反卷积操作，其对应的卷积操作参数为 (i=4,k=3,s=1,p=0)。我们可以发现对应的卷积和非卷积操作其 (k=k′,s=s′)，但是反卷积却多了p′=2。通过对比我们可以发现卷积层中左上角的输入只对左上角的输出有贡献，所以反卷积层会出现 p′=k−p−1=2。通过示意图，我们可以发现，反卷积层的输入输出在 s=s′=1的情况下关系为：$$o′=i′-k′+2p′+1=i′+(k-1)-2p$$GAN的优点 ●GAN是一种生成式模型，相比较其他生成模型（玻尔兹曼机和GSNs）只用到了反向传播 ●相比其他所有模型, GAN可以产生更加清晰，真实的样本 ●GAN采用的是一种无监督的学习方式训练，可以被广泛用在无监督学习和半监督学习领域GAN的缺点●训练GAN需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到.我们还没有找到很好的达到纳什均衡的方法,所以训练GAN相比VAE或者PixelRNN是不稳定的●GAN不适合处理离散形式的数据，比如文本●GAN存在训练不稳定、梯度消失、模式崩溃的问题实例DCGAN网络网络结构 (判别器) 网络结构 (生成器) 二次元动漫人脸（共50个epoch）数据集：51223张动漫人脸 图左为原始数据集，图右为训练过程训练过程生成效果图如下： 真实人脸（共100个epoch）数据集：CelebA 是香港中文大学的开放数据集，包含10,177个名人身份的202,599张人脸图片。（选取了25600张）,数据集如下：训练过程生成效果图如下：]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
        <tag>GAN model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的发展]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-1-14-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%91%E5%B1%95%2F</url>
    <content type="text"><![CDATA[深度学习的发展历程 人工智能 机器学习 深度学习 人工智能 远在古希腊时期，发明家就梦想着创造能自主思考的机器。当人类第一次构思可编程计算机时，就已经在思考计算机能否变得智能（尽管这距造出第一台计算机还有一百多年）(Lovelace, 1842)。如今，人工智能（artificialintelligence, AI）已经成为一个具有众多实际应用和活跃研究课题的领域，并且正在蓬勃发展。我们期望通过智能软件自动地处理常规劳动、理解语音或图像、帮助医学诊断和支持基础科学研究。一个人的日常生活需要关于世界的巨量知识。很多这方面的知识是主观的、直观的，因此很难通过形式化的方式表达清楚。计算机需要获取同样的知识才能表现出智能。人工智能的一个关键挑战就是如何将这些非形式化的知识传达给计算机。 机器学习 机器学习(Machine Learning)是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构并不断改善自身性能的学科。简单来说，机器学习就是通过算法，使得机器能从大量的历史数据中学习规律，从而对新的样本做智能识别或预测未来。机器学习在图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等很多方面的发展还存在着没有良好解决的问题。 上图是机器学习解决问题的一般流程，即将原始数据划分为训练数据和测试数据，并提取数据的特征用以训练模型，最终测试数据用来测试模型的好坏（泛化能力）。 深度学习 深度学习的概念源于人工神经网络的研究，含多隐层的多层感知机就是一种深度学习结构。深度学习通过组合低层特征形式更加抽象的高层表示属性类别或特征了，来发现数据的分布式特征表示。其动机在于建立、模拟人脑进行分析学习的神经网络，它模拟人脑的机制来解释数据，例如图像、声音和文本，深度学习是无监督学习的一种。其实，神经网络早在八九十年代就被提出过，真正使得深度学习兴起有2个方面的因素： 大数据，用于训练数据的增加； 计算机的算力大大增加，更快的CPU、通用GPU 的出现 上图是深度学习的简单结构图，主要包含三个部分：输入层（Visible layer）、隐藏层（hidden layer）和输出层（Output layer）。图中解决的是图片分类问题。输入层输入图片，即像素矩阵；对于隐藏层，第一层可以轻易地通过比较相邻像素的亮度来识别边缘。有了第一隐藏层描述的边缘，第二隐藏层可以容易地搜索可识别为角和扩展轮廓的边集合。给定第二隐藏层中关于角和轮廓的图像描述，第三隐藏层可以找到轮廓和角的特定集合来检测特定对象的整个部分；最后根据图像描述中包含的对象部分，输出层输出图片中所包含的对象类别。 深度学习常见的编程框架 观察发现，Google、Microsoft、Facebook等巨头都参与了这场深度学习框架大战，此外，还有毕业于伯克利大学的贾扬清主导开发的Caffe，蒙特利尔大学Lisa Lab团队开发的Theano，以及其他个人或商业组织贡献的框架。 另外，可以看到各大主流框架基本都支持Python，目前Python在科学计算和数据挖掘领域可以说是独领风骚。虽然有来自R、Julia等语言的竞争压力，但是Python的各种库实在是太完善了，Web开发、数据可视化、数据预处理、数据库连接、爬虫等无所不能，有一个完美的生态环境。仅在数据挖据工具链上，Python就有NumPy、SciPy、Pandas、Scikit-learn、XGBoost等组件，做数据采集和预处理都非常方便，并且之后的模型训练阶段可以和TensorFlow等基于Python的深度学习框架完美衔接。 深度学习的应用无人驾驶 深度学习在无人驾驶领域主要用于图像处理， 也就是摄像头上面。 当然也可以用于雷达的数据处理， 但是基于图像极大丰富的信息以及难以手工建模的特性， 深度学习能最大限度的发挥其优势。 在做无人车的公司中，他们都会用到三个传感器激光雷达（lidar），测距雷达（radar）和摄像头（camera），但还是会各有侧重。比如 Waymo（前谷歌无人车）以激光雷达为主，而特斯拉和中国的图森互联以摄像头为主。我们可以从特斯拉近期放出的一段无人驾驶的视频中看到特斯拉有三个摄像头传感器，左中右各一个。 从上图我们可以看出，特斯拉成功识别了道路线（红色的线）前方整个路面（右中图），这个过程也可以用深度学习完成。 AlphaGo阿尔法狗 阿尔法狗（AlphaGo）是第一个击败人类职业围棋选手、第一个战胜围棋世界冠军的人工智能程序。它主要的原理就是深度学习。早在1997年，IBM的国际象棋系统深蓝，击败了世界冠军卡斯帕罗夫时，采用的算法是通过暴力搜索的方式尝试更多的下棋方法从而战胜人类，其所依赖的更多是计算机的计算资源优势。但在围棋上，深蓝的方式完全不适用。为了战胜人类围棋选手，AlphaGo需要更加智能且强大的算法。深度学习为其提供了可能。 AlphaGo主要包括三个组成部分： 蒙特卡洛搜索树（MonteCarlo tree search，MCTS） 估值网络（Value network） 策略网络（Policy notebook） AlphaGo的一个大脑——策略网络，通过深度学习在当前给定棋盘条件下，预测下一步在哪里落子。通过大量对弈棋谱获取训练数据，该网络预测人类棋手下一步落子点的准确率可达57%以上（当年数据）并可以通过自己跟自己对弈的方式提高落子水平。AlphaGo的另一个大脑——估值网络，判断在当前棋盘条件下黑子赢棋的概率。其使用的数据就是策略网络自己和自己对弈时产生的。AlphaGo使用蒙特卡罗树算法，根据策略网络和估值网络对局势的评判结果来寻找最佳落子点。 人脸识别 人脸识别的方法有很多，如face++，DeepFace，FaceNet……常规的人脸识别流程为：人脸检测—&gt;对齐—&gt;表达—&gt;分类。 人脸对齐的方法包括以下几步：1.通过若干个特征点检测人脸；2.剪切；3.建立Delaunay triangulation;4.参考标准3d模型；5.讲3d模型比对到图片上；6.进行仿射变形；7.最终生成正面图像。 学习深度学习所需的基础知识]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的常见模型（CNN）]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-1-14-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B-CNN%2F</url>
    <content type="text"><![CDATA[CNN的来源 CNN由纽约大学的Yann LeCun于1998年提出。CNN本质上是一个多层感知机，其成功的原因关键在于它所采用的局部连接和共享权值的方式。 一方面减少了的权值的数量使得网络易于优化，另一方面降低了过拟合的风险。CNN是神经网络中的一种，它的权值共享网络结构使之更类似于生物神经网络，降低了网络模型的复杂度，减少了权值的数量。 权重共享：在卷积神经网络中，卷积层的每一个卷积滤波器重复的作用于整个感受野中，对输入图像进行卷积，卷积结果构成了输入图像的特征图，提取出图像的局部特征。每一个卷积滤波器共享相同的参数，包括相同的权重矩阵和偏置项。共享权重的好处是在对图像进行特征提取时不用考虑局部特征的位置。而且权重共享提供了一种有效的方式，使要学习的卷积神经网络模型参数数量大大降低。 CNN的网络架构 卷积神经网络结构包括：卷积层，降采样层，全链接层。每一层有多个特征图，每个特征图通过一种卷积滤波器提取输入的一种特征，每个特征图有多个神经元。 卷积层（Conv） 再举一个卷积过程的例子如下：我们有下面这个绿色的55输入矩阵，卷积核是一个下面这个黄色的33矩阵。卷积的步幅是一个像素。则卷积的过程如下面的动图。卷积的结果是一个3*3的矩阵。 上面举的例子都是二维的输入，卷积的过程比较简单，那么如果输入是多维的呢？比如在前面一组卷积层+池化层的输出是3个矩阵，这3个矩阵作为输入呢，那么我们怎么去卷积呢？又比如输入的是对应RGB的彩色图像，即是三个分布对应R，G和B的矩阵呢？ 池化层（Pooling） 全连接层（Full Connecting） 总结 一般CNN的结构依次为1、 input2、 ((conv–&gt;relu)N–&gt;pool?)M3、 (fc–&gt;relu)*K4、 fc 卷积神经网络的训练算法 与一般的机器学习算法相比，先定义Loss function,衡量和实际结果之间的差距； 找到最小化损失函数的W（权重）和b（偏置），CNN里面最常见的算法为SGD（随机梯度下降）。 卷积神经网络的优缺点优点 共享卷积核，便于处理高维数据； 不像机器学习人为提取特征，网络训练权重自动提取特征，且分类效果好。 缺点 需要大量训练样本和好的硬件支持（GPU、TPU…）; 物理含义模糊（神经网络是一种难以解释的“黑箱模型”，我们并不知道卷积层到底提取的是什么特征）。 卷积神经网络的典型结构 实战演练猫狗大战，即一个简单的二分类问题，训练出一个自动判别猫狗的模型 训练集（共25000张图片，猫狗各12500张）测试集（共3000张图片，猫狗各1500张） 我们通过Tensorflow这个深度学习框架来构建我们的分类网络。通过其自带的可视化工具Tensorboard我们可以看到网络的详细结构，如下左图所示。模型训练完成后，我们用测试集来测试模型的泛化能力，输入一张测试图片，导入模型，输出分类结果，示例见下右图。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>deep learning</tag>
        <tag>CNN model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[欢迎访问我的博客]]></title>
    <url>%2Funcategorized%2F2019-01-13-FirstBlog%2F</url>
    <content type="text"><![CDATA[Hey 大家好，我是一名计算机领域的在读研究生，现研究方向为Deep Learning、Computer vision,欢迎大家来学习交流。 [查看个人简历][访问主页]]]></content>
      <tags>
        <tag>first</tag>
      </tags>
  </entry>
</search>

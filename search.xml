<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Docker集群使用文档]]></title>
    <url>%2FDocker%E5%AE%B9%E5%99%A8%2F2019-2-21-Docker%E9%9B%86%E7%BE%A4%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[用户管理 (管理员权限)添加docker用户组:1sudo groupadd -g 344 docker 添加用户到用户组：1sudo usermod -a -G 用户组 用户 从用户组中删除用户1gpasswd -d 用户 用户组 镜像的基本操作列出本地镜像1docker images 各个选项说明:• REPOSITORY：表示镜像的仓库源（不唯一）• TAG：镜像的标签(不唯一，可以自己设定)• IMAGE ID：镜像ID（唯一）• CREATED：镜像创建时间• SIZE：镜像大小同一个镜像ID可以有多个仓库源和标签，如图中红框所示。 查找镜像 我们可以从Docker Hub网站来搜索镜像，Docker Hub网址为：https://hub.docker.com/我们也可以使用docker search命令来搜索镜像。比如我们需要一个httpd的镜像来作为我们的web服务。我们可以通过docker search命令搜索httpd来寻找适合我们的镜像。 1docker search httpd NAME:镜像仓库源的名称DESCRIPTION:镜像的描述OFFICIAL:是否docker官方发布 下载镜像 当我们在本地主机上使用一个不存在的镜像时 Docker 就会自动下载这个镜像。如果我们想预先下载这个镜像，我们可以使用docker pull命令来下载它。此处以ubuntu:15.10为例,其中15.10为标签，若不写，会默认下载最新的镜像，标签为latest。 1docker pull 镜像名(:标签) 设置镜像标签1docker tag 原始镜像名 新镜像名:标签 发现镜像ID为00a10af6cf18的镜像多了一个新的标签 liufan。 删除镜像 当我们删除某一镜像时，会先尝试删除所有指向该镜像的标签，然后删除该镜像本身。1.若一个镜像有多个标签，我们只想删除已经没用的标签 1docker rmi 仓库源(liufan): 镜像标签(lf) 删除前后我们发现liufan:lf已经被删除 2.彻底删除镜像1docker rmi –f 镜像ID（以8c811b4aec35为例）（不建议-f强制删除） 我们发现8c811b4aec35这个镜像已经被彻底删除（包含所有指向这个镜像的标签） 3.若想删除的镜像有基于它创建的容器存在时，镜像文件是默认无法删除的。（容器会在下面章节有所讲解）1docker run -it --name liufan ubuntu/numpy /bin/bash 我们基于ubuntu/numpy这个镜像创建了一个名为liufan的容器。下面我们退出容器，尝试删除这个镜像，docker会提示有容器在运行，无法删除：若想强制删除，可使用2中的 docker rmi –f 镜像ID，但不建议这样做，因为有容器依赖这个镜像，强制删除会有遗留问题（强制删除的镜像换了新的ID继续存在系统中） 导入导出镜像导出1docker save 镜像(busybox) &gt; 存储位置(/home/lf/aa.tar) 已经在对应目录生成压缩文件 先把本地的busybox镜像删除，然后尝试导入刚刚导出的压缩镜像1docker rmi busybox &amp;&amp; docker images 导入1docker load &lt; (镜像存储位置)/home/lf/aa.tar 我们发现busybox镜像已经成功导入。 注：当已有的镜像不能满足我们的需求时，我们需要自己制作镜像，主要通过下面2中方式：1） 通过Dockerfile文件制作镜像（较难）2） 基于一个原始镜像创建一个容器，在容器里面进行一些操作（安装一些框架或者软件包），然后退出容器，利用commit命令提交生成新的镜像 （简单）### 容器基本操作&gt; 容器是镜像的一个运行实例，它是基于镜像创建的。#### 新建容器12docker create -it --name lf tensorflowdocker ps -a&gt; &gt; 可以看见我们成功创建了一个名为lf，基于tensorflow镜像的容器。使用docker create 命令新建的容器处于停止状态，可以用如下命令来启动并进入它。12docker start lfdocker attach lf#### 启动容器&gt; 启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（stopped）的容器重新启动。1docker run -it --name liufan ubuntu/numpy /bin/bash上述命令等价于先执行docker create,再执行docker start命令。上面我们以交互模式创建了一个基于ubuntu/numpy镜像，名为liufan的容器。&gt; 其中，-i：表示让容器的标准输入保持打开，&gt; -t：让docker分配一个伪终端并绑定到容器的标准输入上，&gt; /bin/bash：不是必要选项，只是在表明创建容器的时候并运行了bash应用，方便我们进入容器内部，不写也可以，不过那就要用其他命令进入容器了。（docker中必须要保持一个进程的运行，要不然整个容器就会退出）我们可以按Ctrl+d或输入exit命令来退出容器。退出后该容器就会处于终止状态（stopped），可通过3.1中的start和attach重新进入容器。#### 查看终止删除容器1docker ps // 查看所有正在运行容器1docker ps -a // 查看所有容器1docker ps -a -q // 查看所有容器ID1docker stop containerId // containerId 是容器的ID或者名字，一个或多个1docker rm containerId // containerId 是容器的ID或者名字，一个或多个1docker rm containerId // containerId 是容器的ID或者名字，一个或多个可以看到lf、wh这两个容器已经被删除1docker stop $(docker ps -a -q) // stop停止所有容器1docker rm $(docker ps -a -q) // remove删除所有容器注：删除容器时必须保证容器是终止态（stopped），若不是先进行docker stop操作再进行docker rm操作，可以-f强制删除但不建议。#### 进入容器&gt; 1.attach命令使用attach命令有时候并不方便。当多个窗口同时attach到同一个容器的时候，所有的窗口都会同步显示。当某个窗口因命令阻塞时，其他窗口就无法执行操作了。&gt; 2.exec命令docker自1.3版本起，提供了一个更加方便的工具exec，可以直接在容器内部运行命令，例如进入到刚创建的容器中，并启动一个bash#### 导入和导出容器1docker run -it --name liufan ubuntu/numpy /bin/bash我们基于ubuntu/numpy镜像创建了一个名为liufan的容器，下面将它&gt; 导出：1docker export 容器名(liufan) &gt; 存储地址(/home/lf/aa.tar)我们将liufan这个容器导出本地并压缩命名为aa.tar文件。&gt; 导入：先将liufan容器删除在尝试导入12docker stop liufan &amp;&amp;docker rm liufan &amp;&amp;docker ps -adocker import /home/lf/aa.tar test/ubuntu:lf我们可以看到刚刚的容器压缩文件已经成功导入，命名为test/ubuntu:lf镜像。前面第一章中的1.6节中，我们介绍过用docker load命令来导入一个镜像文件，其实这边也可以用docker import命令来导入一个容器到本地镜像库。两者的区别是：docker import：丢弃了所有的历史记录和元数据信息，仅保存容器当时的快照状态。在导入的时候可以重新制定标签等元数据信息。docker load：将保存完整记录，体积较大。### 代码实例（以Tensorflow为例）&gt; 上面两章我介绍了镜像和容器的关系和它们的一些基本操作，接下来我将介绍如何在创建的容器里面运行我们的代码。集群上有Tensorflow、Pytorch、Caffe、MXNet等深度学习框架的镜像，此处我已Tensorflow为例，介绍如何在容器里运行我们的代码。#### 创建容器1docker run -it --name liufan bluesliuf/tensorflow /bin/bash我们基于bluesliuf/tensorflow这个镜像创建了一个名为liufan的镜像，进入容器ls查看目录列表，发现此时的容器就类似一个Linux环境，默认的用户权限为root权限。问题：我们的代码和数据集都在本地机器上，如何放到容器内部呢？直接复制困难并且耗时，如果我们的数据集过大。Docker提供了一种方法：挂载。将我们的本地目录挂载到容器内部，实现本机目录文件和容器目录文件共享。#### 挂载本地目录到容器&gt; 注：查询资料发现不能先创建容器，再挂载本地目录，两者必须同时进行，于是我们重新创建容器并挂载本地目录。我的代码和数据集都放在本机/home/lf/lf/catdogs目下，下面将它挂载到容器内。创建容器有2种方式-v:挂载的命令参数红色冒号前：本地目录的绝对路径红色冒号后：容器挂载本地目录的绝对路径蓝色部分表示容器需要使用GPU 时将显卡驱动映射到容器中，默认参数不用修改，如果不使用GPU 可以不加蓝色部分name: 创建的容器名bluesliuf/tensorflow:基于的镜像不调用GPU（本机）：调用GPU（集群）：可以看见我们已经成功将本地目录挂载到了我们指定的容器内部位置。运行代码（本机）：注:本地代码里面通常会有数据集的读取路径，一些生成日志文件的存储路径，我们要对它进行修改，换为容器内读取和存储路径。再去容器内部看，本地的修改已经同步到容器内了。在本地修改文件和容器内修改文件都行，一处修改两者都会同步修改。但建议在本地修改，因为本地修改起来方便，容器内一般用vim编辑器，较为不便。在终端输入命令：python 代码文件名（此处我是tr aining.py） 不调用GPU（本机）： 可以看见代码已经成功运行，并且相应的日志文件也存储到本地目录（容器目录当然也有，两者是同步共享的）此外，docker还提供了类似screen，可以让容器在后台运行的功能，退出时如果想继续运行：按顺序按【ctrl+p】【ctrl+q】，下次再使用docker attach 或者docker exec进入容器，可以看见我们的程序还在继续运行。例如： 调用GPU（本机）：在后台运行和上面一样，也是利用【ctrl+p】【ctrl+q】。 数据卷挂载Docker针对挂载目录还提供了一种高级的用法。叫数据卷。 数据卷：“其实就是一个正常的容器，专门用来提供数据卷供其它容器挂载的”。感觉它就像是由一个容器定义的一个数据挂载信息。其他的容器启动可以直接挂载数据卷容器中定义的挂载信息。示例如下： 1.创建一个普通的容器，名为wuhao，并将本地的文件目录挂载到了容器，接下来把这个容器当做一个数据卷。1docker run -v /home/lf/lf/catdogs:/var/catdogs --name wuhao bluesliuf/tensorflow /bin/bash 2.再创建一个新的容器，来使用这个数据卷。1docker run -it --volumes-from wuhao --name lf bluesliuf/tensorflow /bin/bash –volumes-from用来指定要从哪个数据卷来挂载数据。我们可以发现通过wuhao这个容器（数据卷），我们成功的将本地目录也挂载到了lf这个容器内。 通过数据卷挂载目录更具有优势。1） 我们只需先创建一个容器并挂载本地目录，将其看成数据卷，当我们其他容器也需要挂载同样目录的时候，我们只需要利用–volumes-from就可以实现。2） 当我们需要挂载的本地目录发生改变时，我们只需要修改作为数据卷那个容器挂载的本地目录即可（类似一个全局变量），而无须一个个修改其他容器的本地挂载目录。 挂载成功后。运行代码步骤与4.2.1中一样。]]></content>
      <categories>
        <category>Docker容器</category>
      </categories>
      <tags>
        <tag>docker使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的常见模型（GAN）]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-2-20-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B-GAN%2F</url>
    <content type="text"><![CDATA[GAN的来源 14年Goodfellow提出Generative adversarial nets即生成式对抗网络，它要解决的问题是如何从训练样本中学习出新样本，训练样本是图片就生成新图片，训练样本是文章就输出新文章等等。 GANs简单的想法就是用两个模型， 一个生成模型，一个判别模型。判别模型用于判断一个给定的图片是不是真实的图片（从数据集里获取的图片），生成模型的任务是去创造一个看起来像真的图片一样的图片，有点拗口，就是说模型自己去产生一个图片，可以和你想要的图片很像。而在开始的时候这两个模型都是没有经过训练的，这两个模型一起对抗训练，生成模型产生一张图片去欺骗判别模型，然后判别模型去判断这张图片是真是假，最终在这两个模型训练的过程中，两个模型的能力越来越强，最终达到稳态。 GAN的基本组成 GAN 模型中的两位博弈方分别由生成式模型（generative model）和判别式模型（discriminative model）充当。 生成模型： G 捕捉样本数据的分布，用服从某一分布（均匀分布，高斯分布等）的噪声 z 生成一个类似真实训练数据的样本，追求效果是越像真实样本越好； 判别模型: D 是一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率，如果样本来自于真实的训练数据，D 输出大概率，否则，D 输出小概率。 可以做如下类比：生成网络 G 好比假币制造团伙，专门制造假币，判别网络 D 好比警察，专门检测使用的货币是真币还是假币，G 的目标是想方设法生成和真币一样的货币，使得 D 判别不出来，D 的目标是想方设法检测出来 G 生成的假币。 上图是GAN网络的思想导图，我们用1代表真实数据，0来代表生成的假数据。对于判别器D来说，对于真实数据，它要尽可能判别正确输出值1；而对于生成器G，根据随机噪音向量z生成假数据也输入判别器D，对于这些假数据，判别器要尽可能输出值0。 GAN的训练过程可以看成一个博弈的过程，也可以看成2个人在玩一个极大极小值游戏，可以用如下公式表示： $$\min \limits_{G}\max\limits_{D}GAN(D,G)=E_{x\sim P_{data}(x)}[logD(x)]+E_{z\sim p_z(z)}[log(1-D(G(z)))]$$其本质上是两个优化问题，把拆解就如同下面两个公式： 优化D：$$\max\limits_{D}GAN(D,G)=E_{x\sim P_{data}(x)}[logD(x)]+E_{z\sim p_z(z)}[log(1-D(G(z)))]$$优化G：$$\min\limits_{G}GAN(D,G)=E_{z\sim p_z(z)}[log(1-D(G(z)))]$$ 当优化D时，生成器确定,我们要让判别器尽可能输出高的值，所以要最大化公式(2)的值；当优化G的时候，判别器确定，我们要使判别器判断错误，尽可能使D(G(z))的值更大，所以要最小化公式(3)的值。 GAN的训练过程 上图是GAN的训练过程，解析见图中右边文字。 GAN的算法流程和动态求解过程如下图所示： 一开始我们确定G，最大化D，让点沿着D变大的方向移动(红色箭头)，然后我们确定D，最小化G，让点沿着G变小的方向移动(蓝色箭头)。循环上述若干步后，达到期望的鞍点(理想最优解)。 GAN的网络结构判别器(卷积) 卷积层大家应该都很熟悉了,为了方便说明，定义如下： 二维的离散卷积（N=2）方形的特征输入（i1=i2=i）方形的卷积核尺寸（k1=k2=k ）每个维度相同的步长（s1=s2=s）每个维度相同的padding (p1=p2=p)下图(左)表示参数为 (i=5,k=3,s=2,p=1)的卷积计算过程，从计算结果可以看出输出特征的尺寸为 (o1=o2=o=3)；下图(右)表示参为 (i=6,k=3,s=2,p=1)的卷积计算过程，从计算结果可以看出输出特征的尺寸为 (o1=o2=o=3)。 从上述2个例子我们可以总结出卷积层输入特征和输出特征尺寸和卷积核参数的关系为：$$o=\lfloor\frac{i+2p-k}{s}\rfloor+1$$ 生成器(反卷积) 在介绍反卷积之前，我们先来看一下卷积运算和矩阵运算之间的关系。例有如下运算(i=4,k=3,s=1,p=0)，输出为o=2。 通过上述的分析，我们已经知道卷积层的前向操作可以表示为和矩阵C相乘，那么我们很容易得到卷积层的反向传播就是和C的转置相乘。 反卷积和卷积的关系如下： 右上图表示的是参数为( i′=2,k′=3,s′=1,p′=2)的反卷积操作，其对应的卷积操作参数为 (i=4,k=3,s=1,p=0)。我们可以发现对应的卷积和非卷积操作其 (k=k′,s=s′)，但是反卷积却多了p′=2。通过对比我们可以发现卷积层中左上角的输入只对左上角的输出有贡献，所以反卷积层会出现 p′=k−p−1=2。通过示意图，我们可以发现，反卷积层的输入输出在 s=s′=1的情况下关系为：$$o′=i′-k′+2p′+1=i′+(k-1)-2p$$GAN的优点 ●GAN是一种生成式模型，相比较其他生成模型（玻尔兹曼机和GSNs）只用到了反向传播 ●相比其他所有模型, GAN可以产生更加清晰，真实的样本 ●GAN采用的是一种无监督的学习方式训练，可以被广泛用在无监督学习和半监督学习领域GAN的缺点●训练GAN需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到.我们还没有找到很好的达到纳什均衡的方法,所以训练GAN相比VAE或者PixelRNN是不稳定的●GAN不适合处理离散形式的数据，比如文本●GAN存在训练不稳定、梯度消失、模式崩溃的问题实例DCGAN网络网络结构 (判别器) 网络结构 (生成器) 二次元动漫人脸（共50个epoch）数据集：51223张动漫人脸 图左为原始数据集，图右为训练过程训练过程生成效果图如下： 真实人脸（共100个epoch）数据集：CelebA 是香港中文大学的开放数据集，包含10,177个名人身份的202,599张人脸图片。（选取了25600张）,数据集如下：训练过程生成效果图如下：]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deep learning model (GAN)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的发展]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-1-14-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8F%91%E5%B1%95%2F</url>
    <content type="text"><![CDATA[深度学习的发展历程 人工智能 机器学习 深度学习 人工智能 远在古希腊时期，发明家就梦想着创造能自主思考的机器。当人类第一次构思可编程计算机时，就已经在思考计算机能否变得智能（尽管这距造出第一台计算机还有一百多年）(Lovelace, 1842)。如今，人工智能（artificialintelligence, AI）已经成为一个具有众多实际应用和活跃研究课题的领域，并且正在蓬勃发展。我们期望通过智能软件自动地处理常规劳动、理解语音或图像、帮助医学诊断和支持基础科学研究。一个人的日常生活需要关于世界的巨量知识。很多这方面的知识是主观的、直观的，因此很难通过形式化的方式表达清楚。计算机需要获取同样的知识才能表现出智能。人工智能的一个关键挑战就是如何将这些非形式化的知识传达给计算机。 机器学习 机器学习(Machine Learning)是一门专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构并不断改善自身性能的学科。简单来说，机器学习就是通过算法，使得机器能从大量的历史数据中学习规律，从而对新的样本做智能识别或预测未来。机器学习在图像识别、语音识别、自然语言理解、天气预测、基因表达、内容推荐等很多方面的发展还存在着没有良好解决的问题。 上图是机器学习解决问题的一般流程，即将原始数据划分为训练数据和测试数据，并提取数据的特征用以训练模型，最终测试数据用来测试模型的好坏（泛化能力）。 深度学习 深度学习的概念源于人工神经网络的研究，含多隐层的多层感知机就是一种深度学习结构。深度学习通过组合低层特征形式更加抽象的高层表示属性类别或特征了，来发现数据的分布式特征表示。其动机在于建立、模拟人脑进行分析学习的神经网络，它模拟人脑的机制来解释数据，例如图像、声音和文本，深度学习是无监督学习的一种。其实，神经网络早在八九十年代就被提出过，真正使得深度学习兴起有2个方面的因素： 大数据，用于训练数据的增加； 计算机的算力大大增加，更快的CPU、通用GPU 的出现 上图是深度学习的简单结构图，主要包含三个部分：输入层（Visible layer）、隐藏层（hidden layer）和输出层（Output layer）。图中解决的是图片分类问题。输入层输入图片，即像素矩阵；对于隐藏层，第一层可以轻易地通过比较相邻像素的亮度来识别边缘。有了第一隐藏层描述的边缘，第二隐藏层可以容易地搜索可识别为角和扩展轮廓的边集合。给定第二隐藏层中关于角和轮廓的图像描述，第三隐藏层可以找到轮廓和角的特定集合来检测特定对象的整个部分；最后根据图像描述中包含的对象部分，输出层输出图片中所包含的对象类别。 深度学习常见的编程框架 观察发现，Google、Microsoft、Facebook等巨头都参与了这场深度学习框架大战，此外，还有毕业于伯克利大学的贾扬清主导开发的Caffe，蒙特利尔大学Lisa Lab团队开发的Theano，以及其他个人或商业组织贡献的框架。 另外，可以看到各大主流框架基本都支持Python，目前Python在科学计算和数据挖掘领域可以说是独领风骚。虽然有来自R、Julia等语言的竞争压力，但是Python的各种库实在是太完善了，Web开发、数据可视化、数据预处理、数据库连接、爬虫等无所不能，有一个完美的生态环境。仅在数据挖据工具链上，Python就有NumPy、SciPy、Pandas、Scikit-learn、XGBoost等组件，做数据采集和预处理都非常方便，并且之后的模型训练阶段可以和TensorFlow等基于Python的深度学习框架完美衔接。 深度学习的应用无人驾驶 深度学习在无人驾驶领域主要用于图像处理， 也就是摄像头上面。 当然也可以用于雷达的数据处理， 但是基于图像极大丰富的信息以及难以手工建模的特性， 深度学习能最大限度的发挥其优势。 在做无人车的公司中，他们都会用到三个传感器激光雷达（lidar），测距雷达（radar）和摄像头（camera），但还是会各有侧重。比如 Waymo（前谷歌无人车）以激光雷达为主，而特斯拉和中国的图森互联以摄像头为主。我们可以从特斯拉近期放出的一段无人驾驶的视频中看到特斯拉有三个摄像头传感器，左中右各一个。 从上图我们可以看出，特斯拉成功识别了道路线（红色的线）前方整个路面（右中图），这个过程也可以用深度学习完成。 AlphaGo阿尔法狗 阿尔法狗（AlphaGo）是第一个击败人类职业围棋选手、第一个战胜围棋世界冠军的人工智能程序。它主要的原理就是深度学习。早在1997年，IBM的国际象棋系统深蓝，击败了世界冠军卡斯帕罗夫时，采用的算法是通过暴力搜索的方式尝试更多的下棋方法从而战胜人类，其所依赖的更多是计算机的计算资源优势。但在围棋上，深蓝的方式完全不适用。为了战胜人类围棋选手，AlphaGo需要更加智能且强大的算法。深度学习为其提供了可能。 AlphaGo主要包括三个组成部分： 蒙特卡洛搜索树（MonteCarlo tree search，MCTS） 估值网络（Value network） 策略网络（Policy notebook） AlphaGo的一个大脑——策略网络，通过深度学习在当前给定棋盘条件下，预测下一步在哪里落子。通过大量对弈棋谱获取训练数据，该网络预测人类棋手下一步落子点的准确率可达57%以上（当年数据）并可以通过自己跟自己对弈的方式提高落子水平。AlphaGo的另一个大脑——估值网络，判断在当前棋盘条件下黑子赢棋的概率。其使用的数据就是策略网络自己和自己对弈时产生的。AlphaGo使用蒙特卡罗树算法，根据策略网络和估值网络对局势的评判结果来寻找最佳落子点。 人脸识别 人脸识别的方法有很多，如face++，DeepFace，FaceNet……常规的人脸识别流程为：人脸检测—&gt;对齐—&gt;表达—&gt;分类。 人脸对齐的方法包括以下几步：1.通过若干个特征点检测人脸；2.剪切；3.建立Delaunay triangulation;4.参考标准3d模型；5.讲3d模型比对到图片上；6.进行仿射变形；7.最终生成正面图像。 学习深度学习所需的基础知识]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习的常见模型（CNN）]]></title>
    <url>%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%2F2019-1-14-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B8%B8%E8%A7%81%E6%A8%A1%E5%9E%8B-CNN%2F</url>
    <content type="text"><![CDATA[CNN的来源 CNN由纽约大学的Yann LeCun于1998年提出。CNN本质上是一个多层感知机，其成功的原因关键在于它所采用的局部连接和共享权值的方式。 一方面减少了的权值的数量使得网络易于优化，另一方面降低了过拟合的风险。CNN是神经网络中的一种，它的权值共享网络结构使之更类似于生物神经网络，降低了网络模型的复杂度，减少了权值的数量。 权重共享：在卷积神经网络中，卷积层的每一个卷积滤波器重复的作用于整个感受野中，对输入图像进行卷积，卷积结果构成了输入图像的特征图，提取出图像的局部特征。每一个卷积滤波器共享相同的参数，包括相同的权重矩阵和偏置项。共享权重的好处是在对图像进行特征提取时不用考虑局部特征的位置。而且权重共享提供了一种有效的方式，使要学习的卷积神经网络模型参数数量大大降低。 CNN的网络架构 卷积神经网络结构包括：卷积层，降采样层，全链接层。每一层有多个特征图，每个特征图通过一种卷积滤波器提取输入的一种特征，每个特征图有多个神经元。 卷积层（Conv） 再举一个卷积过程的例子如下：我们有下面这个绿色的55输入矩阵，卷积核是一个下面这个黄色的33矩阵。卷积的步幅是一个像素。则卷积的过程如下面的动图。卷积的结果是一个3*3的矩阵。 上面举的例子都是二维的输入，卷积的过程比较简单，那么如果输入是多维的呢？比如在前面一组卷积层+池化层的输出是3个矩阵，这3个矩阵作为输入呢，那么我们怎么去卷积呢？又比如输入的是对应RGB的彩色图像，即是三个分布对应R，G和B的矩阵呢？ 卷积层（Pooling） 全连接层（Full Connecting） 总结 一般CNN的结构依次为1、 input2、 ((conv–&gt;relu)N–&gt;pool?)M3、 (fc–&gt;relu)*K4、 fc 卷积神经网络的训练算法 与一般的机器学习算法相比，先定义Loss function,衡量和实际结果之间的差距； 找到最小化损失函数的W（权重）和b（偏置），CNN里面最常见的算法为SGD（随机梯度下降）。 卷积神经网络的优缺点优点 共享卷积核，便于处理高维数据； 不像机器学习人为提取特征，网络训练权重自动提取特征，且分类效果好。 缺点 需要大量训练样本和好的硬件支持（GPU、TPU…）; 物理含义模糊（神经网络是一种难以解释的“黑箱模型”，我们并不知道卷积层到底提取的是什么特征）。 卷积神经网络的典型结构 实战演练猫狗大战，即一个简单的二分类问题，训练出一个自动判别猫狗的模型 训练集（共25000张图片，猫狗各12500张）测试集（共3000张图片，猫狗各1500张） 我们通过Tensorflow这个深度学习框架来构建我们的分类网络。通过其自带的可视化工具Tensorboard我们可以看到网络的详细结构，如下左图所示。模型训练完成后，我们用测试集来测试模型的泛化能力，输入一张测试图片，导入模型，输出分类结果，示例见下右图。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>deep learning model (CNN)</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My First Blog]]></title>
    <url>%2Funcategorized%2F2019-01-13-FirstBlog%2F</url>
    <content type="text"><![CDATA[Hey 这是我的第一篇博客。 大家好，我是一名在读的研一学生，现研究领域为Deep Learning, Computer Vision, 欢迎大家来学习交流！]]></content>
      <tags>
        <tag>first</tag>
      </tags>
  </entry>
</search>
